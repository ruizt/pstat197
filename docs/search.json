[
  {
    "objectID": "people.html",
    "href": "people.html",
    "title": "People",
    "section": "",
    "text": "Participants listed by year."
  },
  {
    "objectID": "people.html#section",
    "href": "people.html#section",
    "title": "People",
    "section": "2022-2023",
    "text": "2022-2023\n\nInstructor\nTrevor Ruiz, Statistics and Applied Probability (PSTAT197A-B-C/CMPSC190DD-DE-DF)\n\n\nProject sponsors\nMaxim Ivanov and Bonnie Jin, Amgen\nAri Polakof, Appfolio\nErin Satterthwaite, California Cooperative Oceanic Fisheries Investigations\nAndy Chen and Amy Huynh, Carpe Data\nEleanor Caves and Todd Oakley, Caves Visual Ecology Lab\nKatja Seltmann, Cheadle Center for Biodiversity and Ecological Restoration\nGreg Husak and Shrad Shukla, Climate Hazards Center\nPatrick Green, Evolutionary Ecology and Marine Biology\nNorbert Leinfellner and Paula Sarmiento, Inogen\nArinbjörn Kolbeinsson and Eric Daza, Evidation Health\nSomayeh Dodge and Evgeny Noi, MOVE Lab\nEric Leidersdorf, Peak Performance Project\nDerek Mendez, Stanford National Accelerator Lab\n\n\nProject advisors\nTrevor Ruiz, Statistics and Applied Probability\nAlex Franks, Statistics and Applied Probability\nLaura Baracaldo, Statistics and Applied Probability\nAdam Waterbury, Statistics and Applied Probability\nEnbo Zhou, Geography\nErika McPhillips, Statistics and Applied Probability\nJoshua Bang, Statistics and Applied Probability\nMegan Elcheikhali, Statistics and Applied Probability\nRobin Liu, Statistics and Applied Probability\nGia Ahn, Statistics and Applied Probability\nYan Lashchev, Statistics and Applied Probability\n\n\nStudents\nSafiya Alavi, STSDS\nDaniel Badilla, STSDS\nEllie Burrell, STSDS\nBrian Che, STSDS\nAleksander Cichosz, ECON/STSDS\nFei Du, AMATH/STSDS\nBrian Fan, AMATH/STSDS\nKunxiao Gao, STSDS\nBernie Graves, STSDS\nYuer Hao, FINMS\nDisha Hauzaree, STSDS\nEdward Ho, STSDS\nEvan Ji, STSDS\nDingan Jiang, ECON/STSDS\nAria Kajeh, STSDS\nRaymond Lee, PBS/STSDS\nHannah Li, STSDS\nLily Li, STSDS\nYujie Li, STSDS/LING\nAlex Lim, STSDS\nJustin Liu, STSDS\nTianhong Liu, FINMS\nWilliam Long, STSDS\nAnya Macomber, STSDS\nAnkur Malik, STSDS\nVardan Martirosyan, MATCS\nLex Navarra, STSDS\nJennifer Park, ECACC/STSDS\nMira Patel, STSDS\nRyan Quon, STSDS\nAllester Ramayrat, STSDS\nGiselle Ramirez, ENVST/POLS\nNoa Rapoport, STSDS\nEitan Rashkovan, STSDS\nKaylin Roberts, STSDS\nShannon Rumsey, STSDS\nNealson Setiawan, STSDS\nThomas Shi, ACTSC\nKabir Snell, STSDS\nAshley Son, STSDS\nSofia Spasibenko, STSDS\nRyan Stofer, AMATH/STSDS\nSammy Suliman, AMATH/STSDS\nKassandra Trejo-Zamora, STSDS\nPiero Trujillo, STSDS\nLyndsey Umsted, STSDS\nJai Uparkar, STSDS/CPTCS\nJustin Vo, ECACC/STSDS\nRuoxin Wang, STSDS\nYuqing Xia, STSDS\nChanghee Yoon, STSDS\nTeo Zeng, PBS/AMATH\nChunting Zheng, AMATH/STSDS\nHaoming Deng, STSDS\nCaroline He, BIOCS\nDan Le, CMPSC\nAllen Wang, STSDS"
  },
  {
    "objectID": "people.html#section-1",
    "href": "people.html#section-1",
    "title": "People",
    "section": "2021-2022",
    "text": "2021-2022\n\nInstructors\nAmbuj Singh, Computer Science (PSTAT197A/CMPSC190DD)\nAlex Franks, Statistics and Applied Probability (PSTAT197B-C/CMPSC190DE-DF)\nTrevor Ruiz, Statistics and Applied Probability (PSTAT197B-C/CMPSC190DE-DF)\n\n\nProject sponsors\nChad Spensky and Rita Mounir, Allthenticate\nShyr-Shea Chang and Soeren Thust, Appfolio\nErin Satterthwaite, California Cooperative Oceanic Fisheries Investigations\nCrystal Zhang and Kevin Neal, Carpe Data\nQing Huang, Center for Information Technology Research\nKatja Seltmann, Cheadle Center for Biodiversity and Ecological Restoration\nKelly Caylor, Anna Boser, and Ryan Avery, Earth Research Institute\nMatto Mildenberger, Energy and Environment Transitions Lab\nEric Daza, Evidation Health\nRob Fox and Lauren Wong, HG Insights\nStella Glasauer, Neuroscience Research Institute\nRebert Bernard, Pricewaterhouse Coopers\nTommy Sprague, Sprague Lab\n\n\nProject advisors\nTrevor Ruiz, Statistics and Applied Probability\nAlex Franks, Statistics and Applied Probability\nAdam Waterbury, Statistics and Applied Probability\nLeron Reznikov, Computer Science\nSikun Lin, Computer Science\nErika McPhillips, Statistics and Applied Probability\nJoshua Bang, Statistics and Applied Probability\nMegan Elcheikhali, Statistics and Applied Probability\n\n\nStudents\nAnnie Adams, STSDS\nNicholas Alexander, STSDS\nJoshua Alvarado, STSDS\nAkshat Ataliwala, STSDS\nAndrew Bissell, STSDS\nEvan Bui, STSDS\nPhilip Carey, STSDS\nEric Cha, STSDS\nTristan Chen, STSDS\nTyler Chia, STSDS/ECON\nDante Coletta, STSDS\nAnum Damani, STSDS\nJames Du, PHYS/STSDS\nRomina Fareghbal, STSDS\nEmma Franzblau, POLS\nIan Gascon, FINMS/ECON\nAtherv Gole, STSDS\nMallika Gupta, FINMS\nJoshua Harasaki, STSDS\nEoin Hayes, STSDS\nAnnie Huang, LING/STSDS\nHarleen Kaur, ECON/STSDS\nAlyssa Keehan, STSDS\nJoanne Kim, STSDS\nHarper Klauke, STSDS\nMac Kul, STSDS\nJasmine Kwok, COMM/STSDS\nMichael La, STSDS/ECON\nYan Lashchev, AMATH\nNatasha Leodjaja, STSDS\nConnor Levenson, STSDS\nJorge Murillo, MATCS\nJessica Nguyen, STSDS\nNathan Roll, ECON/LING\nAlex Rudolph, STSDS\nSarah Schlund, STSDS/GEOG\nPriscilla Siow, STSDS\nXavier Speropoulos, STSDS\nJordan Tran, STSDS\nAli Vakilimafakheri, STSDS\nRithvik Vobbilisetty, STSDS\nEric Wang, STSDS\nJonathan Wong-Lau, ACTSC\nYuki Yamazaki, STSDS\nJohnny Yu, STSDS\nNancy Zha, STSDS\nYangyi Zhang, MTHSC/STSDS\nBosco De Enrique Romeu, UNDE\nTeresa Gonzalez, GEOGP\nAshley Grinstead, ENVST/GEOGP\nTiffany Hsu, ENVST\nQiru Hu, CMPSC\nCalvin Jenkins, CMPEN\nPk Kottapalli, PHYS\nSara Mandic, CMPSC\nElise Nguyen, COMM/ECACC\nLucas Nguyen, CMPSC/CMPSC\nJake Stenger, CPTCS\nLawrence Su, EE\nTimi Tao, STSDS"
  },
  {
    "objectID": "people.html#section-2",
    "href": "people.html#section-2",
    "title": "People",
    "section": "2020-2021",
    "text": "2020-2021\n\nInstructors\nAmbuj Singh, Computer Science (PSTAT197A/CMPSC190DD)\nAlex Franks, Statistics and Applied Probability (PSTAT197B-C/CMPSC190DE-DF)\nSang-Yun Oh, Statistics and Applied Probability (PSTAT197B-C/CMPSC190DE-DF)\n\n\nProject sponsors\nPeng-Yuan Lai and Soeren Thust, Appfolio\nErin Satterthwaite, California Cooperative Oceanic Fisheries Investigations\nCrystal Zhang, Carpe Data\nLubi Lenaburg, Center for Science and Engineering Partnerships\nKatja Seltmann and Michelle Lee, Cheadle Center for Biodiversity and Ecological Restoration\nEric Daza, Evidation Health\nLouis Tremblay, FLIR\nMike Lawrence, Invoca\nJulien Brun and Allison Horst, National Center for Ecological Analysis and Synthesis\nJuliana Acosta Uribe and Ken Kosik, Neuroscience Research Institute\nNamino Glantz and David Kerr, Sansum Diabetes Research Institute\n\n\nProject advisors\nAmbuj Singh, Computer Science\nMike Ludkovski, Statistics and Applied Probability\nSang-Yun Oh, Statistics and Applied Probability\nAlex Franks, Statistics and Applied Probability\nKate Kharitonova, Computer Science\nSikun Lin, Computer Science\nJoshua Bang, Statistics and Applied Probability\nJiajing Zheng, Statistics and Applied Probability\nChau Tran, Statistics and Applied Probability\n\n\nStudents\nNick Bachelder, STSDS\nAngel Chen, STSDS\nAmy Cho, ECON/STSDS\nRobin Hollingsworth, STSDS/ECACC\nZoe Holzer, STSDS\nJoanna Kim, STSDS\nKarsyn Lee, STSDS\nAdhitya Logan, STSDS\nAndrew Nguyen, STSDS\nNicholas Nunes, STSDS\nVikram Pasupathy, CMPSC/STSDS\nLia Ran, STSDS\nMitchell Rapaport, AMATH/STSDS\nCristian Razo, STSDS/PRMTH\nJake Simon, STSDS\nSamantha Solomon, FINMS\nSophia Sternberg, COMM/STSDS\nShuyun Tang, STSDS\nGavin Tieng, STSDS\nRomtin Toranji, STSDS\nMeng Vong, STSDS\nJason Yu, STSDS\nKaren Zhao, STSDS/ECACC\nAndrea Anez, CMPEN\nPriyanka Banerjee, CMPSC/STSDS\nPeter Bayerle, AMATH\nKaranveer Benipal, CMPSC\nMatthew Coleman, STSDS\nCibran De La Fuente, PHIL/LING\nShuyi Fang, LING/GERM\nSam Guo, FINMS/CMPSC\nHelen Huang, CMPSC/PHRMA\nShaka Kanenobu, CMPSC\nSaiprem Kathika, CMPEN\nLawrence Lin, STSDS\nManny Medrano, EE/PRMTH\nShivam Misra, PHYS\nSairisheek Muttukuru, CMPEN\nPriya Padmanabhan, CMPSC\nJay Patwardhan, AMATH\nAri Polakof, CMPSC/STSDS\nSurya Pugal, CMPSC\nAustin Quinn, CMPSC\nLeron Reznikov, CMPSC\nAndie Rupprecht, GEOGP\nNeil Sadhukhan, CMPEN\nVanessa Salgado Zavaleta, CMPSC\nEmily San Juan, PRMTH\nSamuel Sheridan, AMATH/STSDS\nMatthew Xu, STSDS\nRey Yue, CMPSC\nBill Zhang, CMPSC"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data science capstone at UCSB",
    "section": "",
    "text": "This is an archived class website from UC Santa Barbara’s data science capstone — the main purpose of this site is to maintain a record of materials, projects, and people from the initial years of the capstone. Some content and links have been removed.\n\n\n\n2022-2023 Cohort"
  },
  {
    "objectID": "pstat197bc/archived/work-in-progress-presentation-guidelines.html",
    "href": "pstat197bc/archived/work-in-progress-presentation-guidelines.html",
    "title": "Work-in-progress presentations",
    "section": "",
    "text": "These work-in-progress presentations are intended to provide:\nWork-in-progress presentations should not aim to provide a broad overview of the project, but instead focus on the leading edge of your (not your teammates’) efforts. So, your goal as presenter should be to identify the most pressing problem you’re currently working on resolving, bring your audience up to speed on that problem, and pose a few specific questions you’d like the group to discuss with you."
  },
  {
    "objectID": "pstat197bc/archived/work-in-progress-presentation-guidelines.html#content",
    "href": "pstat197bc/archived/work-in-progress-presentation-guidelines.html#content",
    "title": "Work-in-progress presentations",
    "section": "Content",
    "text": "Content\nYou will have 15 minutes to present the problem you’re currently working on. In this time, you should cover the following points with roughly the suggested time allocations:\n\n(1min) remind the group which project you’re working on and what the overarching goal is\n(2min) identify the specific problem you’re working on addressing right now\n(10min) explain the problem in detail, filling in background information, data attributes, relevant aspects of software design, prior results, etc., as needed as you go\n(2min) pose a few specific questions you’d like to discuss or want advice on\n\nFollowing your presentation, your group will have 15 minutes for discussion (see agenda below).\n\nThink of the presentation less in terms of performing … and more in terms of getting the help you need from others to make further progress. In that spirit, make sure you allow time to present the leading edge of your work. That means you need to be economical in how you get listeners up to steam about the aspects of your project that you already have firmly in place.\n\nYou should aim to cover exactly the material the audience needs to be in an informed position relative to the questions you pose at the end. For some general tips on presenting research, see [this page]."
  },
  {
    "objectID": "pstat197bc/archived/work-in-progress-presentation-guidelines.html#agenda",
    "href": "pstat197bc/archived/work-in-progress-presentation-guidelines.html#agenda",
    "title": "Work-in-progress presentations",
    "section": "Agenda",
    "text": "Agenda\nFor each presentation, the group should follow this agenda:\n\n(15min) Work-in-progress presentation\n(5-10min) As a group, do the following:\n\nMake sure the audience is all on the same page: are there any aspects of the presentation that need to be clarified? Details that someone at the table wasn’t able to follow? Technical terms that need definition?\nSelect 2 audience members to reflect the presenter’s problem back – have them restate the problem in their own words. Ask the presenter: did they understand the problem correctly?\n\n(5-10min) Return to the presenter’s questions and discuss as a group.\n\nWe will take a 5 minute break between presentations each class meeting."
  },
  {
    "objectID": "pstat197bc/archived/work-in-progress-presentation-guidelines.html#practicalities",
    "href": "pstat197bc/archived/work-in-progress-presentation-guidelines.html#practicalities",
    "title": "Work-in-progress presentations",
    "section": "Practicalities",
    "text": "Practicalities\nUse the monitor at your table to show slides. Note that these monitors cannot be used to extend your laptop display, so you’ll need to upload the materials you need in advance [here] and then open them in the browser on your table’s monitor.\nPosition yourself near the monitor and ensure that everyone can hear you before you begin. Since groups will be presenting in parallel in a confined space and there will be some background noise, this check is important. You may need to rearrange the audience slightly and/or speak up.\nTry to maintain a connection with your table over the course of these presentations by participating in discussion when others present, and when you are presenting, by speaking to the others at the table (rather than to the screen, the ceiling, the tabletop, etc.) and checking to make sure they’re following along."
  },
  {
    "objectID": "pstat197bc/archived/individual-report-guidelines.html",
    "href": "pstat197bc/archived/individual-report-guidelines.html",
    "title": "Individual project summary guidelines",
    "section": "",
    "text": "The purpose of this assignment is to compose a short project summary of your own authorship that, together with your poster, could comprise a work sample to supplement employment or graduate program applications."
  },
  {
    "objectID": "pstat197bc/archived/individual-report-guidelines.html#instructions",
    "href": "pstat197bc/archived/individual-report-guidelines.html#instructions",
    "title": "Individual project summary guidelines",
    "section": "Instructions",
    "text": "Instructions\nPrepare a 1-3 page (single- or double-spaced) project summary highlighting\n\nResearch objectives and findings\nProject outcomes\nYour contributions\n\nThink of the document as an extended abstract. You do not need to go in-depth on methods and results, though these aspects of your work should be mentioned."
  },
  {
    "objectID": "pstat197bc/archived/individual-report-guidelines.html#suggested-format",
    "href": "pstat197bc/archived/individual-report-guidelines.html#suggested-format",
    "title": "Individual project summary guidelines",
    "section": "Suggested format",
    "text": "Suggested format\nYou can arrange the document as you please and as suits your preferences and project, but as a starting point, a recommended general format is outlined below.\nFront matter. At the top of your document, include a title, an abstract, and a statement of your role or area of contribution. Assume your audience may only read this portion in detail and skim the rest of the document, so you want to be sure that based on this alone they can ascertain: (i) what the project is about; (ii) the main achievements; and (iii) your role on the team.\nProject overview. Provide a brief introduction to the research questions, and indicate the data and methodology. Describe your main findings (one paragraph per finding is suggested) and include figures. Write this in an academic style.\nProject outcomes. Indicate key outputs/outcomes – outputs include your poster presentation and any deliverables rendered to your sponsor, such as additional internal presentations or software products. If applicable, you can mention other anticipated outputs such as manuscripts in preparation, but only if there is a concrete plan for making that happen – don’t include a wish list here. If possible, include citations/links.\nPersonal contributions. Detail which parts of the project work you were primarily responsible for. This may include specific analyses (e.g., “I worked primarily on developing and implementing the cluster analysis arm of the project”) or specific roles (e.g., “I managed code repositories and acted as primary responsible for integrating team members’ work into a cohesive body”).\nSupplemental information. If there is any additional material you feel is important to include or mention as part of your work sample that is not covered by the rest of the document or the poster, you can include that here."
  },
  {
    "objectID": "pstat197bc/archived/individual-report-guidelines.html#evaluation",
    "href": "pstat197bc/archived/individual-report-guidelines.html#evaluation",
    "title": "Individual project summary guidelines",
    "section": "Evaluation",
    "text": "Evaluation\nYour summary will be evaluated based primarily on clarity of writing, in particular:\n\neffectiveness as a summary of research\nclarity of personal contributions to the project"
  },
  {
    "objectID": "pstat197bc/archived/about-winter-meetings.html",
    "href": "pstat197bc/archived/about-winter-meetings.html",
    "title": "Winter class meetings",
    "section": "",
    "text": "Schedule is tentative and subject to change. Check back for updates.\n\n\n\n\n\n\n\nWeek\nClass meeting\nAssignments/comments\n\n\n\n\n1 (1/9/23)\nIntroduction and class logistics\n\n\n\n2 (1/16/23)\nPanel: past project experiences\nClass meets WEDNESDAY\nMotivational statements due Friday\n\n\n3 (1/23/23)\nGetting started and tips for success\nTeam contracts due Friday\n\n\n4 (1/30/23)\nAmgen, Appfolio\n\n\n\n5 (2/6/23)\nCCBER, Caves lab\n\n\n\n6 (2/13/23)\nCarpe Data, CalCOFI\nPeer evaluations due Friday\n\n\n7 (2/20/23)\nHPC workshop\nNo class meetings\nWorkshop on WEDNESDAY in Elings 1601\n\n\n8 (2/27/23)\nMonday: EEMB Patrick, MOVE lab\nWednesday: Evidation\nClass meets MONDAY AND WEDNESDAY\n\n\n9 (3/6/23)\nClimate Hazards teams\n\n\n\n10 (3/13/23)\nP3, SLAC\nInterim report due Friday\n\n\n11 (finals)\nN/A\nIndividual reflections due Thursday"
  },
  {
    "objectID": "pstat197bc/archived/about-winter-meetings.html#useful-links",
    "href": "pstat197bc/archived/about-winter-meetings.html#useful-links",
    "title": "Winter class meetings",
    "section": "Useful links",
    "text": "Useful links\n[Attendance reporting form] – fill out once per class meeting.\n[Drive folder for winter presentations] – upload your slides and handouts here.\n[Presentation review form] – fill out once per presentation."
  },
  {
    "objectID": "pstat197bc/archived/about-winter-meetings.html#week-1",
    "href": "pstat197bc/archived/about-winter-meetings.html#week-1",
    "title": "Winter class meetings",
    "section": "Week 1",
    "text": "Week 1\n\nComments: teams should be scheduling their initial project meetings this week to take place during week 2.\nMonday meeting: course overview [slides]\nClass assignments:\n\nread syllabus\nprepare a motivational statement and [submit] by Friday 1/20 11:59pm PST\nprepare a team contract and submit signed copy by Friday 1/27 11:59pm PST [upload to Google drive]"
  },
  {
    "objectID": "pstat197bc/archived/about-winter-meetings.html#week-2",
    "href": "pstat197bc/archived/about-winter-meetings.html#week-2",
    "title": "Winter class meetings",
    "section": "Week 2",
    "text": "Week 2\nNote WEDNESDAY MEETING this week due to MLK Jr. Day\n\nComments: re-read project abstract before your team meeting.\nWednesday meeting: student panel on past project experiences"
  },
  {
    "objectID": "pstat197bc/archived/about-winter-meetings.html#week-3",
    "href": "pstat197bc/archived/about-winter-meetings.html#week-3",
    "title": "Winter class meetings",
    "section": "Week 3",
    "text": "Week 3\nComments: teams should have data in hand this week; presentation schedule updated; contracts due Friday 1/27; see meeting notes template\n\nMonday meeting: getting started and tips for success [slides]\nIn-class presentations to begin next week; should focus on project background, data, goals, and current/planned work\n\nformat: 20min presentation + 10min discussion\nto prepare: 10-15 slides and 1-2 page handout 24h in advance\n[handout template] [evaluation rubric] [last year’s presentations]\nnote last year’s presentations were all given at the end of the quarter and ran 10m shorter"
  },
  {
    "objectID": "pstat197bc/archived/about-winter-meetings.html#week-4",
    "href": "pstat197bc/archived/about-winter-meetings.html#week-4",
    "title": "Winter class meetings",
    "section": "Week 4",
    "text": "Week 4\nComments: HPC workshop scheduled for 2/22, details TBA; presentation schedule adjusted to accommodate workshop.\n\nMonday meeting: Amgen [handout] [slides] Appfolio [handout] [slides]"
  },
  {
    "objectID": "pstat197bc/archived/about-winter-meetings.html#week-5",
    "href": "pstat197bc/archived/about-winter-meetings.html#week-5",
    "title": "Winter class meetings",
    "section": "Week 5",
    "text": "Week 5\nComments: add your (full) meetings to [this calendar] (current meeting organizers have permissions); add your roles to the [project assignments spreadsheet].\n\nMonday meeting: CCBER [slides] [handout]; Caves lab [slides] [handout]"
  },
  {
    "objectID": "pstat197bc/archived/about-winter-meetings.html#week-6",
    "href": "pstat197bc/archived/about-winter-meetings.html#week-6",
    "title": "Winter class meetings",
    "section": "Week 6",
    "text": "Week 6\nComments: complete pre-workshop instructions (see below) before next week; peer reviews due this week.\n\nMonday meeting: Carpe Data [slides] [handout]; CalCOFI [slides] [handout]\nPeer reviews due Friday 2/17 11:59pm PST [form]\nHPC workshop next week, Wednesday 2/22 2pm-3pm in Elings 1601. To prepare:\n\nat least one student per team should attend, RSVP here\nhave one attending student per team create a CSC account with the following details\n\nidentify Trevor as your academic advisor\nprovide “data science capstone project” as the project description\nindicate that your project is not NSF-funded\n\nhave the same student verify that they can connect to the Pod cluster\n\nnote windows users will need to download the PuTTy SSH client to connect\nall users will need to be behind the campus VPN if connecting from off campus (see instructions if you are connecting from off campus)\n\nlastly, have the same student install FileZilla and ensure they can open a session (see instructions above for connecting to the Pod cluster)"
  },
  {
    "objectID": "pstat197bc/archived/about-winter-meetings.html#week-7",
    "href": "pstat197bc/archived/about-winter-meetings.html#week-7",
    "title": "Winter class meetings",
    "section": "Week 7",
    "text": "Week 7\nComments: no class meeting (President’s day)\n\nWednesday: high-performance computing (HPC) workshop [slides]"
  },
  {
    "objectID": "pstat197bc/archived/about-winter-meetings.html#week-8",
    "href": "pstat197bc/archived/about-winter-meetings.html#week-8",
    "title": "Winter class meetings",
    "section": "Week 8",
    "text": "Week 8\nComments: class meetings will be held both Monday and Wednesday, with a shorter meeting on Wednesday; add codes distributed for (most of) 197C and should follow shortly for 190DF (and the rest), please enroll for 4 credits.\n\nMonday meeting: Patrick Green [slides] [handout]; MOVE lab [slides] [handout]\nWednesday meeting: Evidation [slides] [handout]"
  },
  {
    "objectID": "pstat197bc/archived/about-winter-meetings.html#week-9",
    "href": "pstat197bc/archived/about-winter-meetings.html#week-9",
    "title": "Winter class meetings",
    "section": "Week 9",
    "text": "Week 9\n\nMonday meeting: Climate Hazards Center, Greg’s group [slides] [handout] and Shrad’s group [slides] [handout]\nClass assignments:\n\ninterim reports are due Friday 3/17 11:59pm PST\nsee guidelines for interim reports"
  },
  {
    "objectID": "pstat197bc/archived/about-winter-meetings.html#week-10",
    "href": "pstat197bc/archived/about-winter-meetings.html#week-10",
    "title": "Winter class meetings",
    "section": "Week 10",
    "text": "Week 10\nComments: do meet with your teams this week, set aside some time in this last meeting to reflect and set goals for next quarter; take a break from meetings during finals week and spring break; no further class meetings planned for the quarter.\n\nMonday meeting: P3 [slides] [handout]; SLAC [slides] [handout]"
  },
  {
    "objectID": "pstat197bc/about-syllabus.html",
    "href": "pstat197bc/about-syllabus.html",
    "title": "Course syllabus",
    "section": "",
    "text": "Students will devote the vast majority of their time and effort in this course to project work and are expected to engage fully with their projects. Class meetings will be conducted as seminars with rotating presentations by project teams. Students will meet weekly with their project teams and regular progress is expected. Each team will prepare one written project summary at the end of winter quarter and one poster presentation at the end of spring quarter.\nConcurrent course listing: PSTAT197B-C and CMPSC190DE-DF are held concurrently; enrollment is by instructor consent and admitted students may enroll under either listing. The course content, expectations, assessments, and course policies are identical for students enrolled in either course.\nCatalog description: Research opportunities for undergraduate students. Students practice their data science and applied statistics skills by completing a hands-on team project on a practical problem proposed by a project sponsor. Students are expected to give regular oral presentations and prepare at least one written report on their research. Prerequisite: PSTAT197A/CMPSC190DD."
  },
  {
    "objectID": "pstat197bc/about-syllabus.html#learning-outcomes",
    "href": "pstat197bc/about-syllabus.html#learning-outcomes",
    "title": "Course syllabus",
    "section": "Learning outcomes",
    "text": "Learning outcomes\nThrough their project work and participation in class meetings, students can expect to:\n\nmake original contributions on a methodological or applied research topic involving statistics, data science, and computational science;\ndevelop domain expertise in the area(s) of application relevant to their project topic;\ndevelop and practice effective strategies for communicating research outcomes with clarity and confidence;\nimprove collaboration and teamwork skills;\ndeepen their understanding of careers in data science;\nproduce a strong work sample for inclusion in a professional portfolio."
  },
  {
    "objectID": "pstat197bc/about-syllabus.html#meetings",
    "href": "pstat197bc/about-syllabus.html#meetings",
    "title": "Course syllabus",
    "section": "Meetings",
    "text": "Meetings\n\nWinter 2023\nClass meetings are held once weekly. Project meetings are held once weekly in teams at a time of their choosing either via Zoom, on campus, or on site with the project sponsor.\nWe will not meet regularly at other officially scheduled times but we will use the Wednesday slot as an alternate class meeting time. Students are encouraged to block off section times in their schedule to facilitate convenient scheduling of project work. Section rooms are available during those times as workspaces.\n\n\nSpring 2023\nClass meetings are held once weekly. Student teams are expected to meet twice weekly: once with their advisors, and once separately. These meetings can be scheduled at any time according to preference and availability; access will be arranged to PSTAT’s departmental meeting/tutoring spaces for groups who wish to meet on campus.\nAs in Winter, we will reserve the scheduled Wednesday meeting time as a back-up option. In the absence of a scheduled class meeting, students are welcome to use the assigned classroom during the scheduled time as a working space.\nIn place of our final meeting, we will host a public showcase of capstone projects in the Loma Pelona center. Poster presentations will be given during the scheduled class time; all students are expected to participate. Additional events will be scheduled before and after the poster presentation, and students are encouraged to attend."
  },
  {
    "objectID": "pstat197bc/about-syllabus.html#expectations",
    "href": "pstat197bc/about-syllabus.html#expectations",
    "title": "Course syllabus",
    "section": "Expectations",
    "text": "Expectations\nTime commitment. The course carries a time commitment of 12 hours per week (3 hours per credit unit). Class and project meetings account for roughly 2 hours per week, and class assignments are kept to a minimum to facilitate project work; thus, students should anticipate dedicating 8-10 hours outside of the classroom to project work each week. Students are strongly encouraged to establish a secondary meeting without their advisors each week to help ensure effective time and task management among the group.\nTeam contracts. Each team will develop a contract articulating their goals as a group, basic agreements about communication and collaboration, and steps that will be taken in the event of failure to uphold those agreements.\nTeam roles. Each student will take on one of the following roles per term by mutual agreement with their team:\n\na spokesperson who is responsible for communicating with advisors and course staff on behalf of the team;\na meeting organizer who is responsible for arranging project meetings and keeping time during meetings;\na note-taker who is responsible for maintaining a written record of each meeting;\na repository/data/document manager who is responsible for coordinating access to project files and keeping them organized;\nfor five-person teams, an equity manager who is responsible for ensuring a fair distribution of work among the team members.\n\nPeer review. Teams will conduct an anonymized peer review each quarter in which each student rates their teammates on:\n\ncooperation with the group;\ncommunication;\nadherence to with agreements established by the group contract;\nfulfillment of their assigned role;\nthe extent to which they contributed their fair share to project work.\n\nAggregated results will be used to provide individual feedback and identify any necessary interventions to improve quality of collaboration.\nClass presentations. Each team is expected to present some of their work once per term to the class. In-class presentations should be informal and allow time for intermittent discussion and questions. In Winter, teams will give group presentations and should prepare about 10 slides and a 1-2 page handout. It is strongly recommended that just one team member give each presentation. In Spring, each team member will take a turn presenting individually to a smaller audience consisting of their teammates and one other project group.\nPoster presentation. A public showcase of project work will be held on campus at the end of the Spring quarter. Each team is expected to prepare a poster to present at the showcase.\nNon-disclosure agreements. Some projects are subject to non-disclosure agreements (NDAs) between the university and the sponsoring organization. If so, project advisors will notify students of the terms and share copies of the NDA; students are expected to uphold any such NDAs but should not directly sign any agreements."
  },
  {
    "objectID": "pstat197bc/about-syllabus.html#assessments",
    "href": "pstat197bc/about-syllabus.html#assessments",
    "title": "Course syllabus",
    "section": "Assessments",
    "text": "Assessments\nStudents will be evaluated based on the following:\n\nattendance record and class assignments;\nend-of-term advisor assessments of individual participation and contributions;\ninstructor assessments of in-class presentations and poster;\npeer reviews and end-of-term individual reflections."
  },
  {
    "objectID": "pstat197bc/about-syllabus.html#policies",
    "href": "pstat197bc/about-syllabus.html#policies",
    "title": "Course syllabus",
    "section": "Policies",
    "text": "Policies\nAttendance. Regular attendance at class meetings is expected. Each student can miss one class meeting without notice; further absences may impact course grades. Students are responsible for material discussed in their absence and should review notes and consult a classmate.\nDeadlines. Students are expected to meet assignment deadlines in a timely manner. All deadlines have a 24-hour grace period. Late or amended work may or may not be accepted at the instructor’s discretion.\nEmail. Course staff will make their best effort to reply to email within 48 weekday hours. However, due to high volume, staff cannot guarantee that all messages will receive replies.\nIllness. Students who are ill are required to stay home. Students ill with COVID-19 must comply with university policy regarding reporting and isolation. Accommodations will be made to ensure that students absent due to illness do not fall behind.\nAccommodations. Reasonable accommodations will be made for any student with a qualifying disability. Such requests should be made through the Disabled Students Program (DSP). More information, instructions on how to access accommodations, and information on related resources can be found on the DSP website. Note: in this class there are no timed assessments.\nLetter grades. Letter grades are assigned based only on the assessments identified above and according to university guidelines, with the relative weighting of assessments determined at the discretion of the instructor. While grade calculations will not be disclosed, students are entitled to an explanation of the criteria used to determine their grades if desired. Grades will not be changed except in the case of clerical errors. If students feel their grade has been unfairly assigned, they are entitled to contest it following UCSB procedure for contesting grades.\nConduct. All course participants are expected to maintain respectful and honorable conduct consistent with UCSB ethical standards. Students uncomfortable with the behavior of another course participant for any reason should notify the instructor, course staff, or, if the complaint relates to course staff conduct, an administrative or departmental officer. Evidence of academic dishonesty will be reported to the Office of Student Conduct (OSC); evidence of problematic behavior will be addressed on a case-by-case basis in accord with university policies."
  },
  {
    "objectID": "projects/projects-22-23.html",
    "href": "projects/projects-22-23.html",
    "title": "2022-2023 Projects",
    "section": "",
    "text": "Listed alphabetically by project sponsor."
  },
  {
    "objectID": "projects/projects-22-23.html#amgen",
    "href": "projects/projects-22-23.html#amgen",
    "title": "2022-2023 Projects",
    "section": "Amgen",
    "text": "Amgen\nEvaluation of natural language processing algorithms used for knowledge graph generation\nStudent team: Yujie Li, Caroline He, Sammy Suliman, Safiya Alavi, KunXiao Gao\nAdvisors: Maxim Ivanov, Bonnie Jin, Erika McPhilliips, Yan Lashchev\nA Knowledge Graph (KG) represents knowledge as a network that relates entities such as physical objects and concepts. In a KG, the objects and concepts are represented as nodes, while their relations are represented as edges of the graph. Due to the graph structure of this knowledge representation, a KG can be a potent mechanism for searching entities and their relations, discovering new information and enabling complex decision making.\n\n\n\nFigure: Left, example knowledge graph with three entity types and two relations. Right, summary of example relations for possible entity type pairs.\n\n\nThe process of updating and expanding a KG can be manual and laborious and require ongoing collaboration with subject matter experts. Natural language processing (NLP) and text-mining algorithms could streamline this process by automatically extracting essential information about objects, concepts, and their relations. Once deployed, these algorithms could populate nodes and edges of a KG in an automated fashion based on incoming data.\nTwo crucial processes in generating nodes and edges of the KG are Named Entity Recognition and Linking (NER/NEL) and Relation Extraction (RE). NER/NEL automatically analyzes free text to recognize character substrings that identify entities of interest, while RE identifies entity pairs with certain relations.\nAs NER/NEL and RE are necessary steps in generating nodes and edges of the KG, it is critically important that these algorithms accurately extract entities and their relations from free text. To quantify performance of NER/NEL and RE, benchmarking datasets are often used. The entity and relation labels in these datasets are either annotated manually by subject matter experts or algorithmically using semi-supervised and/or rule-based approaches.\nThe focus of this project lies in identifying open-source NER/NEL and RE algorithms that show the highest performance in recognizing such entities as “company”, “molecule”, “protein” and their relations. A critical aspect of the work will be in generating a benchmarking dataset using publicly available data."
  },
  {
    "objectID": "projects/projects-22-23.html#appfolio",
    "href": "projects/projects-22-23.html#appfolio",
    "title": "2022-2023 Projects",
    "section": "Appfolio",
    "text": "Appfolio\nOutlier detection\nStudent team: Ruoxin Wang, Brian Che, Mira Patel, Justin Vo, Dingan Jiang\nProject advisors: Ari Polakof, Erika McPhillips\nAt Appfolio we have been solving multiple problems in the Property Management Accounting domain with Machine Learning technologies. Many customers do accounting with our AppFolio Property Management (APM) solution, which means that they need to input all their records (invoices, bills, checks, etc.) in Appfolio. If the customer has integrated their bank account with Plaid (third-party provider) in APM, we also have data coming directly from their bank transactions.\nCustomers have been telling us that they would appreciate it if they could detect anomalies in their records and transaction history. For example, they would like to know if the water bill is much more expensive than the previous months for one property, since that would likely indicate a problem. They would also appreciate knowing if there’s a bill that usually comes but it doesn’t. This is a general problem with applications in multiple areas inside our software, and so we would like to have a way of detecting anomalies automatically so that property managers don’t have to worry about every single detail for each property.\nFor this project, students will analyze outlier detection methods, performance evaluation metrics for empirical comparisons between methods, and assess the methods from an explainability perspective. Students will develop comprehensive method comparisons and integrate them in a dashboard to present them with the correct data visualization techniques."
  },
  {
    "objectID": "projects/projects-22-23.html#california-cooperative-oceanic-fisheries-investigations-calcofi",
    "href": "projects/projects-22-23.html#california-cooperative-oceanic-fisheries-investigations-calcofi",
    "title": "2022-2023 Projects",
    "section": "California Cooperative Oceanic Fisheries Investigations (CalCOFI)",
    "text": "California Cooperative Oceanic Fisheries Investigations (CalCOFI)\nAn eDNA window into larval fish habitat, ecosystem structure, and function using CalCOFI data\nStudent team: Anya Macomber, Yuqing (Summer) Xia, Kassandra Trejo-Zamora\nProject advisors: Erin Satterthwaite, Trevor Ruiz\nMost marine fishes and invertebrates produce high abundances of tiny larvae that, depending on the species, reside in the open ocean for weeks to months. Mortality is incredibly high (often upwards of 99%) for most fishes during the larval stage but because larvae are so abundant small changes in rates of survival result in orders of magnitude difference in survival to adult stages (i.e., recruitment). For this reason, elucidating the actual mechanisms in the ocean that affect larval survival is an integral component of fisheries management. Yet, despite over a century of recruitment research, accurately estimating these parameters has been difficult and predicting conditions that facilitate larval survival and drive recruitment remains the fundamental challenge in fisheries science and management.\nCalCOFI data include physical, environmental, and biological measurements collected from water samples and tows from ship cruises conducted quarterly since ~1950. Measurements are: (i) georeferenced – taken at recorded spatial coordinates; (ii) longitudinal – taken repeatedly over time; (iii) profiled – recorded across depth in the water column; and (iv) multivariate. The complexity of the datasets due to these attributes presents both promise for discovery and challenges for data analysis and display.\nThis project will aim to ascertain predictive relationships between environmental and biological systems. Focus on the relationship between environmental DNA and fish larvae is anticipated, but students will have some latitude to shape the direction of their work and may utilize secondary datasets as appropriate to the overall project aim. The primary objectives for this project will be: (i) developing interactive data displays effective for exploratory analysis and communication; and (ii) applying statistical methodology for prediction of variables of interest. Students will work primarily in R with version control."
  },
  {
    "objectID": "projects/projects-22-23.html#carpe-data",
    "href": "projects/projects-22-23.html#carpe-data",
    "title": "2022-2023 Projects",
    "section": "Carpe Data",
    "text": "Carpe Data\nBusiness characteristics classification models\n\n\nStudent team: Noa Rapoport, Haoming Deng, Lex Navarra, Dan Le, Yutong Wang\nProject advisors: Andy Chen, Amy Huynh, Joshua Bang\n\n\n\n\nA business owner usually needs to buy insurance to protect their business against property loss and general liability. And insurance carriers would adjust the premium based on how much risk a business has. Risk can refer to the likelihood of a loss, and the severity of a potential loss.\nInsurance carriers are particularly interested in detecting businesses in “risky” industries or detecting risks in businesses. We have defined three risk groups of interest, “explosive”, “entertainment”, and “traffic”. If a business has one or more characteristics in the table attached in the appendix, it is categorized in related risk groups.\nCarpe Data is hoping to improve its current flagging methodology for businesses with risky behavior through the creation of a machine learning model that will categorize businesses into risk groups (defined below). The model will utilize different business information such as its name, industries, and social media data points (i.e. textual reviews, images, etc.) to help label businesses that fall into the “Entertainment” or “Traffic” risk groups. Note that these groups are NOT mutually exclusive and businesses may overlap with each other.\nFor this project, students will apply new NLP techniques and ML methods to label businesses according to risk groups. Students will validate their approach using manually labeled data and compare their approach with Carpe Data’s current classification method."
  },
  {
    "objectID": "projects/projects-22-23.html#caves-visual-ecology-lab",
    "href": "projects/projects-22-23.html#caves-visual-ecology-lab",
    "title": "2022-2023 Projects",
    "section": "Caves Visual Ecology Lab",
    "text": "Caves Visual Ecology Lab\nMeasuring visual acuity in bees from high-resolution images\nStudent team: Eitan Rashkovan, Allen Wang, Kaylin Roberts, Evan Ji\nProject advisors: Eleanor Caves, Todd Oakley, Gia Anh, Trevor Ruiz\nVisual acuity, the ability to perceive detail, is ecologically important, as it dictates what aspects of a visual scene an animal can resolve. Humans have some of the highest acuity in the animal kingdom, so we live in a perceptual world that is rich in spatial detail. Acuity, however, is highly variable, varying by at least four orders of magnitude across animals, meaning that the majority of animals cannot resolve as much spatial detail as we do. Despite acuity’s ecological importance, however, we know little about why this variation exists, and one unanswered question in most animal groups is whether there aspects of a species’ ecology that correlate with variation in acuity. This project will leverage high resolution photographs taken by the NSF-funded Big Bee Project (http://big-bee.net) to estimate acuity bees. Bees are an excellent group in which to examine variation in acuity, because they exhibit a wide range of lifestyles and ecologies. To measure acuity from a photograph, we must measure two parameters: the diameter of the hexagonal facets (ommatidia) that comprise the bee’s compound eye, and the eye’s radius of curvature.\nFor this project, students will begin with a set of images from approximately 50 species of bees. For each species, several photographs are available that view the bee from different angles. Students will use this dataset to develop algorithms for estimating the diameter of ommatidia and radius of curvature from a photograph. Students will use estimates to calculate acuity. If time permits, the project will explore potential correlates of acuity among aspects of bee ecology, for example nocturnality/diurnality, living socially or solitarily, and sex differentiation."
  },
  {
    "objectID": "projects/projects-22-23.html#cheadle-center-for-biodiversity-and-ecological-restoration-ccber",
    "href": "projects/projects-22-23.html#cheadle-center-for-biodiversity-and-ecological-restoration-ccber",
    "title": "2022-2023 Projects",
    "section": "Cheadle Center for Biodiversity and Ecological Restoration (CCBER)",
    "text": "Cheadle Center for Biodiversity and Ecological Restoration (CCBER)\nConstructing three-dimensional bee models from high-resolution images\n\n\nStudent team: Daniel Badilla, Allester Ramayrat, Sofia Spasibenko, William Long\nProject advisors: Katja Seltmann and Joshua Bang\n\n\n\n\nWhile bees are critical to sustaining a large proportion of global food production, as well as pollinating both wild and cultivated plants, they are decreasing in both numbers and diversity. Our understanding of the factors driving these declines is limited, partly because we need more data on the behavioral and anatomical traits that may make bees vulnerable or resilient to human-induced environmental changes, such as habitat loss and climate change.\nAs part of the Big-Bee project (http://big-bee.net), we are creating 2D & 3D high-resolution images of bees to study anatomical variation. Creating 3D images of bees is challenging because they are small, hairy objects. Several technologies for creating these models (e.g., Agisoft Metashape, meshroom, Neural Radiance Fields) are available. Still, it has yet to be known which performs the best or if adjusting specific model parameters will improve performance. Students will begin by learning the basics of 3D modeling using existing images in Agisoft Metashape before using their acquired knowledge to create bee measurements and improve future models."
  },
  {
    "objectID": "projects/projects-22-23.html#climate-hazards-center-chc",
    "href": "projects/projects-22-23.html#climate-hazards-center-chc",
    "title": "2022-2023 Projects",
    "section": "Climate Hazards Center (CHC)",
    "text": "Climate Hazards Center (CHC)\nIdentifying the drivers of food insecurity in the developing world\nStudent team: Giselle Ramirez, Thomas Shi, Alex Lim, Ryan Quon, Disha Hauzaree\nProject advisors: Greg Husak, Shrad Shukla, Enbo Zhou\nThroughout much of the developing world, human livelihoods are dependent on the success of rainfed agriculture and pastoral biomass for food. In these regions, precipitation (PPT), or rainfall, is a critical indicator of vegetative health and success, and so a number of methods have been developed for expressing the quantity and quality of rainfall. In addition to the amount of PPT coming into the system, there has also been increasing attention on the atmospheric demand for water. One way of capturing this is the use of potential evapotranspiration (PET), which combines temperature, humidity, solar radiation, and wind speed to estimate the amount of water which would be soaked up by the atmosphere over a given surface. Many indicators have sought to combine these factors in a meaningful way to identify deficits in available water for vegetation, and anticipate food shortages in the developing world.\nHowever, PPT and PET are inversely related. The factors associated with rainfall (cloud cover, high humidity, latent cooling) also lead to low PET. This capstone project seeks to explore the relationship between PPT and PET in space and time. Where these factors are tightly coupled, drought events would be accompanied by increased PET, and lead to larger water deficits than might be indicated by precipitation alone. Where the relationship is less coupled, there may be independent factors driving the PET, and we will seek to understand the drivers of that independent component.\nThis project will work with quasi-global grids of PPT and PET from the last 40 years to explore these relationships, potentially identify drivers of both PPT and the independent component of PET, and maybe even explore developing statistical forecasts of these factors based on sea surface temperatures.\n\n\n\nEvaluating and validating stations and satellite based daily precipitation dataset\nStudent team: Lily Li, Changhee Yoon, Hannah Li, Russell Liu, Fei Du\nProject advisors: Shrad Shukla, Greg Husak, Enbo Zhou\nDaily precipitation is critical for monitoring climate hazards such as floods and droughts, and for running hydrologic and crop water balance models that provide estimates of available water to better manage and utilize water resources. The Climate Hazards Center (CHC) is a global leader in generating precipitation dataset based on satellites and on ground stations. The dataset is called CHIRPS, which stands for Climate Hazards Center InfraRed Precipitation with Stations (CHIRPS) and is a gridded dataset that covers the entire globe, at a spatial resolution of 5km X 5km. CHIRPS version 2.0 has been widely used globally, for monitoring climate hazards and for understanding long-term changes in precipitation.\nCHC currently is in the process of developing the latest version of this dataset which is CHIRPS 3.0. The goal of this project is to evaluate the daily precipitation values from this dataset. The evaluation process will involve downloading and processing several independent precipitation datasets that come from diverse sources, to compare them with CHIRPS 3.0, and advanced statistical methods to quantify the performance of the daily CHIRPS 3.0 dataset relative to other datasets. The results of this analysis will be documented and submitted for a peer-reviewed publication."
  },
  {
    "objectID": "projects/projects-22-23.html#patrick-green",
    "href": "projects/projects-22-23.html#patrick-green",
    "title": "2022-2023 Projects",
    "section": "Patrick Green",
    "text": "Patrick Green\nHow do mantis shrimp fight in a community of competitors?\nStudent team: Luke Fields, Brian Fan, Ashley Son, Tianhong Liu\nProject advisors: Patrick Green, Adam Waterbury, Trevor Ruiz\nWhat are the strategies that animals use when competing over essential resources like mates, food, or shelter? Decades of animal behavior research into this question has identified several tactics animals may use. However, most studies do not realistically replicate the problem competing animals face in nature. For example, most experimental studies stage contests between two individuals over one resource; in reality, animals likely interact with a population of multiple competitors, all fighting over multiple resources. Understanding how animals fight in these more naturalistic scenarios will help us discover the strategies animals use in natural systems, while reaffirming or contesting the conclusions of previous work.\nThis study intends to track how a population of individuals compete over multiple resources by studying contests in mantis shrimp, crustaceans that use spring-powered strikes during fights over access to protective burrows. Specifically, we will set up “mesocosms”, large areas holding multiple, individually-marked mantis shrimp and multiple burrows. By tracking each individual over weeks to months, we can see who fights with whom, what their contests are like (e.g., how aggressive), and who ends up holding which resource(s).\nThe primary objectives of this project will be to build methods to track uniquely-marked individuals from video recordings, and to identify timestamps in videos when two individuals come into contact (perhaps leading to a fight). Finally, an additional goal of this project will be to gather movement and residence data from each individual—identifying how much individuals move through their environment and which resource(s) they ultimately hold. The methods from this project will form the basis for a long-term project that modifies this approach for multiple experiments in the future."
  },
  {
    "objectID": "projects/projects-22-23.html#inogen",
    "href": "projects/projects-22-23.html#inogen",
    "title": "2022-2023 Projects",
    "section": "Inogen",
    "text": "Inogen\nOptical character recognition and text extraction techniques for portable oxygen concentrator prescription forms\nStudent team: Ankur Malik and Kabir Snell\nProject Advisors: Trevor Ruiz, Paula Sarmiento, Norbert Leinfellner\nInogen is a medical technology enterprise that specializes in the research, development, manufacture, and marketing of portable oxygen concentrators for patients diagnosed with chronic respiratory illnesses. These devices offer a lightweight and practical alternative to conventional oxygen therapy, enabling patients to lead a more active and self-sufficient lifestyle.\nInogen maintains an extensive collection of concentrator patient prescription forms, but these are stored as a large collection of individual documents. The content of these forms is not collated in an easy-to-access database supporting queries or comprehensive analyses of patient data. However, this would produce gains in efficiency and potentially produce to data-driven insights that lead to improvements in patient care.\nOur team is collaborating with faculty and students from Purdue University and The Data Mine to digitize Inogen’s comprehensive patient database of medical forms. We are utilizing two distinct implementations of Optical Character Recognition (OCR) tools – a commercial software tool and custom scripting in Tesseract, an open-source Python package that utilizes Google’s OCR system. Independently of text extraction, additional data manipulation is necessary to render text content in analysis-ready form. Our primary objective for this project is to automate the extraction of key patient information from prescription forms and render this data in analysis-ready form. We are using mock patient forms to develop an automated pipeline and aim to provide Inogen with an approach and/or program that can be implemented at scale with real patient records."
  },
  {
    "objectID": "projects/projects-22-23.html#evidation-health",
    "href": "projects/projects-22-23.html#evidation-health",
    "title": "2022-2023 Projects",
    "section": "Evidation Health",
    "text": "Evidation Health\nImpact of case definition on early detection systems for COVID-19\nStudent team: Shannon Rumsey, Edward Ho, Nealson Setiawan, Jen Park, Chunting Zheng\nProject advisors: Arinbjörn Kolbeinsson, Eric Daza, Megan Elcheikhali\n\nDetecting infectious diseases, such as COVID-19, early can accelerate case isolation and break chains of infection. We can train a model to do this but there is no clear definition of when a case starts, meaning the ground-truth labels (used for training and evaluation) are often inconsistent between studies. A hypothesis is that case definition, i.e., the time-point at which an individual changes labels from healthy to infected, has a significant effect on systems designed to identify individuals.\nThis project will use home testing kit study data to analyse and compare the effect of changing the case definition on detection through existing models (or new models, depending on project progress) and consider whether the evaluation metrics reflect real-world impact. If progress allows, the project will seek to find the optimal case-onset point (or some function of accuracy vs early detection) for an early-detection system."
  },
  {
    "objectID": "projects/projects-22-23.html#move-lab",
    "href": "projects/projects-22-23.html#move-lab",
    "title": "2022-2023 Projects",
    "section": "MOVE Lab",
    "text": "MOVE Lab\nDetecting changes in human mobility and movement patterns associated with wildfires in California\nStudent team: Ellie Burrell, Justing Liu, Lyndsey Umstead, Piero Trujillo\nProject advisors: Somayeh Dodge, Evgeny Noi, Laura Baracaldo, Enbo Zhou\nThe goal of this project is to use movement as a marker to study behavioral responses of people to environmental changes, in particular during natural disasters. As a case study, this project utilizes large and multi-sourced mobility data sets to investigate changes in movement patterns in associations with wildfire events in California. First, the goal is to investigate available data sets and their suitability for representing movement patterns in wildfire-impacted areas. Second, the project focuses on applying machine learning techniques to identify and trace changes in mobility time series and to associate them to the timelines of different wildfire events between 2019-2022 (before, during, and after the fire) compared with a baseline (e.g. a year without wildfire). The outcomes of this project will inform simulation models to assess and estimate wildfire risk on movement flows in Californian communities."
  },
  {
    "objectID": "projects/projects-22-23.html#peak-performance-project-p3",
    "href": "projects/projects-22-23.html#peak-performance-project-p3",
    "title": "2022-2023 Projects",
    "section": "Peak Performance Project (P3)",
    "text": "Peak Performance Project (P3)\nUnderstanding links between biomechanical data and on-court NBA production\nStudent team: Bernie Graves, Raymond Lee, Aria Kajeh, Jai Uparkar\nProject advisors: Eric Leidersdorf, Alex Franks, Erika McPhillips, Yan Lashchev\nOver the past decade, resources poured into analytics efforts and performance staff in the NBA (comprising sport scientists, strength coaches, physical therapists, dietitians, athletic trainers, and physicians) have consistently grown. However, despite continued investment in both of these practices, the understanding of relationships between human performance data and on-court productivity has not yielded the expected return. As a field, our understanding of the physical traits that comprise a competent NBA athlete is limited by a lack of consistent data collection processes, team staff churn, the inherent complexity of studying the human body, and the diversity of player roles in basketball.\nBeginning in 2013, P3 (Peak Performance Project) began collecting biomechanical data on professional and collegiate basketball players performing ballistic movements in their laboratories. As it currently stands, P3 owns the largest biomechanics database on NBA athletes in the world. This dataset comprises 3D Motion Capture and Force Plate data – allowing P3 to create skeletal models of their athletes for in-depth movement analysis. While many of P3’s efforts to date have involved studying injury-risk and training adaptations within this cohort, the extent to which this data can help inform which physical tools are important in today’s NBA landscape remains largely unknown.\nP3’s in-house dataset consists of 3D Motion Capture and Force Plate data conducted on professional and collegiate basketball players – approximately 1300 assessments in total. This data has been collected on a series of vertical plane (jumps) and lateral plane (lateral acceleration drills) actions in P3 labs in Atlanta, GA and Santa Barbara, CA. In addition to P3’s in-house data, scraping publicly available basketball data web will be integral to project success.\n\nFor this project, students will build a web-scraping tool for the purposes of comparing P3’s in-house data with on-court production at the professional and collegiate levels, build ML models to understand links between biomechanics data and on-court production, and develop an R Shiny application to visualize relationships between P3 data and on-court performance."
  },
  {
    "objectID": "projects/projects-22-23.html#stanford-national-accelerator-lab",
    "href": "projects/projects-22-23.html#stanford-national-accelerator-lab",
    "title": "2022-2023 Projects",
    "section": "Stanford National Accelerator Lab",
    "text": "Stanford National Accelerator Lab\nDiffraction image selector\nStudent team: Aleksander Cichosz, Ryan Stofer, Vardan Martirosyan, Teo Zeng\nProject advisors: Derek Mendez and Robin Liu\nThe quality of images used during serial crystallography data processing often weighs heavily on the accuracy of results. It is routine for experimenters to visually inspect and tune parameters to select the best images to process. By using state-of-the-art machine learning technologies, coupled with realistic forward modeling of crystallographic data, one can build a regression model which accepts images as inputs, and outputs experimentally interesting properties. This paves the way towards a machine intelligence capable of auto-selecting the highest quality images during diffraction collection.\nFor this project, students will use resmos2 data comprising about 10000 simulated diffraction images with corresponding labels, as well as additional simulated datasets, to build a regression model that will work in real-time to label images during data collection at SSRL beamlines. This will greatly enhance the user experience at the SSRL/SMB beamlines."
  },
  {
    "objectID": "pstat197a/about-materials.html",
    "href": "pstat197a/about-materials.html",
    "title": "Materials",
    "section": "",
    "text": "Textbooks:\n\nModern Data Science with R by Baumer, Kaplan, and Horton.\nIntroduction to Statistical Learning with Applications in R by James et al.\nFundamentals of Data Visualization by Claus Wilke.\nR for Data Science by Wickham and Grolemund.\nDeep Learning by Goodfellow, Bengio, and Courville.\n\nDocumentation:\n\nTidyverse and tidymodels packages\nGitHub Docs"
  },
  {
    "objectID": "pstat197a/about-materials.html#resources",
    "href": "pstat197a/about-materials.html#resources",
    "title": "Materials",
    "section": "",
    "text": "Textbooks:\n\nModern Data Science with R by Baumer, Kaplan, and Horton.\nIntroduction to Statistical Learning with Applications in R by James et al.\nFundamentals of Data Visualization by Claus Wilke.\nR for Data Science by Wickham and Grolemund.\nDeep Learning by Goodfellow, Bengio, and Courville.\n\nDocumentation:\n\nTidyverse and tidymodels packages\nGitHub Docs"
  },
  {
    "objectID": "pstat197a/about-materials.html#introductory-module",
    "href": "pstat197a/about-materials.html#introductory-module",
    "title": "Materials",
    "section": "Introductory module",
    "text": "Introductory module\nObjectives: set expectations; explore data science raison d’etre; introduce systems and design thinking; introduce software tools and collaborative coding; conduct exploratory/descriptive analysis of class background and interests.\n\nWeek 0\n\nThursday meeting: Course orientation [slides]\nAssignments due by next class meeting:\n\ninstall course software and create github account;\nfill out intake form\nread Peng and Parker (2022);\nprepare a reading response\n\n\n\n\nWeek 1\n\nTuesday meeting: On projects in(volving) data science [slides]\nSection meeting: software and technology overview [activity]\nThursday meeting: basic GitHub actions [activity] [slides]\nAssignments due by next class meeting:\n\nread MDSR 9.1 and 9.2\nprepare a reading response\n\n\n\n\nWeek 2\n\nTuesday meeting: Introducing class intake survey data [slides]\nSection meeting: tidyverse basics [activity]\nThursday meeting: planning group work for analysis of survey data [slides]\nAssignments:\n\nfirst team assignment due Friday, October 14, 11:59 PM PST [accept via GH classroom here]"
  },
  {
    "objectID": "pstat197a/about-materials.html#module-1-biomarker-identification",
    "href": "pstat197a/about-materials.html#module-1-biomarker-identification",
    "title": "Materials",
    "section": "Module 1: biomarker identification",
    "text": "Module 1: biomarker identification\nObjectives: introduce variable selection, classification, and multiple testing problems; discuss classification accuracy metrics and data partitioning; fit logistic regression and random forest classifiers in R; learn to implement multiple testing corrections for FDR control (Benjamini-Hochberg and Benjamini-Yekutieli); discuss selection via penalized estimation. Data from Hewitson et al. (2021) .\n\nWeek 3\n\nTuesday meeting: introducing biomarker data; multiple testing [slides]\nSection meeting: iteration strategies [activity]\nThursday meeting: correlation analysis; random forests [slides] [activity]\nAssignments due by next class meeting:\n\nread MDSR 10.1 - 10.2\nread Hewitson et al. (2021)\nprepare a reading response\n\n\n\n\nWeek 4\n\nTuesday meeting: random forests cont’d; logistic regression [slides]\nSection meeting: logistic regression and classification metrics [activity]\nThursday meeting: LASSO regularization [slides]\nAssignments:\n\nsecond group assignment due Friday, October 28, 11:59pm PST [accept via GH classroom] [group assignments]"
  },
  {
    "objectID": "pstat197a/about-materials.html#module-2-fraud-claims",
    "href": "pstat197a/about-materials.html#module-2-fraud-claims",
    "title": "Materials",
    "section": "Module 2: fraud claims",
    "text": "Module 2: fraud claims\nObjectives: introduce NLP techniques for converting text to data and web scraping tools in R; discuss dimension reduction techniques; introduce multiclass classification; learn to process text, fit multinomial logistic regression models, and train neural networks in R.\n\nWeek 5\n\nTuesday meeting: data introduction and basic NLP techniques [slides]\nSection meeting: string manipulation and text processing in R [activity]\nThursday meeting: dimension reduction; multinomial logistic regression [slides] [activity]\nOptional further reading:\n\nMDSR Ch. 19\nCambria and White (2014)\nKhan et al. (2010)\n\n\n\n\nWeek 6\n\nTuesday meeting: feedforward neural networks [slides]\nSection meeting: fitting neural nets with keras [activity]\nThursday meeting: assignment review and planning [slides]\nAssignments:\n\nMidquarter assessments [form]\nRequest winter add code [form]\nRead Emmert-Streib et al. (2020) (§1-5, §9) and prepare a reading response\nthird group assignment due Monday, November 14, 11:59pm PST [accept via GH classroom] [group assignments]\n\nOptional further reading:\n\nAlzubaidi et al. (2021)\nGoodfellow, Bengio, and Courville (2016) Ch. 6 (advanced)"
  },
  {
    "objectID": "pstat197a/about-materials.html#module-3-soil-temperatures",
    "href": "pstat197a/about-materials.html#module-3-soil-temperatures",
    "title": "Materials",
    "section": "Module 3: soil temperatures",
    "text": "Module 3: soil temperatures\nObjectives: build a forecasting model; introduce concepts of spatial and temporal correlation; discuss function approximation and curve fitting with regression techniques; fit elementary time series models and regression with AR errors; spatial interpolation.\n\nWeek 7\n\nTuesday meeting: data introduction; function approximation using basis expansions [slides]\nSection meeting: curve fitting [activity]\nThursday meeting: temporal correlation; a forecasting model [slides]\nOptional further reading (available through UCSB library)\n\nSections 1.1, 1.2, and 2.3 in Shumway and Stoffer (2017)\nPerperoglou et al. (2019)\n\n\n\n\nWeek 8\n\nTuesday meeting: spatial prediction [slides]\nSection meeting: forecasting [activity]\nThursday meeting: NO CLASS\nOptional further reading:\n\n8.1 – 8.3 in Bivand et al. (2008)\nCh. 12 in Dorman (2022, link)"
  },
  {
    "objectID": "pstat197a/about-materials.html#module-4-vignettes",
    "href": "pstat197a/about-materials.html#module-4-vignettes",
    "title": "Materials",
    "section": "Module 4: vignettes",
    "text": "Module 4: vignettes\nObjectives: learn independently about a method of choice and prepare a teaching vignette illustrating its use; create shared reference material potentially useful for project work.\n\nWeek 9\n\nTuesday meeting: discussion on results of claims module; vignette workshopping [slides]\nSection meeting: NO SECTION MEETING (Thanksgiving)\nThursday meeting: NO CLASS (Thanksgiving)\nAssignments: vignettes [guidelines]\n\ndrafts due in class Thursday, 12/1 2pm PST\nfinal version due Thursday, 12/8 11:59pm PST\n\n\n\n\nWeek 10\n\nTuesday meeting: capstone project overviews [slides]\nSection meeting: office hours for vignette help\nThursday meeting: vignette presentation/exchange/feedback [feedback form]\nAssignments due by Friday, 12/2:\n\nread project abstracts\nfill out preference form"
  },
  {
    "objectID": "pstat197a/activities/multinomial-logit.html",
    "href": "pstat197a/activities/multinomial-logit.html",
    "title": "Activity: multinomial logistic regression",
    "section": "",
    "text": "While we are getting started, on your table’s workstation open RStudio and execute the following command:\n\nsource('https://raw.githubusercontent.com/pstat197/pstat197a/main/materials/scripts/package-installs.R')\n\nThen open a new script, copy-paste the code chunk below, and execute once.\n\n# packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(modelr)\nlibrary(Matrix)\nlibrary(sparsesvd)\nlibrary(glmnet)\n\n# path to activity files on repo\nurl &lt;- 'https://raw.githubusercontent.com/pstat197/pstat197a/main/materials/activities/data/'\n\n# load a few functions for the activity\nsource(paste(url, 'projection-functions.R', sep = ''))\n\n# read in data\nclaims &lt;- paste(url, 'claims-multi-tfidf.csv', sep = '') %&gt;%\n  read_csv()\n\n# preview\nclaims\n\n# A tibble: 552 × 15,871\n   mclass    .id   bclass  adams afternoon  agent android    app arkansas arrest\n   &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n 1 unlawful  url1  relev… 0.0692    0.0300 0.0365  0.0450 0.0330   0.0390 0.0140\n 2 irreleva… url3  irrel… 0         0      0       0      0        0      0     \n 3 other     url4  relev… 0         0      0       0      0        0      0     \n 4 fatality  url5  relev… 0         0      0       0      0        0      0     \n 5 irreleva… url7  irrel… 0         0      0       0      0        0      0.0107\n 6 fatality  url8  relev… 0         0      0       0      0        0      0     \n 7 irreleva… url9  irrel… 0         0      0       0      0        0      0     \n 8 irreleva… url10 irrel… 0         0      0       0      0        0      0     \n 9 unlawful  url11 relev… 0         0      0       0      0        0      0.0716\n10 unlawful  url12 relev… 0         0      0       0      0        0      0.0161\n# … with 542 more rows, and 15,861 more variables: arrive &lt;dbl&gt;, assist &lt;dbl&gt;,\n#   attic &lt;dbl&gt;, barricade &lt;dbl&gt;, block &lt;dbl&gt;, blytheville &lt;dbl&gt;, burn &lt;dbl&gt;,\n#   captain &lt;dbl&gt;, catch &lt;dbl&gt;, check &lt;dbl&gt;, chemical &lt;dbl&gt;, copyright &lt;dbl&gt;,\n#   county &lt;dbl&gt;, custody &lt;dbl&gt;, dehydration &lt;dbl&gt;, demand &lt;dbl&gt;,\n#   department &lt;dbl&gt;, desktop &lt;dbl&gt;, device &lt;dbl&gt;, dispute &lt;dbl&gt;,\n#   division &lt;dbl&gt;, drug &lt;dbl&gt;, enter &lt;dbl&gt;, exit &lt;dbl&gt;, family &lt;dbl&gt;,\n#   federal &lt;dbl&gt;, fire &lt;dbl&gt;, force &lt;dbl&gt;, gas &lt;dbl&gt;, gray &lt;dbl&gt;, …"
  },
  {
    "objectID": "pstat197a/activities/multinomial-logit.html#setup",
    "href": "pstat197a/activities/multinomial-logit.html#setup",
    "title": "Activity: multinomial logistic regression",
    "section": "",
    "text": "While we are getting started, on your table’s workstation open RStudio and execute the following command:\n\nsource('https://raw.githubusercontent.com/pstat197/pstat197a/main/materials/scripts/package-installs.R')\n\nThen open a new script, copy-paste the code chunk below, and execute once.\n\n# packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(modelr)\nlibrary(Matrix)\nlibrary(sparsesvd)\nlibrary(glmnet)\n\n# path to activity files on repo\nurl &lt;- 'https://raw.githubusercontent.com/pstat197/pstat197a/main/materials/activities/data/'\n\n# load a few functions for the activity\nsource(paste(url, 'projection-functions.R', sep = ''))\n\n# read in data\nclaims &lt;- paste(url, 'claims-multi-tfidf.csv', sep = '') %&gt;%\n  read_csv()\n\n# preview\nclaims\n\n# A tibble: 552 × 15,871\n   mclass    .id   bclass  adams afternoon  agent android    app arkansas arrest\n   &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n 1 unlawful  url1  relev… 0.0692    0.0300 0.0365  0.0450 0.0330   0.0390 0.0140\n 2 irreleva… url3  irrel… 0         0      0       0      0        0      0     \n 3 other     url4  relev… 0         0      0       0      0        0      0     \n 4 fatality  url5  relev… 0         0      0       0      0        0      0     \n 5 irreleva… url7  irrel… 0         0      0       0      0        0      0.0107\n 6 fatality  url8  relev… 0         0      0       0      0        0      0     \n 7 irreleva… url9  irrel… 0         0      0       0      0        0      0     \n 8 irreleva… url10 irrel… 0         0      0       0      0        0      0     \n 9 unlawful  url11 relev… 0         0      0       0      0        0      0.0716\n10 unlawful  url12 relev… 0         0      0       0      0        0      0.0161\n# … with 542 more rows, and 15,861 more variables: arrive &lt;dbl&gt;, assist &lt;dbl&gt;,\n#   attic &lt;dbl&gt;, barricade &lt;dbl&gt;, block &lt;dbl&gt;, blytheville &lt;dbl&gt;, burn &lt;dbl&gt;,\n#   captain &lt;dbl&gt;, catch &lt;dbl&gt;, check &lt;dbl&gt;, chemical &lt;dbl&gt;, copyright &lt;dbl&gt;,\n#   county &lt;dbl&gt;, custody &lt;dbl&gt;, dehydration &lt;dbl&gt;, demand &lt;dbl&gt;,\n#   department &lt;dbl&gt;, desktop &lt;dbl&gt;, device &lt;dbl&gt;, dispute &lt;dbl&gt;,\n#   division &lt;dbl&gt;, drug &lt;dbl&gt;, enter &lt;dbl&gt;, exit &lt;dbl&gt;, family &lt;dbl&gt;,\n#   federal &lt;dbl&gt;, fire &lt;dbl&gt;, force &lt;dbl&gt;, gas &lt;dbl&gt;, gray &lt;dbl&gt;, …"
  },
  {
    "objectID": "pstat197a/activities/multinomial-logit.html#activity-1-10-min",
    "href": "pstat197a/activities/multinomial-logit.html#activity-1-10-min",
    "title": "Activity: multinomial logistic regression",
    "section": "Activity 1 (10 min)",
    "text": "Activity 1 (10 min)\nYou’ll be given about ten minutes to do the following on your group’s workstation.\n\nPartition the data into training and test sets.\nUsing the training data, find principal components that preserve at least 80% of the total variance and project the data onto those PCs.\nFit a logistic regression model to the training data.\n\n\nStep 1: partitioning\nThis should be familiar from last week’s lab. Use the code chunk below to partition the data. Do not change the RNG seed or split proportion!\n\n# partition data\nset.seed(102722)\npartitions &lt;- claims %&gt;% initial_split(prop = 0.8)\n\n# separate DTM from labels\ntest_dtm &lt;- testing(partitions) %&gt;%\n  select(-.id, -bclass, -mclass)\ntest_labels &lt;- testing(partitions) %&gt;%\n  select(.id, bclass, mclass)\n\n# same, training set\ntrain_dtm &lt;- training(partitions) %&gt;%\n  select(-.id, -bclass, -mclass)\ntrain_labels &lt;- training(partitions) %&gt;%\n  select(.id, bclass, mclass)\n\nNote that we have separated the document term matrix (DTM) from the labels for both partitions. When we project the data onto a subspace, we only want to project the DTM and not the labels.\n\n\nStep 2: projection\nNow find the number of principal components that capture at least 70% of variation and project the document term matrix (DTM) onto those components. Use the custom function projection_fn(.dtm, .prop) .\n\n# find projections based on training data\nproj_out &lt;- projection_fn(.dtm = train_dtm, .prop = 0.7)\ntrain_dtm_projected &lt;- proj_out$data\n\n# how many components were used?\nproj_out$n_pc\n\nNote: projections were found using the training data only. The test data will ultimately be projected onto the same components, as if it were new information we were feeding into a predictive model developed entirely using the training data.\n\n\nStep 3: regression\nBind the binary labels to the projected document term matrix and fit a logistic regression model.\nThe code chunk below gives you the input data frame you need to use glm(). It’s up to you to specify the other arguments needed to fit the model.\n\ntrain &lt;- train_labels %&gt;%\n  transmute(bclass = factor(bclass)) %&gt;%\n  bind_cols(train_dtm_projected)\n\nfit &lt;- glm(..., data = train, ...)\n\nYou will most likely get a warning of some kind – that’s expected. Take note of what the warning says and stop here.\n\n\n\n\n\n\nImportant\n\n\n\nBriefly discuss with your table: what do you think the warning means?"
  },
  {
    "objectID": "pstat197a/activities/multinomial-logit.html#activity-2-10-min",
    "href": "pstat197a/activities/multinomial-logit.html#activity-2-10-min",
    "title": "Activity: multinomial logistic regression",
    "section": "Activity 2 (10 min)",
    "text": "Activity 2 (10 min)\nThis part will guide you through the following steps.\n\nFit a logistic regression model with an elastic net penalty to the training data.\nQuantify classification accuracy on the test data using sensitivity, specificity, and AUROC.\n\n\nStep 1: fit a regularized logistic regression\nglmnet implements the elastic net penalty when a parameter alpha is provided. In the function call, a predictor matrix and response vector are used to specify the model instead of a formula.\nUse the code chunk below to fit the model for a path of regularization strengths, select a strength, and extract the fitted model corresponding to that strength. Do not adjust the RNG seed.\n\n# store predictors and response as matrix and vector\nx_train &lt;- train %&gt;% select(-bclass) %&gt;% as.matrix()\ny_train &lt;- train_labels %&gt;% pull(bclass)\n\n# fit enet model\nalpha_enet &lt;- 0.3\nfit_reg &lt;- glmnet(x = x_train, \n                  y = y_train, \n                  family = 'binomial',\n                  alpha = alpha_enet)\n\n# choose a strength by cross-validation\nset.seed(102722)\ncvout &lt;- cv.glmnet(x = x_train, \n                y = y_train, \n                family = 'binomial',\n                alpha = alpha_enet)\n\n# store optimal strength\nlambda_opt &lt;- cvout$lambda.min\n\n# view results\ncvout\n\n\n\n\n\n\n\nNote\n\n\n\nComment. The elastic net parameter alpha controls the balance between ridge and LASSO penalties: alpha = 0 corresponds to ridge regression, alpha = 1 corresponds to LASSO, and all other values specify a mixture. When the parameter is closer to 1, the LASSO penalty is stronger relative to ridge; and vice-versa when it’s closer to 0.\n\n\n\n\nStep 2: prediction\nTo compute predictions, we’ll need to project the test data onto the same directions used to transform the training data.\nOnce that’s done, we can simply feed the projected test data, the fitted model fit_reg, and the optimal strength lambda_opt to a predict() call.\n\n# project test data onto PCs\ntest_dtm_projected &lt;- reproject_fn(.dtm = test_dtm, proj_out)\n\n# coerce to matrix\nx_test &lt;- as.matrix(test_dtm_projected)\n\n# compute predicted probabilities\npreds &lt;- predict(fit_reg, \n                 s = lambda_opt, \n                 newx = x_test,\n                 type = 'response')\n\nNext bind the test labels to the predictions:\n\n# store predictions in a data frame with true labels\npred_df &lt;- test_labels %&gt;%\n  transmute(bclass = factor(bclass)) %&gt;%\n  bind_cols(pred = as.numeric(preds)) %&gt;%\n  mutate(bclass.pred = factor(pred &gt; 0.5, \n                              labels = levels(bclass)))\n\n# define classification metric panel \npanel &lt;- metric_set(sensitivity, \n                    specificity, \n                    accuracy, \n                    roc_auc)\n\n# compute test set accuracy\npred_df %&gt;% panel(truth = bclass, \n                  estimate = bclass.pred, \n                  pred, \n                  event_level = 'second')\n\n\n\n\n\n\n\nImportant\n\n\n\nBriefly discuss with your table:\n\nHow satisfied are you with the predictive performance?\nDoes the classifier do a better job picking out relevant pages or irrelevant pages?"
  },
  {
    "objectID": "pstat197a/activities/multinomial-logit.html#activity-3-10-min",
    "href": "pstat197a/activities/multinomial-logit.html#activity-3-10-min",
    "title": "Activity: multinomial logistic regression",
    "section": "Activity 3 (10 min)",
    "text": "Activity 3 (10 min)\nNow we’ll fit a multinomial logistic regression model using the multiclass labels rather than the binary ones, still using regularization to prevent overfitting.\n\nStep 1: multinomial regression\nUse the code chunk below to do the fitting. Notice that it’s as simple as supplying the multiclass labels and changing the family = 'binomial' to family = 'multinomial' , but the number of non-intercept parameters is now\n\\[\n\\text{number of predictors} \\times (\\text{number of classes} - 1)\n\\]\nSo in our case, the logistic regression model had \\(p = 55\\) , but when we fit a multinomial model to the data using labels with \\(k = 5\\) classes, we have \\(p(k - 1) = 220\\) parameters!\n\n# get multiclass labels\ny_train_multi &lt;- train_labels %&gt;% pull(mclass)\n\n# fit enet model\nalpha_enet &lt;- 0.2\nfit_reg_multi &lt;- glmnet(x = x_train, \n                  y = y_train_multi, \n                  family = 'multinomial',\n                  alpha = alpha_enet)\n\n# choose a strength by cross-validation\nset.seed(102722)\ncvout_multi &lt;- cv.glmnet(x = x_train, \n                   y = y_train_multi, \n                   family = 'multinomial',\n                   alpha = alpha_enet)\n\n# view results\ncvout\n\n\n\nStep 2: predictions\nThe predictions from this model are a set of proabilities, one per class:\n\npreds_multi &lt;- predict(fit_reg_multi, \n        s = cvout_multi$lambda.min, \n        newx = x_test,\n        type = 'response')\n\nas_tibble(preds_multi[, , 1]) \n\nIf we choose the most probable class as the prediction and cross-tabulate with the actual label, we end up with the following table:\n\npred_class &lt;- as_tibble(preds_multi[, , 1]) %&gt;% \n  mutate(row = row_number()) %&gt;%\n  pivot_longer(-row, \n               names_to = 'label',\n               values_to = 'probability') %&gt;%\n  group_by(row) %&gt;%\n  slice_max(probability, n = 1) %&gt;%\n  pull(label)\n\npred_tbl &lt;- table(pull(test_labels, mclass), pred_class)\n\npred_tbl\n\n\n\n\n\n\n\nImportant\n\n\n\nIf time, take a moment to discuss:\n\nWhat do you think of the overall accuracy?\nWhich classes are well-predicted and which are not?\nDo you prefer the logistic or multinomial regression and why?"
  },
  {
    "objectID": "pstat197a/activities/github-basics.html",
    "href": "pstat197a/activities/github-basics.html",
    "title": "GitHub basics",
    "section": "",
    "text": "This activity introduces GitHub repositories and basic Git actions; students will be expected to use these skills to access materials and complete assignments.\nObjectives:\nPrerequisites: completion of lab 1, particularly cloning the group sandbox repository."
  },
  {
    "objectID": "pstat197a/activities/github-basics.html#why-are-we-using-git-and-github",
    "href": "pstat197a/activities/github-basics.html#why-are-we-using-git-and-github",
    "title": "GitHub basics",
    "section": "Why are we using Git and GitHub?",
    "text": "Why are we using Git and GitHub?\nVersion control has many benefits, including the ability to track changes and contributions precisely, work in parallel with other contributors, revert to prior versions of files, keep track of issues, quickly share and disseminate work, and solicit user contributions from the coding public. Arguably, for all of these reasons and because of its widespread use, Git/GitHub is a must for data scientists.\nIn this class you’ll learn and practice some basics that will allow you to easily access course files, collaborate with each other, and efficiently submit your coursework. This should equip you to utilize a repository for efficient collaboration with your peers on your capstone project."
  },
  {
    "objectID": "pstat197a/activities/github-basics.html#basic-workflow",
    "href": "pstat197a/activities/github-basics.html#basic-workflow",
    "title": "GitHub basics",
    "section": "Basic workflow",
    "text": "Basic workflow\nIf I am working out of a repository and want to alter a file and make those changes available to anyone else accessing my repository, most of the time I need to:\n\ncreate/update local copies of repository files on my laptop;\nmake the desired change(s) locally;\nsend the changes back to the remote repository.\n\nTypically these steps are performed iteratively as work progresses – they are a basic workflow.\nWorkflow can be understood as a sequence of Git actions: actions that modify the repository files and/or metadata. The most basic sequence that accomplishes the above steps is:\n\ngit pull update the local repository (technically, fetch changes + merge changes from the remote repository);\ngit add stage file changes to be committed to the local repository;\ngit commit commit staged changes to the local repository;\ngit push send committed changes back to the remote repository.\n\nSometimes contributors take different or additional actions; the complexity of the Git actions required to make a change depends largely on repository settings, permissions, and agreements among collaborators about how workflow should be structured."
  },
  {
    "objectID": "pstat197a/activities/github-basics.html#basic-git-actions",
    "href": "pstat197a/activities/github-basics.html#basic-git-actions",
    "title": "GitHub basics",
    "section": "Basic Git actions",
    "text": "Basic Git actions\nHere you’ll make a local change and then push that change to the remote repository.\n\nPull\nThe first step to making a change is ensuring you have the most up-to-date version of the repository files.\n\n\n\n\n\n\nAction (individual)\n\n\n\nPull changes from the remote repository.\n\nIn your GitHub client, open the group sandbox repository and then look for a ‘Pull’ menu item.\nIf you are using GitHub desktop, you can alternatively ‘fetch origin’ first via a toolbar button. This will retrieve changes but without modifying local files, and if changes are detected, a button will appear in the main screen of the client to pull changes.\nIn the terminal: navigate to the root directory of the repository and git pull\n\n\n\nNow check the repository history to see what changes you just pulled. In GitHub Desktop, there is a history tab on the left-hand side that lists commits chronologically. Select a commit to view line-by-line differences for every file that was altered.\nYou should see two changes: that there is now a class-activity folder containing a copy of this activity; and the README file has been updated. Look at the differences on the readme file.\n\n\n\n\n\n\nRemark\n\n\n\nFetching vs. pulling\nFetching allows you to retrieve changes from the remote repository without merging them into your local repository. If there are commits that you haven’t merged, you can examine them before doing so in one of two ways:\n\nin the terminal, git diff main origin/main\nopen the remote repository on github.com and check the commit history (look for a clock icon with the number of commits in the upper right corner of the file navigator in the code menu); open any commit to see a line-by-line comparison of differences.\n\n\n\n\n\nMake changes\nNow that you have the most up-to-date version of all files, create a new markdown file in the class activity folder with a fun fact about you (or anything else if you’d rather) that you’ll upload to the repository.\n\n\n\n\n\n\nAction (individual)\n\n\n\nCreate a markdown file:\n\nIn RStudio, select File &gt; New File &gt; Markdown File\nAdd an ‘About Me’ or similar header (use one or more hashes # before the header text)\nWrite a fun fact about yourself\nSave the file as YOURGITHUBUSERNAME-about.md in the class activity folder\n\n\n\n\n\nStage and commit changes\nNow that your new file is ready to go you can stage the changes to be committed to the repository and create a commit.\nA commit is a bundle of changes that will be submitted to the repository along with a message briefly explaining the changes made. Your GitHub client will often fill in a default message such as ‘update FILENAME.EXT’.\n\n\n\n\n\n\nAction (individual)\n\n\n\nStage and commit:\n\nIn your client, look for a menu item to add or stage changes. By default any changes made to any file will be included. In GitHub Desktop, look for the ‘Changes’ menu next to ‘History’; you can stage changes by simply selecting or unselecting the checkbox next to each file that was altered.\n\nOr in the terminal: git add FILENAME\n\nOnce you have staged changes, look for a menu item to commit changes. Add a message and commit the changes. In GitHub Desktop, this appears at the bottom of the ‘Changes’ menu.\n\nOr in the terminal: git add -m \"your message here\"\n\n\n\n\nOften these actions are performed together. However, in some workflows it may make sense to stage changes incrementally and create commits that bundle several changes at once. For example, if you need to make an update that requires modifying files A, B, and C, it may make sense to edit and stage changes to A first, followed by B, followed by C, and create the commit only once the full update has been implemented.\n\n\nPush\nThe last step is to push your commit to the remote repository. However, as you will see in a moment, too many people trying to push changes at once can create some problems.\n\n\n\n\n\n\nAction (group)\n\n\n\n\nChoose one person at your table to push their changes. The very first person to do this will have no problems, since their local repository is up to date with the remote.\nThen choose someone else to try – use the main screen at your workstation if possible so everyone at the table can see. Since the first person modified the remote repository, the next person to push changes will no longer be up to date. Git will detect this and the push won’t go through.\nHave the second person update their local repository by pulling changes, and then try the push again. It should go through once their local is up to date with the remote.\nHave everyone at your table pull changes but do not push any additional commits.\n\n\n\nSo far everyone is working on independent files and there’s no overlap between changes, so although it would be a bit of a hassle to have everyone check for changes every time they push, in principle it could be done. However, there is a more efficient way to work in parallel: by creating branches."
  },
  {
    "objectID": "pstat197a/activities/github-basics.html#branching",
    "href": "pstat197a/activities/github-basics.html#branching",
    "title": "GitHub basics",
    "section": "Branching",
    "text": "Branching\nInspect your GitHub client closely, and note that you are currently on the ‘main’ branch of the repository. Think of this as the primary version of the repository. Branches allow contributors to create parallel versions of the repository that they can modify for development purposes while leaving the primary version unaffected.\n\nCreate a branch\nHere you’ll use branches to avoid stepping on each others’ toes while pushing your table’s remaining commits. The strategy will be to create a personal branch, push your commit to that branch, and then merge the branch back into the main branch of the repository.\n\n\n\n\n\n\nAction (individual)\n\n\n\nCreate a branch and push your previous commit:\n\nIn your GitHub client, look for a menu item to create and switch to a new branch.\nName your branch your GitHub username.\nCheck to see that you are currently on your personal branch.\nPush your previous commit. You shouldn’t have to repeat any of the previous steps, but you can if need be.\n\nIf you were one of the two who pushed their commit to main, make some small change to your file to push to your personal branch.\n\n\n\n\nAccess your neighbor’s branch\nWhile often the main purpose of branching is to create a version of the repository that only you will modify, contributors can inspect any branch of the repository. This can be useful for sharing ideas or getting input or help.\n\n\n\n\n\n\nAction (in pairs)\n\n\n\nMake a commit to your neighbor’s branch\n\nFind out your neighbor’s username and switch to their branch in your GitHub client.\nIn RStudio, verify that you are on their branch by executing git status in the terminal.\nOpen their markdown file, ask them a simple question about themselves (nothing too personal, please), and add the information to their markdown file.\nStage, commit, and push the change.\nWhen your neighbor has done the same with you, switch back to your own branch in your GitHub client and pull changes."
  },
  {
    "objectID": "pstat197a/activities/github-basics.html#pull-requests",
    "href": "pstat197a/activities/github-basics.html#pull-requests",
    "title": "GitHub basics",
    "section": "Pull requests",
    "text": "Pull requests\nOnce you are ready to integrate changes you’ve developed on a branch you can open a pull request to merge the development branch with the main branch. (Technically, pull requests can be opened between any two branches, so could also be used, for example, to update your branch if the main branch has new commits.)\n‘Pull request’ is a bit of an odd term; think of it as you making a request that your collaborators pull your changes for review.\n\n\n\n\n\n\nAction (in pairs)\n\n\n\nOpen a pull request:\n\nIn your GitHub client, find a menu item for opening a pull request. GitHub Desktop will simply redirect you to github.com to open the request.\nSpecify the pull request from your branch to the main branch and submit.\n\n\n\nOnce a pull request is opened, usually a collaborator with maintain privileges must be the one to merge changes and close the request. However, the rules for this depend on repository settings. For this repository, all contributors can merge and close pull requests.\n\n\n\n\n\n\nAction (in pairs)\n\n\n\n\nOpen the repository in the browser. Navigate to pull requests.\nFind your neighbor’s pull request; merge their changes and close the request. Then delete the branch.\n\n\n\nOnce everyone at the table is finished, examine the repository on the main screen and verify that everyone’s markdown file is present on the main branch. Then have each contributor pull changes and check that they see the same."
  },
  {
    "objectID": "pstat197a/activities/github-basics.html#merge-conflicts",
    "href": "pstat197a/activities/github-basics.html#merge-conflicts",
    "title": "GitHub basics",
    "section": "Merge conflicts",
    "text": "Merge conflicts\nGit is pretty clever at merging changes when you pull, push, or merge branches via pull request. However, occasionally commits will conflict in such a way that can’t be resolved automatically. These are known as merge conflicts.\nMerge conflicts happen when:\n\ntwo commits differ on the same line of the same file;\nfiles are moved or deleted in conflicting ways.\n\nHere you’ll create an artificial merge conflict to see what this looks like and how to fix it.\n\n\n\n\n\n\nAction (group)\n\n\n\nCreate a merge conflict\nEnsure the workstation at your table is up to date with the remote repository. Then:\n\nHave someone at your table open the README file and add the group members’ names in a list on one line, e.g.,\ngroup: trevor ruiz, yan lashchev\nCommit and push changes\nThen on the main screen, without pulling new changes, create a commit with the names shown differently somehow, such as last, first, or initials, or spanning multiple lines with one name per line.\nAttempt to push the commit. Your client will detect ‘upstream’ changes on the remote repository and prompt you to pull changes.\nAttempt to pull the changes. The client will then report a merge conflict and prompt you to resolve the conflict and commit changes. GitHub Desktop in particular will prompt you to open RStudio to resolve the conflict. Go ahead and follow the prompt.\n\nResolve a merge conflict\nYou will see a version of the file with the conflict that shows &lt;&lt;&lt;&lt;HEAD … &gt;&gt;&gt;&gt; followed by a long alphanumeric string. Within the angle brackets the two conflicting versions of the file will be shown, separated by ===== .\n\nAgree with your table on one version of the README file (or another representation of your names).\nCommit and push the change.\n\n\n\nWhen detected, merge conflicts must be resolved with a commit that takes precedence over the conflicting commits. You can read more about resolving merge conflicts here."
  },
  {
    "objectID": "pstat197a/activities/github-basics.html#checklist",
    "href": "pstat197a/activities/github-basics.html#checklist",
    "title": "GitHub basics",
    "section": "Checklist",
    "text": "Checklist\n\nOn github.com, your group-sandbox repository has a directory called class-activity containing a copy of this activity and one markdown file for each group member with two fun facts about them.\nThe repository has only one open branch.\nThe README file lists each group member’s name.\nEach group member has an up-to-date local copy of the repository."
  },
  {
    "objectID": "pstat197a/slides/week2-classdata.html#lessons-from-last-time",
    "href": "pstat197a/slides/week2-classdata.html#lessons-from-last-time",
    "title": "Sampling concepts and descriptive analysis",
    "section": "Lessons from last time",
    "text": "Lessons from last time\n\nadd .DS_Store to .gitignore\nopen repo project in RStudio session (not another project or new session)\nrepo clone directory must be kept intact; can move the entire directory but not individual files\nuse client not terminal, at least to start out\nothers?"
  },
  {
    "objectID": "pstat197a/slides/week2-classdata.html#today",
    "href": "pstat197a/slides/week2-classdata.html#today",
    "title": "Sampling concepts and descriptive analysis",
    "section": "Today",
    "text": "Today\n\nreview sampling concepts\nintroduce class survey data\npresent descriptive analysis"
  },
  {
    "objectID": "pstat197a/slides/week2-classdata.html#samples-and-populations",
    "href": "pstat197a/slides/week2-classdata.html#samples-and-populations",
    "title": "Sampling concepts and descriptive analysis",
    "section": "Samples and populations",
    "text": "Samples and populations\npopulation: collection of all subjects/units of interest\nsample: subjects/units observed in a study\n\nstatistical methodology strives to account for the possibility that the sample could have been different in order to make reliable inferences about the population based on knowledge of the sampling mechanism"
  },
  {
    "objectID": "pstat197a/slides/week2-classdata.html#what-if-inferences-arent-possible",
    "href": "pstat197a/slides/week2-classdata.html#what-if-inferences-arent-possible",
    "title": "Sampling concepts and descriptive analysis",
    "section": "What if inferences aren’t possible?",
    "text": "What if inferences aren’t possible?\nEven if inference isn’t possible, data still have value and could be used for:\n\ndescriptive analysis of the sample;\nhypothesis generation;\ndeveloping analysis pipelines."
  },
  {
    "objectID": "pstat197a/slides/week2-classdata.html#what-about-prediction",
    "href": "pstat197a/slides/week2-classdata.html#what-about-prediction",
    "title": "Sampling concepts and descriptive analysis",
    "section": "What about prediction?",
    "text": "What about prediction?\nPrediction is a separate goal but still a form of generalization.\n\nsamples must reflect a broader population for predictions to be accurate at the population level\n\n\n\nif an analyst can’t expect sample statistics to provide reliable estimates of population quantities, they shouldn’t expect predictions based on the sample to be reliable either"
  },
  {
    "objectID": "pstat197a/slides/week2-classdata.html#common-problems",
    "href": "pstat197a/slides/week2-classdata.html#common-problems",
    "title": "Sampling concepts and descriptive analysis",
    "section": "Common problems",
    "text": "Common problems\nSeveral issues arise very often in practice that compromise or complicate an analyst’s ability to make inferences (or predictions). Among them:\n\nscope of inference from the sample doesn’t match the study population\nsubjects/units are selected haphazardly or by convenience\nresearcher conflates sample size with number of observations, i.e., takes lots of measurements on few subjects/units"
  },
  {
    "objectID": "pstat197a/slides/week2-classdata.html#helpful-questions",
    "href": "pstat197a/slides/week2-classdata.html#helpful-questions",
    "title": "Sampling concepts and descriptive analysis",
    "section": "Helpful questions",
    "text": "Helpful questions\nThe following questions can help make an assessment of the scope of inference:\n\n(protocol) how were subjects/units chosen for measurement and how were measurements collected?\n(mechanism) was there any random selection mechanism?\n(exclusion) are there any subjects/units that couldn’t possibly have been chosen?\n(nonresponse) were any subjects/units selected but not measured?"
  },
  {
    "objectID": "pstat197a/slides/week2-classdata.html#class-survey-data",
    "href": "pstat197a/slides/week2-classdata.html#class-survey-data",
    "title": "Sampling concepts and descriptive analysis",
    "section": "Class survey data",
    "text": "Class survey data\n\nsurvey distributed to all students offered enrollment in PSTAT197A fall 2022\n\\(n = 65\\) responses\n\nincludes a few students who did not enroll\ndoes not include several students who did not enroll\ndoes not include one student who enrolled late\n\nno random selection"
  },
  {
    "objectID": "pstat197a/slides/week2-classdata.html#can-the-data-support-inference",
    "href": "pstat197a/slides/week2-classdata.html#can-the-data-support-inference",
    "title": "Sampling concepts and descriptive analysis",
    "section": "Can the data support inference?",
    "text": "Can the data support inference?\nFrom the reading responses:\n\nIt depends on the question. If you want to draw conclusions about the pstat197a class specifically, this sample is the population and thus will have reliable data. If you want to draw conclusions about the pstat department as a whole, then this is a bad sample because it is likely biased and thus unreliable"
  },
  {
    "objectID": "pstat197a/slides/week2-classdata.html#alternative-perspectives",
    "href": "pstat197a/slides/week2-classdata.html#alternative-perspectives",
    "title": "Sampling concepts and descriptive analysis",
    "section": "Alternative perspectives",
    "text": "Alternative perspectives\nThe comment points to two ways to view the data:\n\na census of PSTAT197A enrollees\na convenience sample of…\n\ncapstone applicants OR\nstudents qualified for capstones OR\nstudents interested in data science OR\nall UCSB students???"
  },
  {
    "objectID": "pstat197a/slides/week2-classdata.html#is-there-a-right-answer",
    "href": "pstat197a/slides/week2-classdata.html#is-there-a-right-answer",
    "title": "Sampling concepts and descriptive analysis",
    "section": "Is there a right answer?",
    "text": "Is there a right answer?\nEither way – census or convenience sample – excludes inference.\n\ncensus \\(\\longrightarrow\\) no inference needed\nconvenience \\(\\longrightarrow\\) no inference possible\n\n\nSo on a practical level, it won’t make much difference for designing an analysis of the survey data."
  },
  {
    "objectID": "pstat197a/slides/week2-classdata.html#descriptive-analysis",
    "href": "pstat197a/slides/week2-classdata.html#descriptive-analysis",
    "title": "Sampling concepts and descriptive analysis",
    "section": "Descriptive analysis",
    "text": "Descriptive analysis\nAny analysis of survey data should be regarded as descriptive in nature:\n\nsummary statistics and/or models are not reliable measures of any broader population\nresults should be interpreted narrowly in terms of the sample at hand"
  },
  {
    "objectID": "pstat197a/slides/week2-classdata.html#a-general-approach",
    "href": "pstat197a/slides/week2-classdata.html#a-general-approach",
    "title": "Sampling concepts and descriptive analysis",
    "section": "A general approach",
    "text": "A general approach\nStart simple and add complexity gradually.\nFrom simpler to more complex consider questions involving:\n\nSample characteristics\nSingle-variable summaries\nMultivariate summaries\nModel-based outputs (estimates, predictions, etc.)"
  },
  {
    "objectID": "pstat197a/slides/week2-classdata.html#questions-of-interest",
    "href": "pstat197a/slides/week2-classdata.html#questions-of-interest",
    "title": "Sampling concepts and descriptive analysis",
    "section": "Questions of interest",
    "text": "Questions of interest\nSample characteristics\n\nIs the proportion of men/women in the class equal (taking into account randomness)?\n\nSingle-variable summaries\n\nAmong the students offered a seat in PSTAT197, what fields of study are the students most interested in?\nWhat level of comfort do students interested in data analysis at UCSB have with mathematics?"
  },
  {
    "objectID": "pstat197a/slides/week2-classdata.html#questions-of-interest-1",
    "href": "pstat197a/slides/week2-classdata.html#questions-of-interest-1",
    "title": "Sampling concepts and descriptive analysis",
    "section": "Questions of interest",
    "text": "Questions of interest\nMultivariate summaries\n\nAre students who ranked themselves as strong in statistics, mathematics, and computing more likely or less likely to select an ‘industry’ project as the project type that they want to work on?\n\nModel-based outputs\n\nAre there distinct groups of students in the class defined by self-assessed proficiencies and/or comfort levels with mathematics, statistics, and programming?"
  },
  {
    "objectID": "pstat197a/slides/week2-classdata.html#sample-characteristics",
    "href": "pstat197a/slides/week2-classdata.html#sample-characteristics",
    "title": "Sampling concepts and descriptive analysis",
    "section": "Sample characteristics",
    "text": "Sample characteristics\nIs the proportion of men/women in the class equal (taking into account randomness)?\n\nClass standingGenderRaceData sharing\n\n\n\n\n\n\n\nstanding\nn\n\n\n\n\nJunior\n9\n\n\nSenior\n56\n\n\n\n\n\n\n\n\n\n\n\n\ngender\nn\n\n\n\n\nFemale\n25\n\n\nMale\n40\n\n\n\n\n\n\n\n\n\n\n\n\nrace\nn\n\n\n\n\nAsian\n41\n\n\nCaucasian\n17\n\n\nPrefer not to say\n6\n\n\nUnknown\n1\n\n\n\n\n\n\n\nColumns: consent to share project preferences\nRows: consent to share background and preparation\n\n\n\n\n\n\nNo\nYes\n\n\n\n\nNo\n3\n3\n\n\nYes\n2\n57"
  },
  {
    "objectID": "pstat197a/slides/week2-classdata.html#majors",
    "href": "pstat197a/slides/week2-classdata.html#majors",
    "title": "Sampling concepts and descriptive analysis",
    "section": "Majors",
    "text": "Majors"
  },
  {
    "objectID": "pstat197a/slides/week2-classdata.html#response-timing",
    "href": "pstat197a/slides/week2-classdata.html#response-timing",
    "title": "Sampling concepts and descriptive analysis",
    "section": "Response timing",
    "text": "Response timing"
  },
  {
    "objectID": "pstat197a/slides/week2-classdata.html#privacy",
    "href": "pstat197a/slides/week2-classdata.html#privacy",
    "title": "Sampling concepts and descriptive analysis",
    "section": "Privacy",
    "text": "Privacy\nThe following information have been removed from the dataset distributed to the class:\n\npersonal information from section 1 of the survey\nlong text and free response answers, contain some personal details\nresponses from students who did not consent to share\ntype distinction between research experiences"
  },
  {
    "objectID": "pstat197a/slides/week2-classdata.html#single-variable-summaries",
    "href": "pstat197a/slides/week2-classdata.html#single-variable-summaries",
    "title": "Sampling concepts and descriptive analysis",
    "section": "Single-variable summaries",
    "text": "Single-variable summaries\nWhat level of comfort do students interested in data analysis at UCSB have with mathematics?\n\nComfortProficiency (numeric)Proficiency (factor)\n\n\n\n\n\n\n\nvariable\nmax\nmean\nmedian\nmin\n\n\n\n\nmath.comf\n5\n3.847458\n4\n2\n\n\nprog.comf\n5\n3.966102\n4\n3\n\n\nstat.comf\n5\n4.084746\n4\n2\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nmean\nmedian\n\n\n\n\nmath\n2.355932\n2\n\n\nprog\n2.237288\n2\n\n\nstat\n2.576271\n3\n\n\n\n\n\n\n\n\n\n\n\n\nprog\nn1\nmath\nn2\nstat\nn3\n\n\n\n\nBeg\n3\nBeg\n3\nBeg\n2\n\n\nInt\n39\nInt\n32\nInt\n21\n\n\nAdv\n17\nAdv\n24\nAdv\n36"
  },
  {
    "objectID": "pstat197a/slides/week2-classdata.html#multivariable-summaries",
    "href": "pstat197a/slides/week2-classdata.html#multivariable-summaries",
    "title": "Sampling concepts and descriptive analysis",
    "section": "Multivariable summaries",
    "text": "Multivariable summaries\nAre students who ranked themselves as strong in statistics, mathematics, and computing more likely or less likely to select an ‘industry’ project as the project type that they want to work on?\n\nCountsProportions\n\n\n\n\n\n\n\nmean.proficiency.fac\nboth\nind\nlab\n\n\n\n\n[1,2.33]\n6\n25\n1\n\n\n(2.33,2.67]\n5\n9\n1\n\n\n(2.67,3]\n3\n6\n1\n\n\n\n\n\n\n\n\n\n\n\n\nmean.proficiency.fac\nboth\nind\nlab\nn\n\n\n\n\n[1,2.33]\n0.188\n0.781\n0.031\n32\n\n\n(2.33,2.67]\n0.333\n0.600\n0.067\n15\n\n\n(2.67,3]\n0.300\n0.600\n0.100\n10"
  },
  {
    "objectID": "pstat197a/slides/week2-classdata.html#combinations",
    "href": "pstat197a/slides/week2-classdata.html#combinations",
    "title": "Sampling concepts and descriptive analysis",
    "section": "Combinations",
    "text": "Combinations\nConsider the distinct combinations of comfort and proficiency ratings (separately):\n\nProficiencyComfort\n\n\n\n\n\n\n\nprog\nmath\nstat\nn\n\n\n\n\n1\n1\n1\n1\n\n\n1\n2\n2\n1\n\n\n1\n2\n3\n1\n\n\n2\n1\n1\n1\n\n\n2\n1\n2\n1\n\n\n2\n2\n2\n13\n\n\n2\n2\n3\n11\n\n\n2\n3\n2\n3\n\n\n2\n3\n3\n10\n\n\n3\n2\n2\n2\n\n\n3\n2\n3\n4\n\n\n3\n3\n2\n1\n\n\n3\n3\n3\n10\n\n\n\n\n\n\n\n\n\n\n\n\nprog\nmath\nstat\nn\n\n\n\n\n3\n2\n2\n1\n\n\n3\n3\n3\n2\n\n\n3\n3\n4\n2\n\n\n3\n3\n5\n2\n\n\n3\n4\n3\n4\n\n\n3\n4\n4\n7\n\n\n4\n3\n3\n2\n\n\n4\n3\n4\n5\n\n\n4\n3\n5\n2\n\n\n4\n4\n3\n1\n\n\n4\n4\n4\n5\n\n\n4\n4\n5\n3\n\n\n4\n5\n3\n1\n\n\n4\n5\n4\n4\n\n\n4\n5\n5\n2\n\n\n5\n3\n4\n5\n\n\n5\n3\n5\n1\n\n\n5\n4\n4\n2\n\n\n5\n4\n5\n1\n\n\n5\n5\n4\n1\n\n\n5\n5\n5\n6"
  },
  {
    "objectID": "pstat197a/slides/week2-classdata.html#clustering",
    "href": "pstat197a/slides/week2-classdata.html#clustering",
    "title": "Sampling concepts and descriptive analysis",
    "section": "Clustering",
    "text": "Clustering\nCan students be grouped based on combinations of preferences and comfort levels?\n\nCentersVisualizationMethodInterpretation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprog.prof\nmath.prof\nstat.prof\nprog.comf\nmath.comf\nstat.comf\nsize\ncluster\n\n\n\n\n2.478\n2.739\n2.957\n4.435\n4.522\n4.522\n23\n1\n\n\n2.048\n1.857\n2.238\n4.048\n3.048\n4.048\n21\n2\n\n\n2.133\n2.467\n2.467\n3.133\n3.933\n3.467\n15\n3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClustering method, “k means”, groups data by nearest Euclidean distance to each of \\(k\\) centers. \\(k\\) is user-specified; the method finds the centers that minimize within-cluster variance.\n\n\nBased on the centers:\n\nCluster 1: advanced proficiency, very comfortable\nCluster 2: intermediate with less mathematical preparation\nCluster 3: intermediate with less programming preparation"
  },
  {
    "objectID": "pstat197a/slides/week2-classdata.html#assignment",
    "href": "pstat197a/slides/week2-classdata.html#assignment",
    "title": "Sampling concepts and descriptive analysis",
    "section": "Assignment",
    "text": "Assignment\nYour task is to extend this analysis with your group by next Tuesday.\n\nHere are some ideas:\n\nexplore variable associations further (e.g., coursework and self-evaluations)\nexperiment with clustering on different variable subsets or using different methods\nsummarize domain or area of interest variables (requires some text manipulation)"
  },
  {
    "objectID": "pstat197a/slides/week2-classdata.html#next-time",
    "href": "pstat197a/slides/week2-classdata.html#next-time",
    "title": "Sampling concepts and descriptive analysis",
    "section": "Next time",
    "text": "Next time\nMost of next meeting we’ll devote to planning your group’s task.\n\nDo a little brainstorming on your own\nCome with a few questions/ideas"
  },
  {
    "objectID": "pstat197a/slides/week10-projects.html#announcementsreminders",
    "href": "pstat197a/slides/week10-projects.html#announcementsreminders",
    "title": "Capstone projects",
    "section": "Announcements/reminders",
    "text": "Announcements/reminders\n\ncome to class next time prepared to show a draft of your vignette\nafter class today look for abstracts on the course site\noffice hours in place of section meetings this Wednesday"
  },
  {
    "objectID": "pstat197a/slides/week10-projects.html#amgen",
    "href": "pstat197a/slides/week10-projects.html#amgen",
    "title": "Capstone projects",
    "section": "Amgen",
    "text": "Amgen\nAmgen is an international biotechnology company.\n\nProject: evaluation of natural language processing algorithms used for knowledge graph generation\n\nNLP algorithms are used to recognize and link entities and identify relations based on text data\noutputs can be used to extract networks (‘knowledge graphs’) from text corpora\n\n\n\nGoals: generate a benchmarking dataset from PubMed database and evaluate performance of NLP-based methods for knowledge graph construction\n\n\nSee this example of related work."
  },
  {
    "objectID": "pstat197a/slides/week10-projects.html#appfolio",
    "href": "pstat197a/slides/week10-projects.html#appfolio",
    "title": "Capstone projects",
    "section": "Appfolio",
    "text": "Appfolio\nAppfolio is a local property management software company. Among other things, clients use their software for accounting purposes.\n\nProject: anomaly detection from property management transaction histories\n\nclients would like to flag smaller/higher transactions than typical without having to inspect full transaction records each month\n\n\n\nGoals: determine applicable anomaly/outlier detection methods, evaluate performance, and develop dashboard based on best method(s)"
  },
  {
    "objectID": "pstat197a/slides/week10-projects.html#calcofi",
    "href": "pstat197a/slides/week10-projects.html#calcofi",
    "title": "Capstone projects",
    "section": "CalCOFI",
    "text": "CalCOFI\nCalCOFI stands for California Cooperative Oceanic Fisheries Investigations. They run a long-term monitoring program of the California current ecosystem.\n\nProject: an eDNA window into larval fish habitat, ecosystem structure, and function\n\nCalCOFI collects physical data and environmental DNA (eDNA) across depth in the water column at multiple monitoring sites longitudinally\na major interest is on impacts of environmental conditions on fisheries\n\n\n\nGoals: develop dashboard for exploration of eDNA data and develop a model for prediction of fish larvae based on eDNA and physical data (or derived variables)\n\n\nSee last year’s project."
  },
  {
    "objectID": "pstat197a/slides/week10-projects.html#carpe-data",
    "href": "pstat197a/slides/week10-projects.html#carpe-data",
    "title": "Capstone projects",
    "section": "Carpe Data",
    "text": "Carpe Data\nCarpe Data is a local company focusing on data-driven solutions for insurance carriers.\n\nProject: business characteristics classification models\n\ninsurance carriers use risk categorization as a factor in determining premiums for property loss and general liability insurance for businesses\n\n\n\nGoals: classify businesses according to characteristics and/or risk levels based on basic business information (name, description, hours, images, etc.) and develop software pipelines for preprocessing and prediction"
  },
  {
    "objectID": "pstat197a/slides/week10-projects.html#caves-visual-ecology-lab",
    "href": "pstat197a/slides/week10-projects.html#caves-visual-ecology-lab",
    "title": "Capstone projects",
    "section": "Caves visual ecology lab",
    "text": "Caves visual ecology lab\n\n\n\n\n\nExample closeup of bee eye.\n\n\n\nThe Caves lab studies visual acuity and its evolutionary and ecological drivers in animals. Bees are great model organisms for studying the relationship between ecology and acuity due to wide variation in lifestyles and ecologies.\n\nProject: measuring visual acuity in bees from high-resolution images\n\nacuity quantification is derived from physical measurements that could potentially be inferred from photographs rather than measured directly\nspecifically, radius of curvature of the eye and width of ommatidia (compound eye facets)\n\nGoals: utilize computer vision techniques to infer acuity measurements from photographs and merge with ecological data to explore correlates"
  },
  {
    "objectID": "pstat197a/slides/week10-projects.html#ccber",
    "href": "pstat197a/slides/week10-projects.html#ccber",
    "title": "Capstone projects",
    "section": "CCBER",
    "text": "CCBER\nThe Cheadle Center for Biodiversity and Ecological Restoration (CCBER) contributes to the Big Bee Project, aimed at creating &gt;1M 2D and 3D high-resolution images of bees for the study of anatomical variation.\n\nProject: constructing three-dimensional bee models from high-resolution images\n\nseveral software tools for constructing 3D models from 2D images are available, but performance with bees specifically is not well understood\nseveral image sets of ~100 photographs each are available for construction of models\n\n\n\nGoals: after receiving training on 3D modeling tools, students will experiment with parameter tuning for optimal rendering of bee models; students will then derive physical measurements from the models."
  },
  {
    "objectID": "pstat197a/slides/week10-projects.html#climate-hazards-center",
    "href": "pstat197a/slides/week10-projects.html#climate-hazards-center",
    "title": "Capstone projects",
    "section": "Climate Hazards Center",
    "text": "Climate Hazards Center\nThe Climate Hazards Center housed in the geography department is a multidisciplinary research center focusing on climate risk analysis and response.\n\nProject 1: identifying the drivers of food insecurity in the developing world\n\nprecipitation and potential evapotranspiration together can help identify deficits in available water and risk of food shortages\nexplore the relationship between precipitation and potential evapotranspiration globally over time and identify drivers after accounting for relationship\n\n\n\nProject 2: evaluating and validating station- and satellite-based daily precipitation datasets\n\nCHC supports precipitation datasets based on interpolating measurements from satellites and ground stations\nnew data releases go through an evaluation/validation process with benchmarking data prior to release\nstudents will carry out evaluation/validation of a new release based on prior strategies"
  },
  {
    "objectID": "pstat197a/slides/week10-projects.html#eembpatrick-green",
    "href": "pstat197a/slides/week10-projects.html#eembpatrick-green",
    "title": "Capstone projects",
    "section": "EEMB/Patrick Green",
    "text": "EEMB/Patrick Green\n\n\n\n\nPatrick Green is a research scientist in EEMB studying animal behavior and competition, and is developing a pilot project studying contests among mantis shrimp.\n\nProject: how do mantis shrimp fight in a community of competitors?\n\nfeasible to carry out continuous video monitoring of small populations in tanks to capture interactions and other behavior\ntime-consuming to review footage\nnot obvious how to define/code interactions\n\nGoal: develop heuristic methodology for detecting time intervals in which interactions occur; generate tracking data summarizing movements of each individual."
  },
  {
    "objectID": "pstat197a/slides/week10-projects.html#evidation-health",
    "href": "pstat197a/slides/week10-projects.html#evidation-health",
    "title": "Capstone projects",
    "section": "Evidation Health",
    "text": "Evidation Health\nEvidation is a California-based company focusing on health data analytics for individuals and for researchers.\n\nProject: Impact of case definition on early detection systems for COVID-19\n\nmodel-based early detection systems for infectious disease use inconsistent criteria for case onset\nthe definition of when a case starts may impact the efficacy and other features of early detection systems\n\n\n\nGoals: assess the impact of case onset definition on existing early detection models and find optimal case onset points."
  },
  {
    "objectID": "pstat197a/slides/week10-projects.html#inogen",
    "href": "pstat197a/slides/week10-projects.html#inogen",
    "title": "Capstone projects",
    "section": "Inogen",
    "text": "Inogen\nInogen is a medical device company that builds portable oxygen concentrators.\n\nProject: analysis of portable oxygen concentrator patient use data\n\nInogen collects a variety of data on patient use of their POC devices and is interested in identifying areas of potential improvement\n\n\n\nGoals: identify patterns of device use from patient data with particular focus on adherence."
  },
  {
    "objectID": "pstat197a/slides/week10-projects.html#move-lab",
    "href": "pstat197a/slides/week10-projects.html#move-lab",
    "title": "Capstone projects",
    "section": "MOVE lab",
    "text": "MOVE lab\nThe MOVE lab at UCSB focuses on movement data science in general and in particular human mobility in response to disruptions.\n\nProject: detecting changes in human mobility and movement patterns associated with wildfires in California\n\nmovement and mobility are often markers of behavior and changes in movement patterns may capture information about behavioral responses to events\nnatural disaster in general and wildfire in particular are likely to produce shifts in movement patterns\n\n\n\nGoals: assess suitability of several candidate datasets for studying behavioral responses to wildfires; identify movement patterns and explore the hypothesis that change points occur in connection with wildfire events; potentially explore demographic covariates."
  },
  {
    "objectID": "pstat197a/slides/week10-projects.html#peak-performance-project-p3",
    "href": "pstat197a/slides/week10-projects.html#peak-performance-project-p3",
    "title": "Capstone projects",
    "section": "Peak Performance Project (P3)",
    "text": "Peak Performance Project (P3)\nP3 is a local company focusing on applied sports science and technology for biomechanical analysis of athletic performance.\n\nProject: understanding links between biomechanical data and on-court NBA production\n\nP3 collects biomechanical data – force plate and motion capture – on professional athletes and maintains a large proprietary database on NBA athletes\nhistorically, have focused on injury risk, but interested in finding biomechanical correlates of real performance\n\n\n\nGoals: scrape publicly available on-court data, merge with biomechanical data, and identify correlates of on-court production; develop visualization tools."
  },
  {
    "objectID": "pstat197a/slides/week10-projects.html#slac-national-accelerator-lab",
    "href": "pstat197a/slides/week10-projects.html#slac-national-accelerator-lab",
    "title": "Capstone projects",
    "section": "SLAC National Accelerator Lab",
    "text": "SLAC National Accelerator Lab\nStanford Synchrotron Ratiation Lightsource (SSRL) is a DOE facility at Stanford supporting a wide range of fundamental research involving bright X-rays.\n\nProject: diffraction image selector\n\nX-ray diffraction data provides insight into the atomic and molecular structure of crystals; serial crystallography involves merging diffraction patterns from several crystals in order to analyze the material structure\nexperimenters tend to select diffraction images manually from serial experiments; this selection has an impact on experimental outputs\n\n\n\nGoals: build a regression model to label images during data collection in real time; develop model from simulated data and validate on real data."
  },
  {
    "objectID": "pstat197a/slides/week10-projects.html#project-preferences",
    "href": "pstat197a/slides/week10-projects.html#project-preferences",
    "title": "Capstone projects",
    "section": "Project Preferences",
    "text": "Project Preferences\nPlease read abstracts first and then fill out the preference form by Friday 12/2.\n\n3 top lab choices\n3 top industry choices\n\n\nWe’ll try to accommodate preferences, but we can’t guarantee you’ll get your top choices.\n\n\nTarget date for assignments: Friday 12/9."
  },
  {
    "objectID": "pstat197a/slides/week2-workshop.html#today",
    "href": "pstat197a/slides/week2-workshop.html#today",
    "title": "Group assignment workshops",
    "section": "Today",
    "text": "Today\n\nfinish discussing results from last time\nsetup for first group assignment due next Friday\n\nassignment objectives and instructions\nreview repository\n\nworkshop ideas and plan tasks in groups"
  },
  {
    "objectID": "pstat197a/slides/week2-workshop.html#assignment-objective",
    "href": "pstat197a/slides/week2-workshop.html#assignment-objective",
    "title": "Group assignment workshops",
    "section": "Assignment objective",
    "text": "Assignment objective\nYour task: prepare and present a descriptive analysis of the survey responses addressing 2-3 questions or goals of your choosing.\n\nquestions or goals should be of moderate complexity; easy to state and understand but should require a little work to answer\n\ntoo simple: what proportion of students have research experience?\nbetter: are students with research experience more confident/comfortable with technical skills than students without, considering coursework history?"
  },
  {
    "objectID": "pstat197a/slides/week2-workshop.html#outcomes",
    "href": "pstat197a/slides/week2-workshop.html#outcomes",
    "title": "Group assignment workshops",
    "section": "Outcomes",
    "text": "Outcomes\nThe learning outcomes for this assignment are less focused on methodology:\n\nformulate questions and plan a simple analysis\npractice using a GitHub repo in a team project setting\nprepare a report\nlearn one or more new-to-you data manipulation techniques"
  },
  {
    "objectID": "pstat197a/slides/week2-workshop.html#assignment-set-up",
    "href": "pstat197a/slides/week2-workshop.html#assignment-set-up",
    "title": "Group assignment workshops",
    "section": "Assignment set-up",
    "text": "Assignment set-up\n\naccept GH classroom assignment here; this will create/add your team repo\nclone repo to local\nreview repo contents:\n\ndata with survey responses and metadata\nscripts with preprocessing and in-class analysis\nresults with report template for preparing write-up\nREADME.md with assignment instructions"
  },
  {
    "objectID": "pstat197a/slides/week2-workshop.html#deliverable",
    "href": "pstat197a/slides/week2-workshop.html#deliverable",
    "title": "Group assignment workshops",
    "section": "Deliverable",
    "text": "Deliverable\nYour ‘submission’ will be in the form of commits to the group repository, in particular:\n\nan updated results/report.qmd file containing your write-up source\na rendered results/report.html file\n\n\nPlease make final commits by Friday, October 14, 11:59pm PST.\n\n\nRecall there is a 24-hour grace period; any commits submitted after Saturday 11:59pm may not receive review."
  },
  {
    "objectID": "pstat197a/slides/week2-workshop.html#resources",
    "href": "pstat197a/slides/week2-workshop.html#resources",
    "title": "Group assignment workshops",
    "section": "Resources",
    "text": "Resources\n\nlab 2 and in-class analysis scripts/week2-surveys.R\nslack and course staff OH\nMDSR ch. 4 on data wrangling\nRStudio cheatsheets and tidyverse documentation, esp. dplyr, tidyr, ggplot2"
  },
  {
    "objectID": "pstat197a/slides/week2-workshop.html#workshopping-today",
    "href": "pstat197a/slides/week2-workshop.html#workshopping-today",
    "title": "Group assignment workshops",
    "section": "Workshopping today",
    "text": "Workshopping today\nTry to accomplish three goals:\n\npool ideas for questions or themes to explore in the data\npair up, divide work, and assign tasks\nagree on a communication plan for finishing work\n\nslack groupchat or similar\nmeeting outside of class sometime next week"
  },
  {
    "objectID": "pstat197a/slides/week5-multinomial.html#from-last-time",
    "href": "pstat197a/slides/week5-multinomial.html#from-last-time",
    "title": "Multinomial logistic regression",
    "section": "From last time",
    "text": "From last time\nLast time we ended having constructed a TF-IDF document term matrix for the claims data.\n\n\\(n = 552\\) observations\n\\(p = 15,868\\) variables (word tokens)\nbinary (rather than multiclass) labels\n\n\n\n\n# A tibble: 552 × 15,870\n   .id    bclass    adams afternoon  agent android    app arkansas arrest arrive\n   &lt;chr&gt;  &lt;fct&gt;     &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 url1   relevant 0.0692    0.0300 0.0365  0.0450 0.0330   0.0390 0.0140 0.0305\n 2 url10  irrelev… 0         0      0       0      0        0      0      0     \n 3 url100 irrelev… 0         0      0       0      0        0      0      0     \n 4 url101 relevant 0         0      0       0      0        0      0      0     \n 5 url102 relevant 0         0      0       0      0        0      0      0     \n 6 url105 relevant 0         0      0       0      0        0      0      0     \n 7 url106 irrelev… 0         0      0       0      0        0      0      0     \n 8 url107 irrelev… 0         0      0       0      0        0      0      0     \n 9 url108 relevant 0         0      0       0      0        0      0      0     \n10 url109 irrelev… 0         0      0       0      0        0      0      0     \n# … with 542 more rows, and 15,860 more variables: assist &lt;dbl&gt;, attic &lt;dbl&gt;,\n#   barricade &lt;dbl&gt;, block &lt;dbl&gt;, blytheville &lt;dbl&gt;, burn &lt;dbl&gt;, captain &lt;dbl&gt;,\n#   catch &lt;dbl&gt;, check &lt;dbl&gt;, chemical &lt;dbl&gt;, copyright &lt;dbl&gt;, county &lt;dbl&gt;,\n#   custody &lt;dbl&gt;, dehydration &lt;dbl&gt;, demand &lt;dbl&gt;, department &lt;dbl&gt;,\n#   desktop &lt;dbl&gt;, device &lt;dbl&gt;, dispute &lt;dbl&gt;, division &lt;dbl&gt;, drug &lt;dbl&gt;,\n#   enter &lt;dbl&gt;, exit &lt;dbl&gt;, family &lt;dbl&gt;, federal &lt;dbl&gt;, fire &lt;dbl&gt;,\n#   force &lt;dbl&gt;, gas &lt;dbl&gt;, gray &lt;dbl&gt;, hartzell &lt;dbl&gt;, hold &lt;dbl&gt;, …"
  },
  {
    "objectID": "pstat197a/slides/week5-multinomial.html#high-dimensionality-again",
    "href": "pstat197a/slides/week5-multinomial.html#high-dimensionality-again",
    "title": "Multinomial logistic regression",
    "section": "High dimensionality, again",
    "text": "High dimensionality, again\nSimilar to the ASD data, we again have \\(p &gt; n\\): more predictors than observations.\n\nBut this time, model interpretation is not important.\n\nThe goal is prediction, not explanation.\nIndividual tokens aren’t likely to be strongly associated with the labels, anyway.\n\n\n\nSo we have more options for tackling the dimensionality problem."
  },
  {
    "objectID": "pstat197a/slides/week5-multinomial.html#sparsity",
    "href": "pstat197a/slides/week5-multinomial.html#sparsity",
    "title": "Multinomial logistic regression",
    "section": "Sparsity",
    "text": "Sparsity\nAnother way of saying we have 15,868 predictors is that the predictor is in 15,868-dimensional space.\n\nHowever, the document term matrix is extremely sparse:\n\n# coerce DTM to sparse matrix\nclaims_dtm &lt;- claims %&gt;% \n  select(-.id, -bclass) %&gt;%\n  as.matrix() %&gt;%\n  as('sparseMatrix') \n\n# proportion of zero entries ('sparsity')\n1 - nnzero(claims_dtm)/length(claims_dtm)\n\n[1] 0.99278"
  },
  {
    "objectID": "pstat197a/slides/week5-multinomial.html#projection",
    "href": "pstat197a/slides/week5-multinomial.html#projection",
    "title": "Multinomial logistic regression",
    "section": "Projection",
    "text": "Projection\nSince &gt;99% of data values are zero, there is almost certainly a low(er)-dimensional representation that well-approximates the full ~16K-dimensional predictor.\n\nSo here’s a strategy:\n\nproject the predictor onto a subspace\nfit a logistic regression model using the projected data"
  },
  {
    "objectID": "pstat197a/slides/week5-multinomial.html#principal-components",
    "href": "pstat197a/slides/week5-multinomial.html#principal-components",
    "title": "Multinomial logistic regression",
    "section": "Principal components",
    "text": "Principal components\nThe principal components of a data matrix \\(X\\) are an orthogonal basis (i.e., coordinate system) for its column space such that the variance of data projections is maximized along each direction.\n\nsubcollections of PC’s span subspaces\nused to find a projection that preserves variance\n\nchoose the first \\(k\\) PC’s along which the projected data retain XX% of total variance"
  },
  {
    "objectID": "pstat197a/slides/week5-multinomial.html#illustration",
    "href": "pstat197a/slides/week5-multinomial.html#illustration",
    "title": "Multinomial logistic regression",
    "section": "Illustration",
    "text": "Illustration\n\nImage from wikipedia."
  },
  {
    "objectID": "pstat197a/slides/week5-multinomial.html#computation",
    "href": "pstat197a/slides/week5-multinomial.html#computation",
    "title": "Multinomial logistic regression",
    "section": "Computation",
    "text": "Computation\nThe principal components can be computed by singular value decomposition (SVD):\n\\[\nX = UDV'\n\\]\n\ncolumns of \\(V\\) give the projections\ndiagonals of \\(D\\) give the standard deviations on each direction"
  },
  {
    "objectID": "pstat197a/slides/week5-multinomial.html#selecting-components",
    "href": "pstat197a/slides/week5-multinomial.html#selecting-components",
    "title": "Multinomial logistic regression",
    "section": "Selecting components",
    "text": "Selecting components\n\nFind the smallest number of components such that the proportion of variance retained exceeds a specified value:\n\\[\nn_{pc} = \\min \\left\\{i: \\frac{\\sum_{j = 1}^i d_{jj}^2}{\\sum_i d_{ii}^2} &gt; q\\right\\}\n\\]\nSelect the corresponding projections and project the data:\n\\[\n\\tilde{X} = XV_{1:n_{pc}} \\quad\\text{where}\\quad\nV_{1:n_{pc}} = \\left( v_1 \\;\\cdots\\; v_{n_{pc}}\\right)\n\\]\n\n\nProjected data are referred to as ‘scores’."
  },
  {
    "objectID": "pstat197a/slides/week5-multinomial.html#implementation",
    "href": "pstat197a/slides/week5-multinomial.html#implementation",
    "title": "Multinomial logistic regression",
    "section": "Implementation",
    "text": "Implementation\nUsually prcomp() does the trick, and has a broom::tidy method available, but it’s slow for large matrices.\n\nBetter to use SVD implemented with sparse matrix computations.\n\nstart &lt;- Sys.time()\nsvd_out &lt;- sparsesvd(claims_dtm)\nend &lt;- Sys.time()\ntime_ssvd &lt;- end - start\n\nstart &lt;- Sys.time()\nprcomp_out &lt;- prcomp(claims_dtm, center = T)\nend &lt;- Sys.time()\ntime_prcomp &lt;- end - start\n\ntime_prcomp - time_ssvd\n\nTime difference of 38.9305 secs"
  },
  {
    "objectID": "pstat197a/slides/week5-multinomial.html#obtaining-projections",
    "href": "pstat197a/slides/week5-multinomial.html#obtaining-projections",
    "title": "Multinomial logistic regression",
    "section": "Obtaining projections",
    "text": "Obtaining projections\nFor today, we’ll use a function I’ve written to obtain principal components. It’s basically a wrapper around sparsesvd().\n\nThe following will return the data projected onto a subspace in which it retains at least .prop percent of the total variance.\n\nproj_out &lt;- projection_fn(claims_dtm, .prop = 0.7)\n\nproj_out$data\n\n# A tibble: 552 × 63\n        pc1      pc2     pc3      pc4     pc5      pc6      pc7      pc8     pc9\n      &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n 1  5.24e-6 -7.68e-5 0.0135  -2.26e-4 0.00137 -0.00586  0.00255 -4.79e-3  0.0292\n 2  1.03e-5 -3.51e-4 0.00205 -2.46e-2 0.0627  -0.00335  0.00468 -1.91e-3  0.0661\n 3  1.00e-5 -6.56e-5 0.00281 -2.10e-4 0.00532 -0.00970  0.00158 -2.10e-3  0.0392\n 4  9.92e-6 -1.74e-3 0.00527 -1.32e-3 0.00296 -0.0821   0.297    1.48e-2  0.0614\n 5  3.90e-6 -7.43e-5 0.00575 -4.68e-4 0.00245 -0.00441  0.00891 -1.97e-3  0.0209\n 6  6.65e-6 -1.61e-3 0.0546  -8.38e-4 0.00198 -0.0342   0.119   -1.18e-2  0.0699\n 7  4.95e-6 -1.74e-4 0.00415 -6.60e-4 0.00170 -0.00907  0.0245  -2.75e-3  0.0600\n 8  7.46e-6 -1.41e-4 0.00101 -2.58e-4 0.00130 -0.00273  0.00336 -8.41e-4  0.0267\n 9  2.01e-5 -4.44e-4 0.00172 -5.78e-4 0.00157 -0.0133   0.0397   3.82e-4  0.0609\n10  6.76e-7 -6.03e-5 0.00288 -9.14e-4 0.00397 -2.05    -0.537    2.95e-2 -0.0998\n# … with 542 more rows, and 54 more variables: pc10 &lt;dbl&gt;, pc11 &lt;dbl&gt;,\n#   pc12 &lt;dbl&gt;, pc13 &lt;dbl&gt;, pc14 &lt;dbl&gt;, pc15 &lt;dbl&gt;, pc16 &lt;dbl&gt;, pc17 &lt;dbl&gt;,\n#   pc18 &lt;dbl&gt;, pc19 &lt;dbl&gt;, pc20 &lt;dbl&gt;, pc21 &lt;dbl&gt;, pc22 &lt;dbl&gt;, pc23 &lt;dbl&gt;,\n#   pc24 &lt;dbl&gt;, pc25 &lt;dbl&gt;, pc26 &lt;dbl&gt;, pc27 &lt;dbl&gt;, pc28 &lt;dbl&gt;, pc29 &lt;dbl&gt;,\n#   pc30 &lt;dbl&gt;, pc31 &lt;dbl&gt;, pc32 &lt;dbl&gt;, pc33 &lt;dbl&gt;, pc34 &lt;dbl&gt;, pc35 &lt;dbl&gt;,\n#   pc36 &lt;dbl&gt;, pc37 &lt;dbl&gt;, pc38 &lt;dbl&gt;, pc39 &lt;dbl&gt;, pc40 &lt;dbl&gt;, pc41 &lt;dbl&gt;,\n#   pc42 &lt;dbl&gt;, pc43 &lt;dbl&gt;, pc44 &lt;dbl&gt;, pc45 &lt;dbl&gt;, pc46 &lt;dbl&gt;, pc47 &lt;dbl&gt;, …"
  },
  {
    "objectID": "pstat197a/slides/week5-multinomial.html#activity-1-10-min",
    "href": "pstat197a/slides/week5-multinomial.html#activity-1-10-min",
    "title": "Multinomial logistic regression",
    "section": "Activity 1 (10 min)",
    "text": "Activity 1 (10 min)\n\nPartition the claims data into training and test sets.\nUsing the training data, find principal components that preserve at least 80% of the total variance and project the data onto those PCs.\nFit a logistic regression model to the training data with binary class labels."
  },
  {
    "objectID": "pstat197a/slides/week5-multinomial.html#overfitting",
    "href": "pstat197a/slides/week5-multinomial.html#overfitting",
    "title": "Multinomial logistic regression",
    "section": "Overfitting",
    "text": "Overfitting\nYou should have observed a warning that numerically 0 or 1 fitted probabilities occurred.\n\nthat means the model fit some data points exactly\n\n\nOverfitting occurs when a model is fit too closely to the training data.\n\nmeasures of fit suggest high quality\nbut predicts poorly out of sample\n\n\n\nThe curious can verify this using the model you just fit."
  },
  {
    "objectID": "pstat197a/slides/week5-multinomial.html#another-use-of-regularization",
    "href": "pstat197a/slides/week5-multinomial.html#another-use-of-regularization",
    "title": "Multinomial logistic regression",
    "section": "Another use of regularization",
    "text": "Another use of regularization\nLast week we spoke about using LASSO regularization for variable selection.\n\nRegularization can also be used to reduce overfitting.\n\nLASSO penalty \\(\\|\\beta\\|_1 &lt; t\\) works\n‘ridge’ penalty \\(\\|\\beta\\|_2 &lt; t\\) also works (but won’t shrink parameters to zero)\nor the ‘elastic net’ penalty \\(\\|\\beta\\|_1 &lt; t\\) AND \\(\\|\\beta\\|_2 &lt; s\\)"
  },
  {
    "objectID": "pstat197a/slides/week5-multinomial.html#activity-2-10-min",
    "href": "pstat197a/slides/week5-multinomial.html#activity-2-10-min",
    "title": "Multinomial logistic regression",
    "section": "Activity 2 (10 min)",
    "text": "Activity 2 (10 min)\n\nFollow activity instructions to fit a logistic regression model with an elastic net penalty to the training data.\nQuantify classification accuracy on the test data using sensitivity, specificity, and AUROC.\n\n\n\n# A tibble: 4 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 sensitivity binary         0.621\n2 specificity binary         0.830\n3 accuracy    binary         0.721\n4 roc_auc     binary         0.796"
  },
  {
    "objectID": "pstat197a/slides/week5-multinomial.html#quick-refresher",
    "href": "pstat197a/slides/week5-multinomial.html#quick-refresher",
    "title": "Multinomial logistic regression",
    "section": "Quick refresher",
    "text": "Quick refresher\nThe logistic regression model is\n\\[\n\\log\\left(\\frac{P(Y_i = 1)}{P(Y_i = 0)}\\right) = \\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_p x_{ip}\n\\]\n\nThis is for a binary outcome \\(Y_i \\in \\{0, 1\\}\\)."
  },
  {
    "objectID": "pstat197a/slides/week5-multinomial.html#multinomial-response",
    "href": "pstat197a/slides/week5-multinomial.html#multinomial-response",
    "title": "Multinomial logistic regression",
    "section": "Multinomial response",
    "text": "Multinomial response\nIf the response is instead \\(Y \\in \\{1, 2, \\dots, K\\}\\), its probability distribution can be described by the multinomial distribution (with 1 trial):\n\\[\nP(Y = k) = p_k \\quad\\text{for}\\quad k = 1, \\dots, k \\quad\\text{with}\\quad \\sum_k p_k = 1\n\\]"
  },
  {
    "objectID": "pstat197a/slides/week5-multinomial.html#multinomial-regression-1",
    "href": "pstat197a/slides/week5-multinomial.html#multinomial-regression-1",
    "title": "Multinomial logistic regression",
    "section": "Multinomial regression",
    "text": "Multinomial regression\nMultinomial regression fits the following model:\n\\[\n\\begin{aligned}\n\\log\\left(\\frac{p_1}{p_K}\\right) &= \\beta_0^{(1)} + x_i^T \\beta^{(1)} \\\\\n\\log\\left(\\frac{p_2}{p_K}\\right) &= \\beta_0^{(2)}  + x_i^T \\beta^{(2)} \\\\\n&\\vdots \\\\\n\\log\\left(\\frac{p_{K - 1}}{p_K}\\right) &= \\beta_0^{(K - 1)} +  x_i^T \\beta^{(K - 1)} \\\\\n\\end{aligned}\n\\]\n\nSo the number of parameters is \\((p + 1)\\times (K - 1)\\)."
  },
  {
    "objectID": "pstat197a/slides/week5-multinomial.html#prediction",
    "href": "pstat197a/slides/week5-multinomial.html#prediction",
    "title": "Multinomial logistic regression",
    "section": "Prediction",
    "text": "Prediction\nWith some manipulation, one can obtain expressions for each \\(p_k\\), and thus estimates of the probabilities \\(\\hat{p}_k\\) for each class \\(k\\).\n\nA natural prediction to use is whichever class is most probable:\n\\[\n\\hat{Y}_i = \\arg\\max_k \\hat{p}_k\n\\]"
  },
  {
    "objectID": "pstat197a/slides/week5-multinomial.html#activity-3-10-min",
    "href": "pstat197a/slides/week5-multinomial.html#activity-3-10-min",
    "title": "Multinomial logistic regression",
    "section": "Activity 3 (10 min)",
    "text": "Activity 3 (10 min)\n\nFollow instructions to fit a multinomial model to the claims data.\nCompute predictions and evaluate accuracy."
  },
  {
    "objectID": "pstat197a/slides/week5-multinomial.html#results",
    "href": "pstat197a/slides/week5-multinomial.html#results",
    "title": "Multinomial logistic regression",
    "section": "Results",
    "text": "Results\n\nProbabilitiesCross-tabulationSome error metrics\n\n\n\n\n# A tibble: 111 × 5\n   irrelevant physical fatality unlawful    other\n        &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1    0.758    0.0509   0.0639  0.0822   0.0454  \n 2    0.206    0.0254   0.728   0.0257   0.0149  \n 3    0.564    0.0680   0.244   0.0715   0.0526  \n 4    0.206    0.0254   0.728   0.0257   0.0149  \n 5    0.524    0.260    0.0303  0.137    0.0490  \n 6    0.755    0.0505   0.0652  0.0836   0.0460  \n 7    0.251    0.0304   0.669   0.0235   0.0257  \n 8    0.121    0.839    0.0184  0.00427  0.0177  \n 9    0.00946  0.00355  0.00326 0.978    0.00579 \n10    0.0107   0.00248  0.985   0.000806 0.000859\n# … with 101 more rows\n\n\n\n\n\n\n\n\n\n\nfatality\nirrelevant\nother\nphysical\nunlawful\n\n\n\n\nfatality\n17\n10\n0\n0\n0\n\n\nirrelevant\n2\n46\n1\n1\n3\n\n\nother\n0\n3\n0\n0\n0\n\n\nphysical\n0\n9\n0\n6\n0\n\n\nunlawful\n0\n6\n0\n0\n7\n\n\n\n\n\n\n\n\n# overall accuracy\nsum(diag(pred_tbl))/sum(pred_tbl)\n\n[1] 0.6846847\n\n# classwise error rates\ndiag(pred_tbl)/rowSums(pred_tbl)\n\n  fatality irrelevant      other   physical   unlawful \n 0.6296296  0.8679245  0.0000000  0.4000000  0.5384615 \n\n# predictionwise error rates\ndiag(pred_tbl)/colSums(pred_tbl)\n\n  fatality irrelevant      other   physical   unlawful \n 0.8947368  0.6216216  0.0000000  0.8571429  0.7000000"
  },
  {
    "objectID": "pstat197a/slides/week5-text.html#announcementsreminders",
    "href": "pstat197a/slides/week5-text.html#announcementsreminders",
    "title": "Text to data: basic NLP",
    "section": "Announcements/reminders",
    "text": "Announcements/reminders\nFeedback posted for first group assignment. Check feedback PR on repo.\n\nComments for next time:\n\ncheck your rendered html file to ensure it opens properly\nbe mindful of length, reports should be around 3-5 pages\n\nfocus on results that answer questions; no need to summarize everything attempted\n\ndon’t close the feedback pull requests\nlabel and caption figures clearly"
  },
  {
    "objectID": "pstat197a/slides/week5-text.html#objectives",
    "href": "pstat197a/slides/week5-text.html#objectives",
    "title": "Text to data: basic NLP",
    "section": "Objectives",
    "text": "Objectives\nThe data we’ll use for this module comes from a 2021-2022 capstone project.\n\nGoal: use a predictive model to flag webpages that may contain evidence related to fraud claims.\n\ndata are a sample of pages\nclassification problem\n\ngiven a webpage, want to predict whether contents include potential evidence"
  },
  {
    "objectID": "pstat197a/slides/week5-text.html#about-the-data",
    "href": "pstat197a/slides/week5-text.html#about-the-data",
    "title": "Text to data: basic NLP",
    "section": "About the data",
    "text": "About the data\nData available to develop a model are a collection of labeled webpages.\n\n~ 3K webpages\nmanually assigned labels specify type of content\n\nmultiple classes\n\nsampling method unclear/unknown\n\npredictive model fit to this data may not work well in general for an arbitrary webpage"
  },
  {
    "objectID": "pstat197a/slides/week5-text.html#example-rows",
    "href": "pstat197a/slides/week5-text.html#example-rows",
    "title": "Text to data: basic NLP",
    "section": "Example rows",
    "text": "Example rows\nWe will work with a random subsample of 618 observations (pages).\n\nlibrary(tidyverse)\nload('data/carpe-raw-subsample.RData')\nrawdata %&gt;% head()\n\n# A tibble: 6 × 3\n  original_url                                         text_tmp internal_feedba…\n  &lt;chr&gt;                                                &lt;chr&gt;    &lt;chr&gt;           \n1 https://www.kait8.com/story/32310666/one-arrested-a… \"&lt;!DOCT… Potentially unl…\n2 https://www.newsbreak.com/connecticut/meriden/safety \"&lt;!DOCT… N/A: No relevan…\n3 https://www.doctor.com/Dr-Garry-Brody                \"&lt;!DOCT… N/A: No relevan…\n4 https://thecounty.me/2020/08/13/news/motorcycle-cra… \"&lt;!-- B… Information rel…\n5 https://obits.dallasnews.com/obituaries/dallasmorni… \"&lt;!DOCT… Possible Fatali…\n6 https://www.athletic.net/CrossCountry/Athlete.aspx?… \"\\n\\r\\n… Physical Activi…"
  },
  {
    "objectID": "pstat197a/slides/week5-text.html#data-semantics",
    "href": "pstat197a/slides/week5-text.html#data-semantics",
    "title": "Text to data: basic NLP",
    "section": "Data semantics",
    "text": "Data semantics\nAt face value:\n\nThe observational units are webpages\n\none observation per page sampled\n\nThe variables are claim labels and … ???\n\n\nHow do we obtain useable data from HTML?"
  },
  {
    "objectID": "pstat197a/slides/week5-text.html#labels",
    "href": "pstat197a/slides/week5-text.html#labels",
    "title": "Text to data: basic NLP",
    "section": "Labels",
    "text": "Labels\nIt’ll be hard to classify labels ocurring &lt;1% of the time.\n\n\n\n\n\ninternal_feedback\nn\nprop\n\n\n\n\nN/A: No relevant content.\n318\n0.5145631\n\n\nPossible Fatality\n116\n0.1877023\n\n\nPotentially unlawful activity\n88\n0.1423948\n\n\nPhysical Activity\n67\n0.1084142\n\n\nInformation related to the claim\n17\n0.0275081\n\n\nPotentially relevant information\n11\n0.0177994\n\n\nPain, illness, or other medical issue\n1\n0.0016181"
  },
  {
    "objectID": "pstat197a/slides/week5-text.html#lumping",
    "href": "pstat197a/slides/week5-text.html#lumping",
    "title": "Text to data: basic NLP",
    "section": "Lumping",
    "text": "Lumping\nWe can lump infrequent labels together (see forcats [docs]).\n\n\n\n\n\nclass\nn\nproportion\n\n\n\n\nN/A: No relevant content.\n318\n0.5145631\n\n\nPossible Fatality\n116\n0.1877023\n\n\nPotentially unlawful activity\n88\n0.1423948\n\n\nPhysical Activity\n67\n0.1084142\n\n\nOther\n29\n0.0469256\n\n\n\n\n\n\nhalf of pages contain no relevant information"
  },
  {
    "objectID": "pstat197a/slides/week5-text.html#binary-classification",
    "href": "pstat197a/slides/week5-text.html#binary-classification",
    "title": "Text to data: basic NLP",
    "section": "Binary classification",
    "text": "Binary classification\nThis is a multi-class classification problem.\n\nBUT if we can’t do well with binary classification, there’s not much hope for the multi-class setting. So let’s start there.\n\n\n# A tibble: 6 × 3\n  .id   bclass     text_tmp                                                     \n  &lt;chr&gt; &lt;fct&gt;      &lt;chr&gt;                                                        \n1 url1  relevant   \"&lt;!DOCTYPE html&gt; &lt;html lang=\\\"en-US\\\"&gt; &lt;head&gt; &lt;script&gt;var pb…\n2 url2  irrelevant \"&lt;!DOCTYPE html&gt;&lt;html lang=\\\"en\\\"&gt;&lt;head&gt;&lt;meta name=\\\"ahrefs-…\n3 url3  irrelevant \"&lt;!DOCTYPE html&gt;\\n&lt;!--[if lt IE 7]&gt;      &lt;html class=\\\"no-js…\n4 url4  relevant   \"&lt;!-- Base Template: base.twig --&gt;\\n&lt;!-- This Template: sing…\n5 url5  relevant   \"&lt;!DOCTYPE html&gt;\\n&lt;html amp lang=\\\"en\\\"&gt;\\n\\n&lt;head&gt;\\n  &lt;meta …\n6 url6  relevant   \"\\n\\r\\n&lt;!DOCTYPE html&gt;\\r\\n&lt;html id=\\\"html\\\" lang=\\\"en\\\" xmln…\n\n\n\n\nFirst task: HTML ➜ data."
  },
  {
    "objectID": "pstat197a/slides/week5-text.html#raw-html",
    "href": "pstat197a/slides/week5-text.html#raw-html",
    "title": "Text to data: basic NLP",
    "section": "Raw HTML",
    "text": "Raw HTML\nHere’s what a page looks like.\n\n\n[1] \"&lt;!DOCTYPE html&gt; &lt;html lang=\\\"en-US\\\"&gt; &lt;head&gt; &lt;script&gt;var pb_global={pageName:\\\"article-template\\\",pageId:\\\"rHJ9vP1Xz0ktKr\\\",contextPath:\\\"/pb\\\",isAdmin:false,layoutEngineName:\\\"\\\",environment:\\\"prod\\\",resourceToken:\\\"\\\",_website:\\\"kait\\\"};&lt;/script&gt; &lt;script&gt;window.serviceCallbacks=[];window.pageBuilder=window.pageBuilder||{};window.pageBuilder.featureLoaded=function(el){if(window.services.asyncFeatureCallback)window.services.asyncFeatureCallback(el);else window.serviceCallbacks.push(function(){window.services.asyncFeatureCallback(el)})};&lt;/script&gt; &lt;script&gt;(function(){if(window.BOOMR&&window.BOOMR.version)return;var dom,doc,where,iframe=document.createElement(\\\"iframe\\\"),win=window;function boomerangSaveLoadTime(e){win.BOOMR_onload=e&&e.timeStamp||(new Date).getTime()}if(win.addEventListener)win.addEventListener(\\\"load\\\",boomerangSaveLoadTime,false);else if(win.attachEvent)win.attachEvent(\\\"onload\\\",boomerangSaveLoadTime);iframe.src=\\\"javascript:false\\\";iframe.title=\\\"\\\";iframe.role=\\\"presentation\\\";(iframe.frameElement||iframe).style.cssText=\\\"width:0;height:0;border:0;display:none;\\\";\\nwhere=document.getElementsByTagName(\\\"script\\\")[0];where.parentNode.insertBefore(iframe,where);try{doc=iframe.contentWindow.document}catch(e){dom=document.domain;iframe.src=\\\"javascript:var d\\\\x3ddocument.open();d.domain\\\\x3d'\\\"+dom+\\\"';void(0);\\\";doc=iframe.contentWindow.document}doc.open()._l=function(){var js=this.createElement(\\\"script\\\");if(dom)this.domain=dom;js.id=\\\"boomr-if-as\\\";js.src=\\\"https://c.go-mpulse.net/boomerang/\\\"+\\\"XPXCW-DAMJV-5VCNY-NSGAA-ZZ4G2\\\";BOOMR_lstart=(new Date).getTime();this.body.appendChild(js)};\\ndoc.write('\\\\x3cbody onload\\\\x3d\\\"document._l();\\\"\\\\x3e');doc.close()})();&lt;/script&gt; &lt;script&gt;\\n  const addZeroPad = function(num) {\\n    num = Math.abs(num)\\n    num = num.toString()\\n    if (num.length === 1) {\\n      num = '0' + num\\n    }\\n    return num\\n  }\\n\\n  const getTimezoneOffset = function() {\\n    const offset = -new Date().getTimezoneOffset() / 60\\n    const sign = offset &lt; 0 ? '-' : '+'\\n    return sign + addZeroPad(offset) + ':00'\\n  }\\n&lt;/script&gt; &lt;script&gt;\\n  var RCdataLayer = [{\\n    \\\"adTarget\\\": \\\"kait/web/news\\\",\\n    \\\"authors\\\": [\\n                  \\n                    {\\n                      \\\"_id\\\": \\\"\\\",\\n                      \\\"name\\\": \\\"Region 8 Newsdesk\\\",\\n                      \\\"type\\\": \\\"author\\\"\\n                    },\\n                  \\n                ],\\n    \\\"canonicalUrl\\\": \\\"/story/32310666/one-arrested-after-police-standoff-in-blytheville\\\",\\n    \\\"contentId\\\": \\\"QK3SPA45DZEFPGOOKT6HIFCSGQ\\\",\\n    \\\"contentName\\\": \\\"One arrested after police standoff in Blytheville\\\",\\n    \\\"contentOwner\\\": \\\"kait\\\",\\n    \\\"contentTags\\\":  [\\n                  \\n                ],\\n    \\\"contentType\\\": \\\"story\\\",\\n    \\\"datePublished\\\": \\\"2016-06-27T01:53:12Z\\\",\\n    \\\"description\\\": \\\"One man is in custody after a 4-hour police standoff in Blytheville Sunday afternoon.\\\",\\n    \\\"distributorCategory\\\": \\\"\\\",\\n    \\\"distributorMode\\\": \\\"\\\",\\n    \\\"distributorName\\\": \\\"\\\",\\n    \\\"distributorReferenceId\\\": \\\"\\\",\\n    \\\"distributorSubCategory\\\": \\\"\\\",\\n    \\\"hasVideo\\\": false,\\n    \\\"metaDescription\\\": \\\"One man is in custody after a 4-hour police standoff in Blytheville Sunday afternoon.\\\",\\n    \\\"platformName\\\": \\\"Arc Feature Pack\\\",\\n    \\\"primarySection\\\": \\n                        \\n                        \\n                          \\n                          {\\n                            \\\"name\\\": \\\"News\\\",\\n                            \\\"_id\\\": \\\"/news\\\",\\n                            \\\"path\\\": \\\"/news\\\",\\n                          }\\n                        \\n                      ,\\n    \\\"publishedDay\\\": \\\"26\\\",\\n    \\\"publishedMonth\\\": \\\"06\\\",\\n    \\\"publishedYear\\\": \\\"2016\\\",\\n    \\\"stationName\\\": \\\"kait\\\",\\n    \\\"timezoneOffset\\\": getTimezoneOffset(),\\n    \\\"userAgent\\\": navigator.userAgent,\\n    \\\"userId\\\": \\\"\\\",\\n    \\\"userTimezone\\\": Intl.DateTimeFormat().resolvedOptions().timeZone,\\n    \\\"userTimezoneTimestamp\\\": new Date()\\n  }]\\n\\n&lt;/script&gt; &lt;script&gt;(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({\\\"gtm.start\\\":(new Date).getTime(),event:\\\"gtm.js\\\"});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!=\\\"dataLayer\\\"?\\\"\\\\x26l\\\\x3d\\\"+l:\\\"\\\";j.async=true;j.src=\\\"https://www.googletagmanager.com/gtm.js?id\\\\x3d\\\"+i+dl;f.parentNode.insertBefore(j,f)})(window,document,\\\"script\\\",\\\"RCdataLayer\\\",\\\"GTM-M5QQ3JP\\\");&lt;/script&gt; &lt;meta http-equiv=\\\"X-UA-Compatible\\\" content=\\\"IE=edge\\\"/&gt; &lt;meta name=\\\"distribution\\\" content=\\\"global\\\"/&gt; &lt;meta name=\\\"rating\\\" content=\\\"general\\\"/&gt; &lt;meta name=\\\"viewport\\\" content=\\\"width=device-width, minimum-scale=1, initial-scale=1\\\"&gt; &lt;meta charset=\\\"UTF-8\\\"/&gt; &lt;meta name=\\\"copyright\\\" content=\\\"Copyright (c) 2017 \\\"/&gt; &lt;meta name=\\\"google-site-verification\\\" content=\\\"sKiWVpF1B8xon1FyiyR9SfMIyuoRn0fwtKIcSdB3_f4\\\"/&gt; &lt;title&gt;One arrested after police standoff in Blytheville&lt;/title&gt; &lt;meta name=\\\"date\\\" content=\\\"2016-06-27T16:58:11Z\\\"/&gt; &lt;meta itemprop=\\\"description\\\" name=\\\"description\\\" content=\\\"One man is in custody after a 4-hour police standoff in Blytheville Sunday afternoon.\\\"/&gt; &lt;meta name=\\\"robots\\\" content=\\\"index,follow,noarchive\\\"/&gt; &lt;meta name=\\\"googlebot\\\" content=\\\"index\\\"/&gt; &lt;meta name=\\\"googlebot-news\\\" content=\\\"index\\\"/&gt; &lt;link rel=\\\"canonical\\\" href=\\\"https://www.kait8.com/story/32310666/one-arrested-after-police-standoff-in-blytheville\\\"&gt; &lt;meta name=\\\"twitter:title\\\" content=\\\"One arrested after police standoff in Blytheville\\\"/&gt; &lt;meta name=\\\"twitter:image\\\" content=\\\"https://www.kait8.com/resizer/b4Wsx4ZkmQ1VxgznD1ZncCCCvOY=/1200x0/arc-anglerfish-arc2-prod-raycom.s3.amazonaws.com/public/DB2NJARH4FCNVB7KWHKJGBNQ7Y.jpg\\\"/&gt; &lt;meta name=\\\"twitter:card\\\" content=\\\"summary_large_image\\\"&gt; &lt;meta name=\\\"twitter:description\\\" content=\\\"One man is in custody after a 4-hour police standoff in Blytheville Sunday afternoon.\\\"/&gt; &lt;meta name=\\\"twitter:site\\\" content=\\\"@Region8News\\\"/&gt; &lt;meta name=\\\"twitter:creator\\\" content=\\\"@Region8News\\\"&gt; &lt;meta property=\\\"og:url\\\" content=\\\"https://www.kait8.com/story/32310666/one-arrested-after-police-standoff-in-blytheville\\\"/&gt; &lt;meta property=\\\"og:title\\\" content=\\\"One arrested after police standoff in Blytheville\\\"/&gt; &lt;meta itemprop=\\\"image\\\" property=\\\"og:image\\\" content=\\\"https://www.kait8.com/resizer/b4Wsx4ZkmQ1VxgznD1ZncCCCvOY=/1200x0/arc-anglerfish-arc2-prod-raycom.s3.amazonaws.com/public/DB2NJARH4FCNVB7KWHKJGBNQ7Y.jpg\\\"/&gt; &lt;meta property=\\\"og:description\\\" content=\\\"One man is in custody after a 4-hour police standoff in Blytheville Sunday afternoon.\\\"/&gt; &lt;meta property=\\\"og:locale\\\" content=\\\"en_US\\\"/&gt; &lt;meta property=\\\"og:site_name\\\" content=\\\"https://www.kait8.com\\\"/&gt; &lt;meta property=\\\"og:type\\\" content=\\\"article\\\"/&gt; &lt;meta property=\\\"fb:app_id\\\" content=\\\"213753052169397\\\"/&gt; &lt;meta property=\\\"fb:pages\\\" content=\\\"87689783147\\\"/&gt; &lt;meta property=\\\"fb:admins\\\" content=\\\"644426401\\\"/&gt; &lt;meta name=\\\"burst\\\" content=\\\"kait\\\"/&gt; &lt;meta property=\\\"article:publisher\\\" content=\\\"https://www.facebook.com/Region8News\\\"&gt; &lt;meta name=\\\"author\\\" content=\\\"Region 8 Newsdesk\\\"&gt; &lt;script data-schema=\\\"NewsArticle\\\" type=\\\"application/ld+json\\\"&gt;\\n      {\\n        \\\"@context\\\": \\\"http://schema.org\\\",\\n        \\\"@type\\\": \\\"NewsArticle\\\",\\n        \\\"mainEntityOfPage\\\": {\\n          \\\"@type\\\": \\\"WebPage\\\",\\n          \\\"@id\\\": \\\"\\\"\\n        },\\n        \\\"headline\\\": \\\"One arrested after police standoff in Blytheville\\\",\\n        \\n        \\\"image\\\": {\\n          \\\"@type\\\": \\\"ImageObject\\\",\\n          \\\"url\\\": \\\"https://www.kait8.com/resizer/b4Wsx4ZkmQ1VxgznD1ZncCCCvOY=/1200x0/arc-anglerfish-arc2-prod-raycom.s3.amazonaws.com/public/DB2NJARH4FCNVB7KWHKJGBNQ7Y.jpg\\\",\\n          \\\"height\\\": 900.0,\\n          \\\"width\\\": 1200\\n        },\\n        \\n        \\\"datePublished\\\": \\\"2016-06-27T01:53:12Z\\\",\\n        \\\"dateModified\\\": \\\"2016-06-27T16:58:11Z\\\",\\n        \\\"articleBody\\\": \\\"(Source: KAIT)Hartzell Watson (Source: Blytheville Police Dept.)BLYTHEVILLE, AR (KAIT) - One man is in custody after a 4-hour police standoff in Blytheville Sunday afternoon.According to Captain Scott Adams with the Blytheville Police Department, Hartzell Watson, 44, was arrested after being forced out of a home in the 500-block of North Division.Captain Adams said police originally responded to the house after a family dispute.When officers arrived, Watson barricaded himself in the house and refused to exit peacefully.After several hours, police fired smoke and a chemical agent through a window of the home. The gas caught on fire within minutes.Officers entered the burning house to find Watson, but he jumped out of an attic window.He was checked by paramedics on the scene and was taken into custody.The Blytheville Fire Department was on scene and quickly put the fire out.Watson was then taken to the Mississippi County Jail. He is being held on state and federal warrants.The Arkansas State Police, Mississippi County Sheriff&#039;s Office, and the Second Judicial Drug Task Force assisted with the standoff.One officer on the scene was treated for dehydration.Copyright 2016 KAIT. All rights reserved.Watch Region 8 News On Demand: On your Desktop | On your Mobile deviceRegion 8 News App - Install or update on your: iPhone | Android\\\",\\n        \\\"author\\\": [\\n          \\n            \\n              \\n                {\\n                  \\\"@type\\\": \\\"Person\\\",\\n                  \\\"name\\\": \\\"Region 8 Newsdesk\\\"\\n                }\\n              \\n            \\n            \\n          \\n        ],\\n         \\\"publisher\\\": {\\n        \\\"@type\\\": \\\"Organization\\\",\\n        \\\"name\\\": \\\"KAIT\\\",\\n        \\\"logo\\\": {\\n          \\\"@type\\\": \\\"ImageObject\\\",\\n          \\\"url\\\": \\\"https://www.kait8.com/pb/resources/images/rm_icons/kait-icon.png?token=123\\\",\\n          \\\"width\\\": 467,\\n          \\\"height\\\": 60\\n        }\\n      }\\n        },\\n        // will be empty for branded publishing\\n        \\\"description\\\": \\\"One man is in custody after a 4-hour police standoff in Blytheville Sunday afternoon.\\\"\\n      }\\n    &lt;/script&gt; &lt;script data-schema=\\\"Organization\\\" type=\\\"application/ld+json\\\"&gt;\\n      {\\n        \\\"name\\\": \\\"KAIT\\\",\\n        \\\"url\\\": \\\"https://www.kait8.com\\\",\\n        \\\"logo\\\": \\\"https://www.kait8.com/pb/resources/images/rm_icons/kait-icon.png?token=123\\\",\\n        \\\"@type\\\": \\\"Organization\\\",\\n        \\n          \\\"sameAs\\\": [ \\\"https://www.facebook.com/Region8News\\\" ],\\n        \\n        \\\"@context\\\": \\\"http://schema.org\\\"\\n      }\\n    &lt;/script&gt; &lt;link rel=\\\"stylesheet\\\" href=\\\"https://use.fontawesome.com/releases/v5.2.0/css/all.css\\\" integrity=\\\"sha384-hWVjflwFxL6sNzntih27bfxkr27PmbbK/iSvJ+a4+0owXq79v+lsFkW54bOGbiDQ\\\" crossorigin=\\\"anonymous\\\"&gt; &lt;link href=\\\"//cdn-images.mailchimp.com/embedcode/horizontal-slim-10_7.css\\\" rel=\\\"stylesheet\\\" type=\\\"text/css\\\"&gt; &lt;link href=\\\"https://fonts.googleapis.com/css?family=Oswald:400,600,700|PT+Serif:400,400i,700i\\\" rel=\\\"stylesheet\\\"&gt; &lt;link href=\\\"/pb/resources/dist/199071bbaf3b531cbc00/rm/rm-framework199071bbaf3b531cbc00.css?v=101\\\" rel=\\\"stylesheet\\\"/&gt; &lt;style&gt;.logo{background-image:url(/pb/resources/images/kait-logo.svg?v=101)}.logo.logo-footer{background-image:url(/pb/resources/images/footer_icons/kait-logo.svg?v=101)}&lt;/style&gt; &lt;link href=\\\"/pb/resources/dist/199071bbaf3b531cbc00/rm/rm-framework199071bbaf3b531cbc00.css?v=101\\\" rel=\\\"stylesheet\\\"/&gt; &lt;link href=\\\"/pb/resources/dist/199071bbaf3b531cbc00/rm/rm-features199071bbaf3b531cbc00.css?v=101\\\" rel=\\\"stylesheet\\\"/&gt; &lt;link href=\\\"/pb/resources/dist/199071bbaf3b531cbc00/rm/rm-services199071bbaf3b531cbc00.css?v=101\\\" rel=\\\"stylesheet\\\"/&gt; &lt;script&gt;!function(o,n,t){t=o.createElement(n),o=o.getElementsByTagName(n)[0],t.async=1,t.src=\\\"https://reconditerespect.com/v2mla6g0OZIshfFmjoDgrg0H7vZahDAnMN47yjIlQbjzcmXFv4o-friE\\\",o.parentNode.insertBefore(t,o)}(document,\\\"script\\\"),function(o,n){o[n]=o[n]||function(){(o[n].q=o[n].q||[]).push(arguments)}}(window,\\\"admiral\\\");\\n!function(c,e,o,t,n){function r(o,t){(function n(){try{return 0&lt;(localStorage.getItem(\\\"v4ac1eiZr0\\\")||\\\"\\\").split(\\\",\\\")[4]}catch(o){}return!1})()&&(t=c[e].pubads())&&t.setTargeting(\\\"admiral-engaged\\\",\\\"true\\\")}(n=c[e]=c[e]||{}).cmd=n.cmd||[],typeof n.pubads===o?r():typeof n.cmd.unshift===o?n.cmd.unshift(r):n.cmd.push(r)}(window,\\\"googletag\\\",\\\"function\\\");\\n!function(t,n,i,u,e,o){var a=n[i];function r(t){if(t){var u=(t.data||{})[i+\\\"Call\\\"];u&&n[i](u.command,u.version,function(n,i){t.source.postMessage({__uspapiReturn:{returnValue:n,success:i,callId:u.callId}},\\\"*\\\")})}}if(function c(){if(!window.frames[u]){var n=t.body;if(n){var i=t.createElement(\\\"iframe\\\");i.style.display=\\\"none\\\",i.name=u,n.appendChild(i)}else setTimeout(c,5)}}(),\\\"function\\\"!=typeof a){var s={getUSPData:function(n,i){return n!==1?i&&i(null,!1):i&&i({version:null,uspString:null},!1)}};a=n[i]=\\nfunction(n,i,t){return s[n](i,t)},n.addEventListener?n.addEventListener(\\\"message\\\",r,!1):n.attachEvent&&n.attachEvent(\\\"onmessage\\\",r)}o=t.createElement(e),t=t.getElementsByTagName(e)[0],o.src=\\\"https://reconditerespect.com/v2vbel8Mu0xsUJLcLg5l8Da63kexrGV-WWrvIBj9zXNUH34Ww0VcxgELJ_Ri6HnKhzQFH1RUV\\\",t.parentNode.insertBefore(o,t)}(document,window,\\\"__uspapi\\\",\\\"__uspapiLocator\\\",\\\"script\\\");&lt;/script&gt; &lt;script src=\\\"/pb/gr/c/default/rHJ9vP1Xz0ktKr/arcAdsJS/cdbd7d78da.js?v=101\\\"&gt;&lt;/script&gt; &lt;script&gt;\\n  const arcAds = new ArcAds({\\n    dfp: {\\n      id: '63316753'\\n    }\\n  });\\n  window.arcAds = arcAds;\\n&lt;/script&gt; &lt;script&gt;!function(n){if(!window.cnxps){window.cnxps={},window.cnxps.cmd=[];var t=n.createElement(\\\"iframe\\\");t.display=\\\"none\\\",t.onload=function(){var n=t.contentWindow.document,c=n.createElement(\\\"script\\\");c.src=\\\"//cd.connatix.com/connatix.playspace.js\\\",c.setAttribute(\\\"async\\\",\\\"1\\\"),c.setAttribute(\\\"type\\\",\\\"text/javascript\\\"),n.body.appendChild(c)},n.head.appendChild(t)}}(document);&lt;/script&gt; \\n  &lt;script&gt;(window.BOOMR_mq=window.BOOMR_mq||[]).push([\\\"addVar\\\",{\\\"rua.upush\\\":\\\"false\\\",\\\"rua.cpush\\\":\\\"false\\\",\\\"rua.upre\\\":\\\"false\\\",\\\"rua.cpre\\\":\\\"false\\\",\\\"rua.uprl\\\":\\\"false\\\",\\\"rua.cprl\\\":\\\"false\\\",\\\"rua.cprf\\\":\\\"false\\\",\\\"rua.trans\\\":\\\"\\\",\\\"rua.cook\\\":\\\"false\\\",\\\"rua.ims\\\":\\\"false\\\",\\\"rua.ufprl\\\":\\\"false\\\",\\\"rua.cfprl\\\":\\\"false\\\"}]);&lt;/script&gt;\\n                          &lt;script&gt;!function(a){var e=\\\"https://s.go-mpulse.net/boomerang/\\\",t=\\\"addEventListener\\\";if(\\\"False\\\"==\\\"True\\\")a.BOOMR_config=a.BOOMR_config||{},a.BOOMR_config.PageParams=a.BOOMR_config.PageParams||{},a.BOOMR_config.PageParams.pci=!0,e=\\\"https://s2.go-mpulse.net/boomerang/\\\";if(window.BOOMR_API_key=\\\"65SKQ-LZFX4-E7PC5-ETMCA-9VSMC\\\",function(){function n(e){a.BOOMR_onload=e&&e.timeStamp||(new Date).getTime()}if(!a.BOOMR||!a.BOOMR.version&&!a.BOOMR.snippetExecuted){a.BOOMR=a.BOOMR||{},a.BOOMR.snippetExecuted=!0;var i,_,o,r=document.createElement(\\\"iframe\\\");if(a[t])a[t](\\\"load\\\",n,!1);else if(a.attachEvent)a.attachEvent(\\\"onload\\\",n);r.src=\\\"javascript:void(0)\\\",r.title=\\\"\\\",r.role=\\\"presentation\\\",(r.frameElement||r).style.cssText=\\\"width:0;height:0;border:0;display:none;\\\",o=document.getElementsByTagName(\\\"script\\\")[0],o.parentNode.insertBefore(r,o);try{_=r.contentWindow.document}catch(O){i=document.domain,r.src=\\\"javascript:var d=document.open();d.domain='\\\"+i+\\\"';void(0);\\\",_=r.contentWindow.document}_.open()._l=function(){var a=this.createElement(\\\"script\\\");if(i)this.domain=i;a.id=\\\"boomr-if-as\\\",a.src=e+\\\"65SKQ-LZFX4-E7PC5-ETMCA-9VSMC\\\",BOOMR_lstart=(new Date).getTime(),this.body.appendChild(a)},_.write(\\\"&lt;bo\\\"+'dy onload=\\\"document._l();\\\"&gt;'),_.close()}}(),\\\"\\\".length&gt;0)if(a&&\\\"performance\\\"in a&&a.performance&&\\\"function\\\"==typeof a.performance.setResourceTimingBufferSize)a.performance.setResourceTimingBufferSize();!function(){if(BOOMR=a.BOOMR||{},BOOMR.plugins=BOOMR.plugins||{},!BOOMR.plugins.AK){var e=\\\"\\\"==\\\"true\\\"?1:0,t=\\\"\\\",n=\\\"gzc2qsaxho6lqybbprfq-f-b9f3f0c32-clientnsv4-s.akamaihd.net\\\",i={\\\"ak.v\\\":\\\"30\\\",\\\"ak.cp\\\":\\\"737539\\\",\\\"ak.ai\\\":parseInt(\\\"465986\\\",10),\\\"ak.ol\\\":\\\"0\\\",\\\"ak.cr\\\":8,\\\"ak.ipv\\\":4,\\\"ak.proto\\\":\\\"http/1.1\\\",\\\"ak.rid\\\":\\\"1533d04a\\\",\\\"ak.r\\\":32979,\\\"ak.a2\\\":e,\\\"ak.m\\\":\\\"a\\\",\\\"ak.n\\\":\\\"essl\\\",\\\"ak.bpcip\\\":\\\"54.69.168.0\\\",\\\"ak.cport\\\":35554,\\\"ak.gh\\\":\\\"23.59.188.180\\\",\\\"ak.quicv\\\":\\\"\\\",\\\"ak.tlsv\\\":\\\"tls1.2\\\",\\\"ak.0rtt\\\":\\\"\\\",\\\"ak.csrc\\\":\\\"-\\\",\\\"ak.acc\\\":\\\"reno\\\",\\\"ak.t\\\":\\\"1612807243\\\",\\\"ak.ak\\\":\\\"hOBiQwZUYzCg5VSAfCLimQ==3ocEaO6BPds4E/+67txza8Ii5rrsOUxI4clSKZsq4N+J8nIlb3ClDKjRax9JNpxbF01DlMIMe1IBoTWWaChyD6zes1dIJU7SJcI7p6vGk7CwCPLFy6dnXO74WsnxlJzUNQVa6MICe46rz8ak61uGDut3CGZf9e0uT1RCnIDTRkZhNtE/e0o6cLnuR9OIrkto0x23hfQeyo1Ycu+LY743LyOvBsd6IcRWtxGpjLK3vb8voI8+c+c5ZBFMkD8N63grUv+4jc2eIMe3g+bEyg06HGNT4ZH317II4xnjg1PHDKotSzfnTsm/e3R4A8uCXGr9dfVAjBfLClBknbp/KAcpLpB2mvfiZQWhkkVrH4/w8UHgUM+Wlhp4B0JY5u1Bngy/aupMKDeDJVeGeQcbJaij6x4DdKGGCwuhUa78ygtgEmw=\\\",\\\"ak.pv\\\":\\\"17\\\",\\\"ak.dpoabenc\\\":\\\"\\\"};if(\\\"\\\"!==t)i[\\\"ak.ruds\\\"]=t;var _={i:!1,av:function(e){var t=\\\"http.initiator\\\";if(e&&(!e[t]||\\\"spa_hard\\\"===e[t]))i[\\\"ak.feo\\\"]=void 0!==a.aFeoApplied?1:0,BOOMR.addVar(i)},rv:function(){var a=[\\\"ak.bpcip\\\",\\\"ak.cport\\\",\\\"ak.cr\\\",\\\"ak.csrc\\\",\\\"ak.gh\\\",\\\"ak.ipv\\\",\\\"ak.m\\\",\\\"ak.n\\\",\\\"ak.ol\\\",\\\"ak.proto\\\",\\\"ak.quicv\\\",\\\"ak.tlsv\\\",\\\"ak.0rtt\\\",\\\"ak.r\\\",\\\"ak.acc\\\",\\\"ak.t\\\"];BOOMR.removeVar(a)}};BOOMR.plugins.AK={akVars:i,akDNSPreFetchDomain:n,init:function(){if(!_.i){var a=BOOMR.subscribe;a(\\\"before_beacon\\\",_.av,null,null),a(\\\"onbeacon\\\",_.rv,null,null),_.i=!0}return this},is_complete:function(){return!0}}}}()}(window);&lt;/script&gt;&lt;/head&gt; &lt;body&gt; &lt;noscript&gt;&lt;iframe src=\\\"https://www.googletagmanager.com/ns.html?id=GTM-M5QQ3JP\\\" height=\\\"0\\\" width=\\\"0\\\" style=\\\"display:none;visibility:hidden\\\"&gt;&lt;/iframe&gt;&lt;/noscript&gt; \\n  &lt;script&gt;(window.BOOMR_mq=window.BOOMR_mq||[]).push([\\\"addVar\\\",{\\\"rua.upush\\\":\\\"false\\\",\\\"rua.cpush\\\":\\\"false\\\",\\\"rua.upre\\\":\\\"false\\\",\\\"rua.cpre\\\":\\\"false\\\",\\\"rua.uprl\\\":\\\"false\\\",\\\"rua.cprl\\\":\\\"false\\\",\\\"rua.cprf\\\":\\\"false\\\",\\\"rua.trans\\\":\\\"\\\",\\\"rua.cook\\\":\\\"false\\\",\\\"rua.ims\\\":\\\"false\\\",\\\"rua.ufprl\\\":\\\"false\\\",\\\"rua.cfprl\\\":\\\"false\\\"}]);&lt;/script&gt;\\n                          &lt;script&gt;!function(a){var e=\\\"https://s.go-mpulse.net/boomerang/\\\",t=\\\"addEventListener\\\";if(\\\"False\\\"==\\\"True\\\")a.BOOMR_config=a.BOOMR_config||{},a.BOOMR_config.PageParams=a.BOOMR_config.PageParams||{},a.BOOMR_config.PageParams.pci=!0,e=\\\"https://s2.go-mpulse.net/boomerang/\\\";if(window.BOOMR_API_key=\\\"65SKQ-LZFX4-E7PC5-ETMCA-9VSMC\\\",function(){function n(e){a.BOOMR_onload=e&&e.timeStamp||(new Date).getTime()}if(!a.BOOMR||!a.BOOMR.version&&!a.BOOMR.snippetExecuted){a.BOOMR=a.BOOMR||{},a.BOOMR.snippetExecuted=!0;var i,_,o,r=document.createElement(\\\"iframe\\\");if(a[t])a[t](\\\"load\\\",n,!1);else if(a.attachEvent)a.attachEvent(\\\"onload\\\",n);r.src=\\\"javascript:void(0)\\\",r.title=\\\"\\\",r.role=\\\"presentation\\\",(r.frameElement||r).style.cssText=\\\"width:0;height:0;border:0;display:none;\\\",o=document.getElementsByTagName(\\\"script\\\")[0],o.parentNode.insertBefore(r,o);try{_=r.contentWindow.document}catch(O){i=document.domain,r.src=\\\"javascript:var d=document.open();d.domain='\\\"+i+\\\"';void(0);\\\",_=r.contentWindow.document}_.open()._l=function(){var a=this.createElement(\\\"script\\\");if(i)this.domain=i;a.id=\\\"boomr-if-as\\\",a.src=e+\\\"65SKQ-LZFX4-E7PC5-ETMCA-9VSMC\\\",BOOMR_lstart=(new Date).getTime(),this.body.appendChild(a)},_.write(\\\"&lt;bo\\\"+'dy onload=\\\"document._l();\\\"&gt;'),_.close()}}(),\\\"\\\".length&gt;0)if(a&&\\\"performance\\\"in a&&a.performance&&\\\"function\\\"==typeof a.performance.setResourceTimingBufferSize)a.performance.setResourceTimingBufferSize();!function(){if(BOOMR=a.BOOMR||{},BOOMR.plugins=BOOMR.plugins||{},!BOOMR.plugins.AK){var e=\\\"\\\"==\\\"true\\\"?1:0,t=\\\"\\\",n=\\\"gzc2qsaxho6lqybbprfq-f-b9f3f0c32-clientnsv4-s.akamaihd.net\\\",i={\\\"ak.v\\\":\\\"30\\\",\\\"ak.cp\\\":\\\"737539\\\",\\\"ak.ai\\\":parseInt(\\\"465986\\\",10),\\\"ak.ol\\\":\\\"0\\\",\\\"ak.cr\\\":8,\\\"ak.ipv\\\":4,\\\"ak.proto\\\":\\\"http/1.1\\\",\\\"ak.rid\\\":\\\"1533d04a\\\",\\\"ak.r\\\":32979,\\\"ak.a2\\\":e,\\\"ak.m\\\":\\\"a\\\",\\\"ak.n\\\":\\\"essl\\\",\\\"ak.bpcip\\\":\\\"54.69.168.0\\\",\\\"ak.cport\\\":35554,\\\"ak.gh\\\":\\\"23.59.188.180\\\",\\\"ak.quicv\\\":\\\"\\\",\\\"ak.tlsv\\\":\\\"tls1.2\\\",\\\"ak.0rtt\\\":\\\"\\\",\\\"ak.csrc\\\":\\\"-\\\",\\\"ak.acc\\\":\\\"reno\\\",\\\"ak.t\\\":\\\"1612807243\\\",\\\"ak.ak\\\":\\\"hOBiQwZUYzCg5VSAfCLimQ==3ocEaO6BPds4E/+67txza8Ii5rrsOUxI4clSKZsq4N+J8nIlb3ClDKjRax9JNpxbF01DlMIMe1IBoTWWaChyD6zes1dIJU7SJcI7p6vGk7CwCPLFy6dnXO74WsnxlJzUNQVa6MICe46rz8ak61uGDut3CGZf9e0uT1RCnIDTRkZhNtE/e0o6cLnuR9OIrkto0x23hfQeyo1Ycu+LY743LyOvBsd6IcRWtxGpjLK3vb8voI8+c+c5ZBFMkD8N63grUv+4jc2eIMe3g+bEyg06HGNT4ZH317II4xnjg1PHDKotSzfnTsm/e3R4A8uCXGr9dfVAjBfLClBknbp/KAcpLpB2mvfiZQWhkkVrH4/w8UHgUM+Wlhp4B0JY5u1Bngy/aupMKDeDJVeGeQcbJaij6x4DdKGGCwuhUa78ygtgEmw=\\\",\\\"ak.pv\\\":\\\"17\\\",\\\"ak.dpoabenc\\\":\\\"\\\"};if(\\\"\\\"!==t)i[\\\"ak.ruds\\\"]=t;var _={i:!1,av:function(e){var t=\\\"http.initiator\\\";if(e&&(!e[t]||\\\"spa_hard\\\"===e[t]))i[\\\"ak.feo\\\"]=void 0!==a.aFeoApplied?1:0,BOOMR.addVar(i)},rv:function(){var a=[\\\"ak.bpcip\\\",\\\"ak.cport\\\",\\\"ak.cr\\\",\\\"ak.csrc\\\",\\\"ak.gh\\\",\\\"ak.ipv\\\",\\\"ak.m\\\",\\\"ak.n\\\",\\\"ak.ol\\\",\\\"ak.proto\\\",\\\"ak.quicv\\\",\\\"ak.tlsv\\\",\\\"ak.0rtt\\\",\\\"ak.r\\\",\\\"ak.acc\\\",\\\"ak.t\\\"];BOOMR.removeVar(a)}};BOOMR.plugins.AK={akVars:i,akDNSPreFetchDomain:n,init:function(){if(!_.i){var a=BOOMR.subscribe;a(\\\"before_beacon\\\",_.av,null,null),a(\\\"onbeacon\\\",_.rv,null,null),_.i=!0}return this},is_complete:function(){return!0}}}}()}(window);&lt;/script&gt;&lt;/head&gt; &lt;body&gt; &lt;div id=\\\"pb-root\\\" class=\\\"\\\"&gt;&lt;a class=\\\"skip-link\\\" href=\\\"#header\\\"&gt;Skip to content&lt;/a&gt; &lt;section id=\\\"nav\\\" class=\\\"relative zindex-higher col\\\"&gt; &lt;div class=\\\"wrapper clearfix full pb-feature pb-layout-item pb-f-global-sales-nav\\\" id=\\\"faYVSr1Xz0ktKr\\\"&gt; &lt;div id=\\\"sales-nav\\\" class=\\\"background-grey-minimal visible-lg\\\"&gt; &lt;div class=\\\"container\\\"&gt; &lt;ul class=\\\"list-unstyled flex-container-row padded padded-top padded-bottom padded-xs\\\"&gt; &lt;li class=\\\"padded padded-right padded-md flex-shrink\\\"&gt;&lt;a href=\\\"https://www.kait8.com/2019/05/03/ask-expert/\\\" class=\\\"color-grey-dark\\\"&gt;Ask The Expert&lt;/a&gt;&lt;/li&gt; &lt;li class=\\\"padded padded-right padded-md flex-shrink\\\"&gt;&lt;a href=\\\"https://www.kait8.com/community/energy-alert/\\\" class=\\\"color-grey-dark\\\"&gt;Energy Alert&lt;/a&gt;&lt;/li&gt; &lt;li class=\\\"padded padded-right padded-md flex-shrink\\\"&gt;&lt;a href=\\\"http://www.kait8.com/community/gr8-acts-of-kindness/\\\" class=\\\"color-grey-dark\\\"&gt;GR8 Acts of Kindness&lt;/a&gt;&lt;/li&gt; &lt;li class=\\\"padded padded-right padded-md flex-shrink\\\"&gt;&lt;a href=\\\"http://www.kait8.com/health/medical-minute/\\\" class=\\\"color-grey-dark\\\"&gt;Medical Minute&lt;/a&gt;&lt;/li&gt; &lt;li class=\\\"padded padded-right padded-md flex-shrink\\\"&gt;&lt;a href=\\\"https://kait8.com/roadtrip/\\\" class=\\\"color-grey-dark\\\"&gt;Take a Road Trip&lt;/a&gt;&lt;/li&gt; &lt;li class=\\\"padded padded-right padded-md flex-shrink\\\"&gt;&lt;a href=\\\"https://www.graytvlocal.com/market/jonesboro-ar\\\" class=\\\"color-grey-dark\\\"&gt;Shop Local&lt;/a&gt;&lt;/li&gt; &lt;li class=\\\"padded padded-right padded-md flex-shrink\\\"&gt;&lt;a href=\\\"https://www.kait8.com/bethehero/\\\" class=\\\"color-grey-dark\\\"&gt;Be The Hero Blood Drive&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=\\\"wrapper clearfix full pb-feature pb-layout-item pb-f-test-html-box\\\" id=\\\"f0aKfloXz0ktKr\\\"&gt; &lt;style&gt;.pb-f-article-body .card .img-container.width-full img.width-full{display:block;max-height:calc(100vh - 200px);width:auto;max-width:100%;margin-left:auto;margin-right:auto}.pb-f-article-body .card .img-container.width-full,.pb-f-article-header .card .img-container.width-full{background-color:#f9f9f9}.pb-f-article-header .card .img-container.width-full img.width-full{display:block;max-height:calc(100vh - 150px);width:auto;max-width:100%;margin-left:auto;margin-right:auto}&lt;/style&gt; &lt;/div&gt; &lt;div class=\\\"pb-container\\\"&gt; &lt;/div&gt; &lt;/section&gt; &lt;header id=\\\"sticky-nav\\\" class=\\\"sticky zindex-higher col\\\"&gt; &lt;div class=\\\"pb-container\\\"&gt; &lt;div class=\\\"wrapper clearfix col pb-feature pb-layout-item pb-f-global-navigation-bar\\\" id=\\\"f0ExsO8Xz0ktKr\\\"&gt; &lt;div id=\\\"app-bar\\\" class=\\\"app-bar card padded padded-sm padded-top padded-bottom width-full\\\" role=\\\"navigation\\\"&gt; &lt;div class=\\\"justify-space-between align-items-center flex-container-row relative app-bar-inner container\\\"&gt; &lt;button id=\\\"app-bar-topics-button\\\" name=\\\"topicsMenu\\\" class=\\\"app-button justify-center flex-container-row no-border\\\" data-role=\\\"open-tray\\\" data-tray=\\\"topics\\\"&gt; &lt;span class=\\\"fa fa-2x fa-bars\\\" aria-hidden=\\\"true\\\"&gt;&lt;/span&gt; &lt;/button&gt; &lt;div class=\\\"logo-container flex flex-container-row align-items-center justify-left\\\"&gt; &lt;a aria-label=\\\"Visit homepage\\\" href=\\\"https://www.kait8.com\\\"&gt; &lt;div class=\\\"logo logo-slim\\\"&gt;&lt;/div&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class=\\\"app-bar-top-container flex-3 flex-container-row relative spaced spaced-sm spaced-left spaced-right hidden-sm\\\"&gt; &lt;ul id=\\\"app-bar-top-menu\\\" class=\\\"list-unstyled flex-container-row topics-list uppercase align-items-center flex-shrink\\\"&gt; &lt;li class=\\\"spaced spaced-right spaced-lg li-1\\\"&gt; &lt;h3&gt; &lt;a href=\\\"/news\\\" class=\\\"\\\"&gt;News&lt;/a&gt; &lt;/h3&gt; &lt;/li&gt; &lt;li class=\\\"spaced spaced-right spaced-lg li-1\\\"&gt; &lt;h3&gt; &lt;a href=\\\"/weather\\\" class=\\\"\\\"&gt;Weather&lt;/a&gt; &lt;/h3&gt; &lt;/li&gt; &lt;li class=\\\"spaced spaced-right spaced-lg li-1\\\"&gt; &lt;h3&gt; &lt;a href=\\\"/sports\\\" class=\\\"\\\"&gt;Sports&lt;/a&gt; &lt;/h3&gt; &lt;/li&gt; &lt;li class=\\\"spaced spaced-right spaced-lg li-1\\\"&gt; &lt;h3&gt; &lt;a href=\\\"http://www.kait8.com/video-gallery\\\" class=\\\"\\\"&gt;VIDEO&lt;/a&gt; &lt;/h3&gt; &lt;/li&gt; &lt;li class=\\\"spaced spaced-right spaced-lg li-1\\\"&gt; &lt;h3&gt; &lt;a href=\\\"/health\\\" class=\\\"\\\"&gt;Health&lt;/a&gt; &lt;/h3&gt; &lt;/li&gt; &lt;li class=\\\"spaced spaced-right spaced-lg li-1\\\"&gt; &lt;h3&gt; &lt;a href=\\\"/community\\\" class=\\\"\\\"&gt;Community&lt;/a&gt; &lt;/h3&gt; &lt;/li&gt; &lt;li class=\\\"spaced spaced-right spaced-lg li-1\\\"&gt; &lt;h3&gt; &lt;a href=\\\"/about-us\\\" class=\\\"\\\"&gt;About Us&lt;/a&gt; &lt;/h3&gt; &lt;/li&gt; &lt;li class=\\\"spaced spaced-right spaced-lg li-1\\\"&gt; &lt;h3&gt; &lt;a href=\\\"/contests\\\" class=\\\"\\\"&gt;Contests&lt;/a&gt; &lt;/h3&gt; &lt;/li&gt; &lt;li class=\\\"spaced spaced-right spaced-lg li-1\\\"&gt; &lt;h3&gt; &lt;a href=\\\"http://www.kait8.com/programming/schedule/\\\" class=\\\"\\\"&gt;WHAT'S ON&lt;/a&gt; &lt;/h3&gt; &lt;/li&gt; &lt;li class=\\\"spaced spaced-right spaced-lg li-1\\\"&gt; &lt;h3&gt; &lt;a href=\\\"https://www.graytvlocal.com/market/jonesboro-ar\\\" class=\\\"\\\"&gt;Shop Local&lt;/a&gt; &lt;/h3&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;div class=\\\"search-box-container search-box flex flex-container-row align-items-center justify-end\\\"&gt; &lt;div class=\\\"search-box-inner\\\"&gt; &lt;form class=\\\"search search-expandable-form small\\\" action=\\\"/search/\\\" method=\\\"GET\\\"&gt; &lt;input type=\\\"checkbox\\\" class=\\\"checkbox\\\" name=\\\"search-action\\\" id=\\\"search-action\\\"/&gt; &lt;label class=\\\"magnifying-glass\\\" for=\\\"search-action\\\"&gt; &lt;i class=\\\"search-button fa fa-search\\\"&gt;&lt;/i&gt; &lt;i class=\\\"close-button fa fa-times\\\"&gt;&lt;/i&gt; &lt;/label&gt; &lt;input type=\\\"text\\\" class=\\\"search-text-field\\\" id=\\\"expandable-search\\\" name=\\\"q\\\" placeholder=\\\"Search...\\\"/&gt; &lt;/form&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=\\\"app-bar-tray\\\" data-role=\\\"tray\\\" data-tray=\\\"topics\\\"&gt; &lt;nav [class]=\\\"visible == true ? 'tray left-0 hidden width-full zindex-high tray-open visible' : 'tray left-0 hidden width-full zindex-high hidden'\\\" class=\\\"tray left-0 hidden width-full zindex-high\\\"&gt; &lt;div class=\\\"topics-tray tray-contents card card-dark padded padded-bottom padded-md\\\"&gt; &lt;div class=\\\"tray-contents-inner container\\\" tabindex=\\\"0\\\"&gt; &lt;div class=\\\"five-hundred-screen-height\\\"&gt; &lt;div class=\\\"container\\\"&gt; &lt;div class=\\\"flex-grid\\\"&gt; &lt;div class=\\\"tray-section tray-section-1 tray-section-odd justify-start uppercase links min-width-25 padded padded-md padded-right padded-left spaced spaced-xl spaced-top \\\"&gt; &lt;div class=\\\"tray-section spaced spaced-md spaced-left spaced-right li-1\\\"&gt; &lt;h3 class=\\\"spaced spaced-xs spaced-bottom padded padded-xs padded-right padded-left\\\"&gt; &lt;a href=\\\"/homepage\\\" class=\\\"color-white\\\"&gt;Home&lt;/a&gt; &lt;/h3&gt; &lt;div class=\\\"tray-section-children flex-grid flex-wrap no-transform padded padded-sm padded-bottom g-ulc-1 \\\"&gt; &lt;div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-3\\\"&gt; &lt;a href=\\\"http://www.kait8.com/live\\\" class=\\\"color-white\\\"&gt;Region 8 News Live and Replay&lt;/a&gt; &lt;/div&gt; &lt;div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-4\\\"&gt; &lt;a href=\\\"https://www.kait8.com/roadtrip/\\\" class=\\\"color-white\\\"&gt;Take a Road Trip&lt;/a&gt; &lt;/div&gt; &lt;div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-5\\\"&gt; &lt;a href=\\\"http://www.kait8.com/apps\\\" class=\\\"color-white\\\"&gt;Region 8 Digital Apps&lt;/a&gt; &lt;/div&gt; &lt;div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-6\\\"&gt; &lt;a href=\\\"http://www.kait8.com/video-gallery\\\" class=\\\"color-white\\\"&gt;Recent Video&lt;/a&gt; &lt;/div&gt; &lt;div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-7\\\"&gt; &lt;a href=\\\"http://www.kait8.com/tips/\\\" class=\\\"color-white\\\"&gt;Submit a News Tip&lt;/a&gt; &lt;/div&gt; &lt;div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-8\\\"&gt; &lt;a href=\\\"https://www.kait8.com/community/user-content/\\\" class=\\\"color-white\\\"&gt;See It - Snap It - Send It&lt;/a&gt; &lt;/div&gt; &lt;div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-9\\\"&gt; &lt;a href=\\\"https://www.graytvlocal.com/market/jonesboro-ar\\\" class=\\\"color-white\\\"&gt;Shop Local&lt;/a&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=\\\"tray-section tray-section-2 tray-section-even justify-start uppercase links min-width-25 padded padded-md padded-right padded-left spaced spaced-xl spaced-top \\\"&gt; &lt;div class=\\\"tray-section spaced spaced-md spaced-left spaced-right li-1\\\"&gt; &lt;h3 class=\\\"spaced spaced-xs spaced-bottom padded padded-xs padded-right padded-left\\\"&gt; &lt;a href=\\\"/news\\\" class=\\\"color-white\\\"&gt;News&lt;/a&gt; &lt;/h3&gt; &lt;div class=\\\"tray-section-children flex-grid flex-wrap no-transform padded padded-sm padded-bottom g-ulc-1 \\\"&gt; &lt;div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-3\\\"&gt; &lt;a href=\\\"/news/national\\\" class=\\\"color-white\\\"&gt;National&lt;/a&gt; &lt;/div&gt; &lt;div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-4\\\"&gt; &lt;a href=\\\"/news/entertainment\\\" class=\\\"color-white\\\"&gt;Entertainment&lt;/a&gt; &lt;/div&gt; &lt;div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-5\\\"&gt; &lt;a href=\\\"/news/education\\\" class=\\\"color-white\\\"&gt;Education&lt;/a&gt; &lt;/div&gt; &lt;div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-6\\\"&gt; &lt;a href=\\\"/news/crime\\\" class=\\\"color-white\\\"&gt;Crime&lt;/a&gt; &lt;/div&gt; &lt;div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-7\\\"&gt; &lt;a href=\\\"/politics\\\" class=\\\"color-white\\\"&gt;Politics&lt;/a&gt; &lt;/div&gt; &lt;div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-8\\\"&gt; &lt;a href=\\\"/business\\\" class=\\\"color-white\\\"&gt;Business&lt;/a&gt; &lt;/div&gt; &lt;div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-9\\\"&gt; &lt;a href=\\\"/news/consumer\\\" class=\\\"color-white\\\"&gt;Consumer&lt;/a&gt; &lt;/div&gt; &lt;div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-10\\\"&gt; &lt;a href=\\\"/news/technology\\\" class=\\\"color-white\\\"&gt;Technology&lt;/a&gt; &lt;/div&gt; &lt;div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-11\\\"&gt; &lt;a href=\\\"/news/oddities\\\" class=\\\"color-white\\\"&gt;Oddities&lt;/a&gt; &lt;/div&gt; &lt;div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-12\\\"&gt; &lt;a href=\\\"/news/editorial\\\" class=\\\"color-white\\\"&gt;A Better Region 8&lt;/a&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=\\\"tray-section tray-section-3 tray-section-odd justify-start uppercase links min-width-25 padded padded-md padded-right padded-left spaced spaced-xl spaced-top \\\"&gt; &lt;div class=\\\"tray-section spaced spaced-md spaced-left spaced-right li-1\\\"&gt; &lt;h3 class=\\\"spaced spaced-xs spaced-bottom padded padded-xs padded-right padded-left\\\"&gt; &lt;a href=\\\"/weather\\\" class=\\\"color-white\\\"&gt;Weather&lt;/a&gt; &lt;/h3&gt; &lt;div class=\\\"tray-section-children flex-grid flex-wrap no-transform padded padded-sm padded-bottom g-ulc-1 \\\"&gt; &lt;div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-3\\\"&gt; &lt;a href=\\\"/weather/cams\\\" class=\\\"color-white\\\"&gt;Weather Cams&lt;/a&gt; &lt;/div&gt; &lt;div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-4\\\"&gt; &lt;a href=\\\"https://www.kait8.com/weatheralerts/\\\" class=\\\"color-white\\\"&gt;Weather Alerts&lt;/a&gt; &lt;/div&gt; &lt;div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-5\\\"&gt; &lt;a href=\\\"https://water.weather.gov/ahps2/index.php?wfo=lzk\\\" class=\\\"color-white\\\"&gt;River Stages&lt;/a&gt; &lt;/div&gt; &lt;div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-6\\\"&gt; &lt;a href=\\\"http://www.kait8.com/health/allergies/sneezecast/\\\" class=\\\"color-white\\\"&gt;Sneezecast&lt;/a&gt; &lt;/div&gt; &lt;div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-7\\\"&gt; &lt;a href=\\\"/weather/closings\\\" class=\\\"color-white\\\"&gt;Closings&lt;/a&gt; &lt;/div&gt; &lt;div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-8\\\"&gt; &lt;a href=\\\"http://www.kait8.com/apps/\\\" class=\\\"color-white\\\"&gt;Download Apps&lt;/a&gt; &lt;/div&gt; &lt;div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-9\\\"&gt; &lt;a href=\\\"/weather/tornado\\\" class=\\\"color-white\\\"&gt;NE Arkansas Tornadoes&lt;/a&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=\\\"tray-section tray-section-4 tray-section-even justify-start uppercase links min-width-25 padded padded-md padded-right padded-left spaced spaced-xl spaced-top \\\"&gt; &lt;div class=\\\"tray-section spaced spaced-md spaced-left spaced-right li-1\\\"&gt; &lt;h3 class=\\\"spaced spaced-xs spaced-bottom padded padded-xs padded-right padded-left\\\"&gt; &lt;a href=\\\"/sports\\\" class=\\\"color-white\\\"&gt;Sports&lt;/a&gt; &lt;/h3&gt; &lt;div class=\\\"tray-section-children flex-grid flex-wrap no-transform padded padded-sm padded-bottom g-ulc-1 \\\"&gt; &lt;div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-3\\\"&gt; &lt;a href=\\\"/sports/high-school/basketball\\\" class=\\\"color-white\\\"&gt;Fastbreak Friday Night&lt;/a&gt; &lt;/div&gt; &lt;div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-4\\\"&gt; &lt;a href=\\\"/sports/ncaa/a-state\\\" class=\\\"color-white\\\"&gt;A-State Red Wolves&lt;/a&gt; &lt;/div&gt; &lt;div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-5\\\"&gt; &lt;a href=\\\"/sports/ncaa/arkansas\\\" class=\\\"color-white\\\"&gt;Arkansas Razorbacks&lt;/a&gt; &lt;/div&gt; &lt;div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-6\\\"&gt; &lt;a href=\\\"/sports/nba\\\" class=\\\"color-white\\\"&gt;NBA&lt;/a&gt; &lt;/div&gt; &lt;div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-7\\\"&gt; &lt;a href=\\\"/sports/nhl\\\" class=\\\"color-white\\\"&gt;NHL&lt;/a&gt; &lt;/div&gt; &lt;div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-8\\\"&gt; &lt;a href=\\\"/sports/national\\\" class=\\\"color-white\\\"&gt;National&lt;/a&gt; &lt;/div&gt; &lt;div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-9\\\"&gt; &lt;a href=\\\"/sports/outdoors\\\" class=\\\"color-white\\\"&gt;Outdoors&lt;/a&gt; &lt;/div&gt; &lt;div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-10\\\"&gt; &lt;a href=\\\"/sports/olympics\\\" class=\\\"color-white\\\"&gt;Olympics&lt;/a&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=\\\"tray-section tray-section-5 tray-section-odd justify-start uppercase links min-width-25 padded padded-md padded-right padded-left spaced spaced-xl spaced-top \\\"&gt; &lt;div class=\\\"tray-section spaced spaced-md spaced-left spaced-right li-1\\\"&gt; &lt;h3 class=\\\"spaced spaced-xs spaced-bottom padded padded-xs padded-right padded-left\\\"&gt; &lt;a href=\\\"/community\\\" class=\\\"color-white\\\"&gt;Community&lt;/a&gt; &lt;/h3&gt; &lt;div class=\\\"tray-section-children flex-grid flex-wrap no-transform padded padded-sm padded-bottom g-ulc-1 \\\"&gt; &lt;div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-3\\\"&gt; &lt;a href=\\\"/news/military\\\" class=\\\"color-white\\\"&gt;Military&lt;/a&gt; &lt;/div&gt; &lt;div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-4\\\"&gt; &lt;a href=\\\"/community/religion-today\\\" class=\\\"color-white\\\"&gt;Religion Today&lt;/a&gt; &lt;/div&gt; &lt;div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-5\\\"&gt; &lt;a href=\\\"/community/a-family-for-me\\\" class=\\\"color-white\\\"&gt;A Family for Me&lt;/a&gt; &lt;/div&gt; &lt;div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-6\\\"&gt; &lt;a href=\\\"/community/gr8-acts-of-kindness\\\" class=\\\"color-white\\\"&gt;GR8 Acts of Kindness&lt;/a&gt; &lt;/div&gt; &lt;div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-7\\\"&gt; &lt;a href=\\\"/traffic/gas-prices\\\" class=\\\"color-white\\\"&gt;Pump Patrol&lt;/a&gt; &lt;/div&gt; &lt;div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-8\\\"&gt; &lt;a href=\\\"https://www.kait8.com/community/energy-alert/\\\" class=\\\"color-white\\\"&gt;Energy Alert&lt;/a&gt; &lt;/div&gt; &lt;div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-9\\\"&gt; &lt;a href=\\\"/community/calendar\\\" class=\\\"color-white\\\"&gt;Calendar&lt;/a&gt; &lt;/div&gt; &lt;div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-10\\\"&gt; &lt;a href=\\\"/pets\\\" class=\\\"color-white\\\"&gt;Pets&lt;/a&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=\\\"tray-section tray-section-6 tray-section-even justify-start uppercase links min-width-25 padded padded-md padded-right padded-left spaced spaced-xl spaced-top \\\"&gt; &lt;div class=\\\"tray-section spaced spaced-md spaced-left spaced-right li-1\\\"&gt; &lt;h3 class=\\\"spaced spaced-xs spaced-bottom padded padded-xs padded-right padded-left\\\"&gt; &lt;a href=\\\"/health\\\" class=\\\"color-white\\\"&gt;Health&lt;/a&gt; &lt;/h3&gt; &lt;div class=\\\"tray-section-children flex-grid flex-wrap no-transform padded padded-sm padded-bottom g-ulc-1 \\\"&gt; &lt;div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-3\\\"&gt; &lt;a href=\\\"/health/coronavirus\\\" class=\\\"color-white\\\"&gt;Coronavirus&lt;/a&gt; &lt;/div&gt; &lt;div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-4\\\"&gt; &lt;a href=\\\"/health/medical-minute\\\" class=\\\"color-white\\\"&gt;Medical Minute&lt;/a&gt; &lt;/div&gt; &lt;div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-5\\\"&gt; &lt;a href=\\\"/health/national\\\" class=\\\"color-white\\\"&gt;National&lt;/a&gt; &lt;/div&gt; &lt;div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-6\\\"&gt; &lt;a href=\\\"/food\\\" class=\\\"color-white\\\"&gt;Food&lt;/a&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=\\\"tray-section tray-section-7 tray-section-odd justify-start uppercase links min-width-25 padded padded-md padded-right padded-left spaced spaced-xl spaced-top \\\"&gt; &lt;div class=\\\"tray-section spaced spaced-md spaced-left spaced-right li-1\\\"&gt; &lt;h3 class=\\\"spaced spaced-xs spaced-bottom padded padded-xs padded-right padded-left\\\"&gt; &lt;a href=\\\"/programming\\\" class=\\\"color-white\\\"&gt;Programming&lt;/a&gt; &lt;/h3&gt; &lt;div class=\\\"tray-section-children flex-grid flex-wrap no-transform padded padded-sm padded-bottom g-ulc-1 \\\"&gt; &lt;div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-3\\\"&gt; &lt;a href=\\\"http://www.kait8.com/programming/schedule/\\\" class=\\\"color-white\\\"&gt;What's On KAIT&lt;/a&gt; &lt;/div&gt; &lt;div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-4\\\"&gt; &lt;a href=\\\"https://www.fullcourtgreta.com/\\\" class=\\\"color-white\\\"&gt;Full Court Press with Greta Van Susteren&lt;/a&gt; &lt;/div&gt; &lt;div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-5\\\"&gt; &lt;a href=\\\"http://www.circleallaccess.com\\\" class=\\\"color-white\\\"&gt;Circle - Country Music & Lifestyle&lt;/a&gt; &lt;/div&gt; &lt;div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-6\\\"&gt; &lt;a href=\\\"https://www.graydc.com/\\\" class=\\\"color-white\\\"&gt;Gray DC Bureau&lt;/a&gt; &lt;/div&gt; &lt;div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-7\\\"&gt; &lt;a href=\\\"https://www.investigatetv.com/\\\" class=\\\"color-white\\\"&gt;Investigate TV&lt;/a&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=\\\"tray-section tray-section-8 tray-section-even justify-start uppercase links min-width-25 padded padded-md padded-right padded-left spaced spaced-xl spaced-top \\\"&gt; &lt;div class=\\\"tray-section spaced spaced-md spaced-left spaced-right li-1\\\"&gt; &lt;h3 class=\\\"spaced spaced-xs spaced-bottom padded padded-xs padded-right padded-left\\\"&gt; &lt;a href=\\\"/about-us\\\" class=\\\"color-white\\\"&gt;About Us&lt;/a&gt; &lt;/h3&gt; &lt;div class=\\\"tray-section-children flex-grid flex-wrap no-transform padded padded-sm padded-bottom g-ulc-1 \\\"&gt; &lt;div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-3\\\"&gt; &lt;a href=\\\"https://www.kait8.com/2018/09/25/meet-region-news-team/\\\" class=\\\"color-white\\\"&gt;Meet Our News Team&lt;/a&gt; &lt;/div&gt; &lt;div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-4\\\"&gt; &lt;a href=\\\"https://gray.tv/careers#currentopenings\\\" class=\\\"color-white\\\"&gt;KAIT Careers&lt;/a&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=\\\"tray-section tray-section-9 tray-section-odd justify-start uppercase links min-width-25 padded padded-md padded-right padded-left spaced spaced-xl spaced-top \\\"&gt; &lt;div class=\\\"tray-section spaced spaced-md spaced-left spaced-right li-1\\\"&gt; &lt;h3 class=\\\"spaced spaced-xs spaced-bottom padded padded-xs padded-right padded-left\\\"&gt; &lt;a href=\\\"/contests\\\" class=\\\"color-white\\\"&gt;Contests&lt;/a&gt; &lt;/h3&gt; &lt;div class=\\\"tray-section-children flex-grid flex-wrap no-transform padded padded-sm padded-bottom g-ulc-1 \\\"&gt; &lt;div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-3\\\"&gt; &lt;a href=\\\"http://www.kait8.com/2019/04/16/gr-acts-kindness/\\\" class=\\\"color-white\\\"&gt;GR8 Acts of Kindness Submissions&lt;/a&gt; &lt;/div&gt; &lt;div class=\\\"tray-section-child padded padded-xs padded-right padded-left li-4\\\"&gt; &lt;a href=\\\"http://www.kait8.com/2018/08/28/bartons-weather-umbrella-contest-entry/\\\" class=\\\"color-white\\\"&gt;Barton's Umbrella&lt;/a&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=\\\"tray-section tray-section-10 tray-section-even justify-start uppercase links min-width-25 padded padded-md padded-right padded-left spaced spaced-xl spaced-top \\\"&gt; &lt;div class=\\\"tray-section spaced spaced-md spaced-left spaced-right li-1\\\"&gt; &lt;h3 class=\\\"spaced spaced-xs spaced-bottom padded padded-xs padded-right padded-left\\\"&gt; &lt;a href=\\\"https://www.vuit.com/live/17196/kait\\\" class=\\\"color-white\\\"&gt;Latest Newscasts&lt;/a&gt; &lt;/h3&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/nav&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/header&gt; &lt;section id=\\\"top\\\" class=\\\"width-full\\\"&gt; &lt;div class=\\\"wrapper clearfix full pb-feature pb-layout-item pb-f-global-breaking-news\\\" id=\\\"f0MEpCZXz0ktKr\\\"&gt; &lt;div id=\\\"breaking-news-wrapper\\\" class=\\\"breaking-news-wrapper\\\" data-feature-name=\\\"breaking-news-bar\\\" data-feature-grouping=\\\".pb-f-global-breaking-news\\\" data-page-uri=\\\"/fragment-kait-breaking/\\\" data-url=\\\"/pb/api/v2/render/feature\\\" data-website=\\\"kait\\\" data-view-unpublished=\\\"\\\"&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=\\\"wrapper clearfix full pb-feature pb-layout-item pb-f-weather-weather-alerts\\\" id=\\\"fZgFpz1Xz0ktKr\\\"&gt; &lt;div id=\\\"weather-alerts-wrapper\\\" class=\\\"weather-alerts-wrapper\\\" data-feature-name=\\\"weather-alerts-bar\\\" data-feature-grouping=\\\".pb-f-weather-weather-alerts\\\" data-page-uri=\\\"/kait-weather-alerts/\\\" data-url=\\\"/pb/api/v2/render/feature\\\" data-website=\\\"kait\\\"&gt; &lt;div data-href=\\\"/pb/api/v2/render/feature\\\"&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=\\\"wrapper clearfix full pb-feature pb-layout-item pb-f-ads-arcads\\\" id=\\\"fOcnHR1Xz0ktKr\\\"&gt; &lt;div id=\\\"ad693-sticky\\\" class=\\\"flex-container-column \\\"&gt; &lt;div class=\\\"flex-container-row justify-center \\\"&gt; &lt;div id=\\\"ad693\\\" class=\\\"arcad ad-728x90 zindex-med\\\"&gt;&lt;/div&gt; &lt;script&gt;window.serviceCallbacks.push(function(){window.helpers.includeAd(arcAds,{id:\\\"ad693\\\",slotName:\\\"kait/web/news\\\",adType:\\\"flex-leaderboard\\\",dimensions:\\\"[[[970, 250], [970, 90], [728, 90]], [[728, 90]], [[320, 50]]]\\\",targeting:{position:{\\\"as\\\":\\\"posn\\\"},position_type:\\\"\\\",pt:\\\"a\\\",cid:\\\"QK3SPA45DZEFPGOOKT6HIFCSGQ\\\"},display:\\\"all\\\",sizemap:{breakpoints:\\\"[ [1024, 0], [768, 0], [319, 0] ]\\\",refresh:true},bidding:{prebid:{enabled:true,timeout:1E3,bids:[{bidder:\\\"appnexus\\\",params:{placementId:14567590}}]}}},{lazyload:true})});&lt;/script&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=\\\"pb-container\\\"&gt; &lt;/div&gt; &lt;/section&gt; &lt;div class=\\\"container flex-grid\\\"&gt; &lt;article id=\\\"content-well\\\" class=\\\"col desktop-8 tablet-12 mobile-12\\\"&gt; &lt;div class=\\\"wrapper clearfix col full pb-feature pb-layout-item pb-f-article-header\\\" id=\\\"f0G8x4vXz0ktKr\\\"&gt; &lt;div class=\\\"card spaced card-article width-full spaced-bottom spaced-xs\\\"&gt; &lt;div class=\\\"text-align-left card-content\\\"&gt; &lt;ul class=\\\"tag-list-wrapper list-unstyled flex-container-row text-align-left uppercase unstyled-link spaced spaced-top spaced-sm call-to-action bold\\\"&gt; &lt;/ul&gt; &lt;h1 class=\\\"spaced spaced-xs spaced-top spaced-bottom\\\"&gt;One arrested after police standoff in Blytheville&lt;/h1&gt; &lt;h2 class=\\\"spaced spaced-xs spaced-top\\\"&gt;&lt;/h2&gt; &lt;/div&gt; &lt;div class=\\\"spaced spaced-top spaced-sm\\\"&gt; &lt;div class=\\\"card-content\\\"&gt; &lt;figure class=\\\"\\\"&gt; &lt;div class=\\\"width-full img-container aspect-ratio-2x1\\\"&gt; &lt;img class=\\\"width-full \\\" alt=\\\"One arrested after police standoff in Blytheville\\\" src=\\\"https://www.kait8.com/resizer/t_zzFx7BpJSTC74xbZBml4bdrWI=/1200x600/arc-anglerfish-arc2-prod-raycom.s3.amazonaws.com/public/DB2NJARH4FCNVB7KWHKJGBNQ7Y.jpg\\\"/&gt; &lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=\\\"flex-grid flex-container-row flex-mobile-column col col-no-side-gutter pb-layout-item pb-chain pb-c-article-double-chain full\\\" id=\\\"c0qBKcXXz0ktKr\\\"&gt; &lt;section id=\\\"article-sharebar\\\" class=\\\"col col-no-gutter desktop-1 visible-lg spaced spaced-top spaced-sm\\\"&gt; &lt;div class=\\\"flex flex-container-row sticky\\\"&gt; &lt;span class=\\\"sharebar app-bar-share sharebar flex-container-column text-align-right\\\"&gt; &lt;button class=\\\"social-icon spaced spaced-top spaced-sm\\\" aria-label=\\\"share on twitter\\\" name=\\\"twitter\\\" onclick=\\\"window.open( 'https://twitter.com/intent/tweet?text=One+arrested+after+police+standoff+in+Blytheville&amp;url=https%3A%2F%2Fwww.kait8.com%2Fstory%2F32310666%2Fone-arrested-after-police-standoff-in-blytheville', 'share_popup', 'height=500,width=650' ); return false;\\\" role=\\\"button\\\" data-arctrack=\\\"socialShareTwtr\\\" data-referrer=\\\"\\\" target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\"&gt; &lt;span class=\\\"fa-stack fa-lg\\\"&gt; &lt;i class=\\\"fa fa-circle fa-stack-2x\\\"&gt;&lt;/i&gt; &lt;i class=\\\"fab fa-twitter social-logo icon-light fa-stack-1x icon-light\\\"&gt;&lt;/i&gt; &lt;/span&gt; &lt;/button&gt; &lt;button class=\\\"social-icon spaced spaced-top spaced-sm\\\" name=\\\"facebook\\\" aria-label=\\\"share on facebook\\\" onclick=\\\"window.open( 'https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.kait8.com%2Fstory%2F32310666%2Fone-arrested-after-police-standoff-in-blytheville&amp;title=One+arrested+after+police+standoff+in+Blytheville', 'share_popup', 'height=500,width=650' ); return false;\\\" data-arctrack=\\\"socialShareFb\\\" data-referrer=\\\"\\\" target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\"&gt; &lt;span class=\\\"fa-stack fa-lg\\\"&gt; &lt;i class=\\\"fa fa-circle fa-stack-2x\\\"&gt;&lt;/i&gt; &lt;i class=\\\"fab fa-facebook-f social-logo icon-light fa-stack-1x icon-light\\\"&gt;&lt;/i&gt; &lt;/span&gt; &lt;/button&gt; &lt;a href=\\\"mailto:?subject=One%20arrested%20after%20police%20standoff%20in%20Blytheville&amp;body=Read%20more%20on%20One%20arrested%20after%20police%20standoff%20in%20Blytheville%20at%20https%3A%2F%2Fwww.kait8.com%2Fstory%2F32310666%2Fone-arrested-after-police-standoff-in-blytheville\\\"&gt; &lt;button class=\\\"social-icon spaced spaced-top spaced-sm\\\" role=\\\"button\\\" aria-label=\\\"share story via email\\\" data-arctrack=\\\"socialShareEmail\\\" data-referrer=\\\"\\\" rel=\\\"noopener noreferrer\\\"&gt; &lt;span class=\\\"fa-stack fa-lg\\\"&gt; &lt;i class=\\\"fa fa-circle fa-stack-2x\\\"&gt;&lt;/i&gt; &lt;i class=\\\"far fa-envelope social-logo icon-light fa-stack-1x icon-light\\\"&gt;&lt;/i&gt; &lt;/span&gt; &lt;/button&gt; &lt;/a&gt; &lt;/span&gt; &lt;/div&gt; &lt;/section&gt; &lt;section class=\\\"col col-no-gutter desktop-11 tablet-12 mobile-12 spaced spaced-top spaced-sm\\\"&gt; &lt;div class=\\\"wrapper clearfix full pb-feature pb-layout-item pb-f-article-byline\\\" id=\\\"f0z3yJ5Xz0ktKr\\\"&gt; &lt;div class=\\\"card\\\"&gt; &lt;div class=\\\"byline spaced spaced-bottom spaced-md card-content display-inline\\\"&gt; By &lt;span class=\\\"font-bold color-black\\\"&gt;Region 8 Newsdesk&lt;/span&gt; &lt;span class=\\\"font-bold\\\"&gt;|&lt;/span&gt; &lt;span class=\\\"timestamp\\\"&gt; June 26, 2016 at 8:53 PM CDT - Updated July 2 at 5:06 AM &lt;/span&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=\\\"wrapper clearfix full pb-feature pb-layout-item pb-f-article-body\\\" id=\\\"f0OiAz6Xz0ktKr\\\"&gt; &lt;div class=\\\"collection collection-cards\\\" id=\\\"article-body\\\"&gt; &lt;div data-type=\\\"image\\\" class=\\\"card card-pull-left card-captioned collection-item card-embedded-content\\\"&gt; &lt;div class=\\\"card-content card-article\\\"&gt; &lt;figure class=\\\"\\\"&gt; &lt;div class=\\\"width-full img-container \\\"&gt; &lt;img class=\\\"b-lazy width-full \\\" alt=\\\"(Source: KAIT)\\\" data-src=\\\"https://www.kait8.com/resizer/mrzQQy_83Oh0R4u1gPE0aMjxjmk=/1400x0/arc-anglerfish-arc2-prod-raycom.s3.amazonaws.com/public/LA55CKBPMVGPTJLWXDOGTWCBZM.jpg\\\" src=\\\"https://www.kait8.com/resizer/I01ZrjLWgZ91iaDrcG1ufjUs4YY=/0x10/arc-anglerfish-arc2-prod-raycom.s3.amazonaws.com/public/LA55CKBPMVGPTJLWXDOGTWCBZM.jpg\\\"&gt; &lt;/div&gt; &lt;div&gt; &lt;figcaption class=\\\"caption-text spaced flex-container-row justify-space-between \\\"&gt; (Source: KAIT) &lt;/figcaption&gt; &lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div data-type=\\\"image\\\" class=\\\"card card-pull-left card-captioned collection-item card-embedded-content\\\"&gt; &lt;div class=\\\"card-content card-article\\\"&gt; &lt;figure class=\\\"\\\"&gt; &lt;div class=\\\"width-full img-container \\\"&gt; &lt;img class=\\\"b-lazy width-full \\\" alt=\\\"Hartzell Watson (Source: Blytheville Police Dept.)\\\" data-src=\\\"https://www.kait8.com/resizer/ApJ0IYK3MmE61aBTZkRwf6U_cd4=/1400x0/arc-anglerfish-arc2-prod-raycom.s3.amazonaws.com/public/SMLB7T7EZRESPO7VHEJTZTQG2Q.jpg\\\" src=\\\"https://www.kait8.com/resizer/VT7c8VfAAmL6WreA-8jjNpay75Y=/0x10/arc-anglerfish-arc2-prod-raycom.s3.amazonaws.com/public/SMLB7T7EZRESPO7VHEJTZTQG2Q.jpg\\\"&gt; &lt;/div&gt; &lt;div&gt; &lt;figcaption class=\\\"caption-text spaced flex-container-row justify-space-between \\\"&gt; Hartzell Watson (Source: Blytheville Police Dept.) &lt;/figcaption&gt; &lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;/div&gt; &lt;div data-type=\\\"text\\\" class=\\\"card collection-item \\\"&gt; &lt;div class=\\\"card-content card-article\\\"&gt; &lt;p&gt;BLYTHEVILLE, AR (KAIT) - One man is in custody after a 4-hour police standoff in Blytheville Sunday afternoon.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div data-type=\\\"text\\\" class=\\\"card collection-item \\\"&gt; &lt;div class=\\\"card-content card-article\\\"&gt; &lt;p&gt;According to Captain Scott Adams with the Blytheville Police Department, Hartzell Watson, 44, was arrested after being forced out of a home in the 500-block of North Division.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div data-type=\\\"text\\\" class=\\\"card collection-item \\\"&gt; &lt;div class=\\\"card-content card-article\\\"&gt; &lt;p&gt;Captain Adams said police originally responded to the house after a family dispute.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div data-type=\\\"text\\\" class=\\\"card collection-item \\\"&gt; &lt;div class=\\\"card-content card-article\\\"&gt; &lt;p&gt;When officers arrived, Watson barricaded himself in the house and refused to exit peacefully.&lt;/p&gt; &lt;div class=\\\"flex-container-column align-items-center hidden-lg spaced spaced-top spaced-sm\\\"&gt; &lt;div id=\\\"ad1732-sticky\\\" class=\\\"flex-container-column visible-sm\\\"&gt; &lt;div class=\\\"flex-container-row justify-center background-grey-minimal padded padded-bottom padded-top padded-sm \\\"&gt; &lt;div id=\\\"ad1732\\\" class=\\\"arcad ad-300x250\\\"&gt;&lt;/div&gt; &lt;script&gt;window.serviceCallbacks.push(function(){window.helpers.includeAd(arcAds,{id:\\\"ad1732\\\",slotName:\\\"kait/web/news/QK3SPA45DZEFPGOOKT6HIFCSGQ\\\",adType:\\\"cube\\\",dimensions:\\\"[[[300, 250]], [[300, 250]], [[300, 250]]]\\\",targeting:{position:{\\\"as\\\":\\\"posn\\\"},position_type:\\\"\\\",pt:\\\"\\\",cid:\\\"\\\"},display:\\\"mobile\\\",sizemap:{breakpoints:\\\"[ [1024, 0], [768, 0], [319, 0] ]\\\",refresh:true},bidding:{prebid:{enabled:true,timeout:1E3,bids:[{bidder:\\\"appnexus\\\",params:{placementId:14567590}}]}}},{lazyload:true})});&lt;/script&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div data-type=\\\"text\\\" class=\\\"card collection-item \\\"&gt; &lt;div class=\\\"card-content card-article\\\"&gt; &lt;p&gt;After several hours, police fired smoke and a chemical agent through a window of the home. The gas caught on fire within minutes.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div data-type=\\\"text\\\" class=\\\"card collection-item \\\"&gt; &lt;div class=\\\"card-content card-article\\\"&gt; &lt;p&gt;Officers entered the burning house to find Watson, but he jumped out of an attic window.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div data-type=\\\"text\\\" class=\\\"card collection-item \\\"&gt; &lt;div class=\\\"card-content card-article\\\"&gt; &lt;p&gt;He was checked by paramedics on the scene and was taken into custody.&lt;/p&gt; &lt;div class=\\\"flex-container-column align-items-center hidden-sm spaced spaced-top spaced-sm\\\"&gt; &lt;div id=\\\"ad1376-sticky\\\" class=\\\"flex-container-column \\\"&gt; &lt;div class=\\\"flex-container-row justify-center background-grey-minimal padded padded-bottom padded-top padded-sm \\\"&gt; &lt;div id=\\\"ad1376\\\" class=\\\"arcad ad-300x250\\\"&gt;&lt;/div&gt; &lt;script&gt;window.serviceCallbacks.push(function(){window.helpers.includeAd(arcAds,{id:\\\"ad1376\\\",slotName:\\\"kait/web/news/QK3SPA45DZEFPGOOKT6HIFCSGQ\\\",adType:\\\"cube\\\",dimensions:\\\"[[[300, 250]], [[300, 250]], [[300, 250]]]\\\",targeting:{position:{\\\"as\\\":\\\"posn\\\"},position_type:\\\"\\\",pt:\\\"\\\",cid:\\\"\\\"},display:\\\"all\\\",sizemap:{breakpoints:\\\"[ [1024, 0], [768, 0], [319, 0] ]\\\",refresh:true},bidding:{prebid:{enabled:true,timeout:1E3,bids:[{bidder:\\\"appnexus\\\",params:{placementId:14567590}}]}}},{lazyload:true})});&lt;/script&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div data-type=\\\"text\\\" class=\\\"card collection-item \\\"&gt; &lt;div class=\\\"card-content card-article\\\"&gt; &lt;p&gt;The Blytheville Fire Department was on scene and quickly put the fire out.&lt;/p&gt; &lt;div class=\\\"flex-container-column align-items-center hidden-lg spaced spaced-top spaced-sm\\\"&gt; &lt;div id=\\\"ad485-sticky\\\" class=\\\"flex-container-column visible-sm\\\"&gt; &lt;div class=\\\"flex-container-row justify-center background-grey-minimal padded padded-bottom padded-top padded-sm \\\"&gt; &lt;div id=\\\"ad485\\\" class=\\\"arcad ad-300x250\\\"&gt;&lt;/div&gt; &lt;script&gt;window.serviceCallbacks.push(function(){window.helpers.includeAd(arcAds,{id:\\\"ad485\\\",slotName:\\\"kait/web/news/QK3SPA45DZEFPGOOKT6HIFCSGQ\\\",adType:\\\"cube\\\",dimensions:\\\"[[[300, 250]], [[300, 250]], [[300, 250]]]\\\",targeting:{position:{\\\"as\\\":\\\"posn\\\"},position_type:\\\"\\\",pt:\\\"\\\",cid:\\\"\\\"},display:\\\"mobile\\\",sizemap:{breakpoints:\\\"[ [1024, 0], [768, 0], [319, 0] ]\\\",refresh:true},bidding:{prebid:{enabled:true,timeout:1E3,bids:[{bidder:\\\"appnexus\\\",params:{placementId:14567590}}]}}},{lazyload:true})});&lt;/script&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div data-type=\\\"text\\\" class=\\\"card collection-item \\\"&gt; &lt;div class=\\\"card-content card-article\\\"&gt; &lt;p&gt;Watson was then taken to the Mississippi County Jail. He is being held on state and federal warrants.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div data-type=\\\"text\\\" class=\\\"card collection-item \\\"&gt; &lt;div class=\\\"card-content card-article\\\"&gt; &lt;p&gt;The Arkansas State Police, Mississippi County Sheriff's Office, and the Second Judicial Drug Task Force assisted with the standoff.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div data-type=\\\"text\\\" class=\\\"card collection-item \\\"&gt; &lt;div class=\\\"card-content card-article\\\"&gt; &lt;p&gt;One officer on the scene was treated for dehydration.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div data-type=\\\"text\\\" class=\\\"card collection-item \\\"&gt; &lt;div class=\\\"card-content card-article\\\"&gt; &lt;p&gt;&lt;em&gt;Copyright 2016&nbsp;&lt;a href=\\\"http://www.kait8.com/\\\"&gt;KAIT&lt;/a&gt;. All rights reserved.&lt;/em&gt;&lt;/p&gt; &lt;div class=\\\"flex-container-column align-items-center hidden-lg spaced spaced-top spaced-sm\\\"&gt; &lt;div id=\\\"ad646-sticky\\\" class=\\\"flex-container-column visible-sm\\\"&gt; &lt;div class=\\\"flex-container-row justify-center background-grey-minimal padded padded-bottom padded-top padded-sm \\\"&gt; &lt;div id=\\\"ad646\\\" class=\\\"arcad ad-300x250\\\"&gt;&lt;/div&gt; &lt;script&gt;window.serviceCallbacks.push(function(){window.helpers.includeAd(arcAds,{id:\\\"ad646\\\",slotName:\\\"kait/web/news/QK3SPA45DZEFPGOOKT6HIFCSGQ\\\",adType:\\\"cube\\\",dimensions:\\\"[[[300, 250]], [[300, 250]], [[300, 250]]]\\\",targeting:{position:{\\\"as\\\":\\\"posn\\\"},position_type:\\\"\\\",pt:\\\"\\\",cid:\\\"\\\"},display:\\\"mobile\\\",sizemap:{breakpoints:\\\"[ [1024, 0], [768, 0], [319, 0] ]\\\",refresh:true},bidding:{prebid:{enabled:true,timeout:1E3,bids:[{bidder:\\\"appnexus\\\",params:{placementId:14567590}}]}}},{lazyload:true})});&lt;/script&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div data-type=\\\"text\\\" class=\\\"card collection-item \\\"&gt; &lt;div class=\\\"card-content card-article\\\"&gt; &lt;p&gt;&lt;strong&gt;Watch Region 8 News&nbsp;On Demand:&nbsp;&lt;a href=\\\"http://bit.ly/Region8live\\\"&gt;On your Desktop&lt;/a&gt;&nbsp;|&nbsp;&lt;a href=\\\"http://ftpcontent4.worldnow.com/raycom/mobile/liveplayer/kait.html#vrvextbrowser=yes\\\"&gt;On your Mobile device&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div data-type=\\\"text\\\" class=\\\"card collection-item card-border-bottom card-border-bottom-thick card-border-bottom-dark\\\"&gt; &lt;div class=\\\"card-content card-article\\\"&gt; &lt;p&gt;&lt;strong&gt;Region 8 News App - Install or update on your:&nbsp;&lt;a href=\\\"http://itunes.apple.com/us/app/kait-region-8-news/id449621749?mt=8\\\"&gt;iPhone&lt;/a&gt;&nbsp;|&nbsp;&lt;a href=\\\"https://market.android.com/details?id=com.raycom.kait&amp;feature=search_result\\\"&gt;Android&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=\\\"wrapper clearfix full pb-feature pb-layout-item pb-f-ads-connatix\\\" id=\\\"fwaAKx1Xz0ktKr\\\"&gt; &lt;div class=\\\"flex-grid ad-connatix spaced spaced-lg spaced-top spaced-bottom \\\"&gt; &lt;div class=\\\"col desktop-12 tablet-12 mobile-12\\\"&gt; &lt;div id=\\\"connatix-highlights-embed\\\" class=\\\"connatix-container box-shadow-light\\\"&gt; &lt;script id=\\\"a1c1c57f95064f50bba9396a0b142ced\\\"&gt;cnxps.cmd.push(function(){cnxps({playerId:\\\"cec234f2-82ca-435a-a30a-271018f43d08\\\"}).render(\\\"a1c1c57f95064f50bba9396a0b142ced\\\")});&lt;/script&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=\\\"wrapper clearfix full pb-feature pb-layout-item pb-f-ads-outbrain\\\" id=\\\"f06efnJXz0ktKr\\\"&gt; &lt;div id=\\\"outbrain-article-ad\\\" class=\\\"\\\"&gt; &lt;div class=\\\"OUTBRAIN\\\" data-widget-id=\\\"AR_8\\\" data-src=\\\"https://www.kait8.com/story/32310666/one-arrested-after-police-standoff-in-blytheville\\\" data-ob-template=\\\"raycom\\\"&gt; &lt;/div&gt; &lt;div class=\\\"OUTBRAIN\\\" data-widget-id=\\\"AR_9\\\" data-src=\\\"https://www.kait8.com/story/32310666/one-arrested-after-police-standoff-in-blytheville\\\" data-ob-template=\\\"raycom\\\"&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/section&gt; &lt;/div&gt; &lt;div class=\\\"pb-container\\\"&gt; &lt;/div&gt; &lt;/article&gt; &lt;aside id=\\\"right-rail\\\" class=\\\"col desktop-4 tablet-12 mobile-12 right-rail\\\"&gt; &lt;div class=\\\"pb-container\\\"&gt; &lt;div class=\\\"wrapper clearfix col pb-feature pb-layout-item pb-f-ads-arcads tablet-6 mobile-12 desktop-12\\\" id=\\\"fzMDJM1Xz0ktKr\\\"&gt; &lt;div id=\\\"ad1105-sticky\\\" class=\\\"flex-container-column \\\"&gt; &lt;div class=\\\"flex-container-row justify-center \\\"&gt; &lt;div id=\\\"ad1105\\\" class=\\\"arcad ad-300x250\\\"&gt;&lt;/div&gt; &lt;script&gt;window.serviceCallbacks.push(function(){window.helpers.includeAd(arcAds,{id:\\\"ad1105\\\",slotName:\\\"kait/web/news\\\",adType:\\\"flex-cube\\\",dimensions:\\\"[[[300, 600], [300, 250]], [[300, 250]], [[300, 250]]]\\\",targeting:{position:{\\\"as\\\":\\\"posn\\\"},position_type:\\\"\\\",pt:\\\"a\\\",cid:\\\"QK3SPA45DZEFPGOOKT6HIFCSGQ\\\"},display:\\\"all\\\",sizemap:{breakpoints:\\\"[ [1024, 0], [768, 0], [319, 0] ]\\\",refresh:true},bidding:{prebid:{enabled:true,timeout:1E3,bids:[{bidder:\\\"appnexus\\\",params:{placementId:14567590}}]}}},{lazyload:true})});&lt;/script&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=\\\"wrapper clearfix col desktop-12 tablet-6 mobile-12 pb-feature pb-layout-item pb-f-weather-forecast-box\\\" id=\\\"f47aum1Xz0ktKr\\\"&gt; &lt;div class=\\\"invisible weather-gradient flex-grid color-white pb-weather day\\\" id=\\\"weather-forecast-box\\\" data-valid-content=\\\"true\\\"&gt; &lt;div id=\\\"forecast-box-bg-wrapper\\\" data-current-bg-image=\\\"day-cloudy\\\" data-default-location-code=\\\"USAR0304\\\" class=\\\"weather-item flex pb-weather pb-forecast-box padded padded-md padded-top padded-bottom padded-right padded-left\\\"&gt; &lt;div id=\\\"forecast-box-content\\\"&gt; &lt;input type=\\\"hidden\\\" id=\\\"forecast-box-zip-code\\\" data-url=\\\"/pb/api/v2/render/feature/weather/forecast-box\\\" data-feature-grouping=\\\".pb-f-weather-forecast-box\\\" data-website=\\\"kait\\\" data-location-code=\\\"USAR0304\\\" data-language=\\\"en\\\" data-content-config-keys='Code,Language' data-content-config-values='USAR0304,en' data-custom-field-keys='spanish' data-custom-field-values=''/&gt; &lt;div class=\\\"col spaced spaced-top spaced-sm\\\"&gt; &lt;div class=\\\"flex-container-row justify-center\\\"&gt; &lt;div class=\\\"spaced spaced-right spaced-lg flex-container-row align-items-center\\\"&gt; &lt;a href=\\\"https://www.kait8.com/weather\\\"&gt; &lt;img class=\\\"radar-image\\\" src=\\\"//webpubcontent.raycommedia.com/kait/wximages/SatRad_DMA_640.jpg\\\" width=\\\"100%\\\"/&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class=\\\"flex-container-column\\\"&gt; &lt;div class=\\\"flex-container-row\\\"&gt; &lt;div class=\\\"font-bold temp temp-lg weather-temp temp-lg-forecast-box\\\" data-temp-f=\\\"36\\\" data-temp-c=\\\"2\\\"&gt; 36&lt;/div&gt; &lt;div class=\\\"icon spaced spaced-left spaced-md flex-container-row align-items-center\\\"&gt; &lt;img width=\\\"40px\\\" height=\\\"40px\\\" src=\\\"/pb/resources/images/weather/weather-condition-icons/400x400/67_daily_forecast.png\\\" alt=\\\"weather icon\\\"/&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=\\\"spaced spaced-sm spaced-top text-align-center\\\"&gt; &lt;div class=\\\"flex-container-column relative\\\"&gt; &lt;div class=\\\"text-align-center\\\"&gt; &lt;span class=\\\"location font-bold\\\"&gt; Currently in&lt;br/&gt;Jonesboro, AR&lt;/span&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=\\\"flex-container-row spaced spaced-top spaced-md justify-center\\\"&gt; &lt;a href=\\\"https://www.kait8.com/weather\\\"&gt; &lt;button class=\\\"weather uppercase button button-mini-radius button-transparent button-white-border color-white bold font-sm padded padded-xs padded-left padded-right padded-top padded-bottom\\\"&gt; Full Forecast&lt;/button&gt; &lt;/a&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=\\\"flex-container-row justify-center spaced spaced-md spaced-bottom spaced-top font-semi-bold\\\"&gt; &lt;span class=\\\"spaced spaced-right spaced-sm\\\"&gt;Sponsored By&lt;/span&gt; &lt;div id=\\\"ad1264-sticky\\\" class=\\\"flex-container-column \\\"&gt; &lt;div class=\\\"flex-container-row justify-center \\\"&gt; &lt;div id=\\\"ad1264\\\" class=\\\"arcad ad-120x30\\\"&gt;&lt;/div&gt; &lt;script&gt;window.serviceCallbacks.push(function(){window.helpers.includeAd(arcAds,{id:\\\"ad1264\\\",slotName:\\\"kait/web/weather-widget\\\",adType:\\\"weather-sponsorship\\\",dimensions:\\\"[[[120, 30]], [[120, 30]], [[120, 30]]]\\\",targeting:{position:{\\\"as\\\":\\\"posn\\\"},position_type:\\\"\\\",pt:\\\"weather\\\",cid:\\\"\\\"},display:\\\"all\\\",sizemap:{breakpoints:\\\"[ [1024, 0], [768, 0], [319, 0] ]\\\",refresh:true},bidding:{prebid:{enabled:false,timeout:1E3,bids:[{bidder:\\\"appnexus\\\",params:{placementId:14567590}}]}}},{lazyload:true})});&lt;/script&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=\\\"wrapper clearfix col desktop-12 tablet-12 mobile-12 pb-feature pb-layout-item pb-f-article-author-bio\\\" id=\\\"frRRQ42Xz0ktKr\\\"&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=\\\"wrapper clearfix full pb-feature pb-layout-item pb-f-global-mailchimp-signup\\\" id=\\\"fxwHfN1Xz0ktKr\\\"&gt; &lt;/div&gt; &lt;div class=\\\"col pb-layout-item pb-chain pb-c-single-chain full\\\" id=\\\"c0NRdx1Xz0ktKr\\\"&gt; &lt;div class=\\\"flex-grid\\\"&gt; &lt;section class=\\\"col desktop-12 tablet-12 mobile-12\\\"&gt; &lt;div class=\\\"flex-grid\\\"&gt; &lt;div class=\\\"wrapper clearfix col full pb-feature pb-layout-item pb-f-homepage-story-feed\\\" id=\\\"fhr3sb2Xz0ktKr\\\"&gt; &lt;div class=\\\"flex-feature \\\"&gt; &lt;h3 class=\\\"bold\\\"&gt;&lt;span class=\\\"rectangle\\\"&gt;&lt;/span&gt; RECENT CONTENT&lt;/h3&gt; &lt;div class=\\\"flex-feature height-full flex-grid api-returned-stories\\\" id=\\\"load-more-wrapper-111303\\\" data-content-service=\\\"content-feed\\\"&gt; &lt;div class=\\\"col desktop-12 tablet-6 mobile-12\\\"&gt; &lt;div class=\\\"card skin\\\"&gt; &lt;div class=\\\"card-media width-full\\\"&gt; &lt;div class=\\\"align-items-center\\\"&gt; &lt;div class=\\\"width-full spaced card-media\\\"&gt; &lt;figure class=\\\"\\\"&gt; &lt;div class=\\\"width-full img-container aspect-ratio-2x1\\\"&gt; &lt;a href=\\\"/2021/02/08/city-councilman-arrested-sodomy-kidnapping-charges/\\\"&gt; &lt;img class=\\\"width-full \\\" alt=\\\"City councilman arrested on sodomy, kidnapping charges\\\" src=\\\"https://www.kait8.com/resizer/wg3Y3JODCJ2nJzchyq_R3eHUqZY=/1200x600/cloudfront-us-east-1.images.arcpublishing.com/raycom/L73F4RLYERGQ3ORWPPS7HHYTUA.png\\\"&gt; &lt;/a&gt; &lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=\\\"card card-article-mobile-only width-full flex-container-column\\\"&gt; &lt;h4 class=\\\"promo-header spaced spaced-xs spaced-bottom spaced-top font-normal\\\"&gt;&lt;a class=\\\"unstyled-link \\\" href=\\\"/2021/02/08/city-councilman-arrested-sodomy-kidnapping-charges/\\\"&gt; City councilman arrested on sodomy, kidnapping charges&lt;/a&gt;&lt;/h4&gt; &lt;div class=\\\"spaced color-grey-dark font-md\\\" data-pb-field=\\\"custom.blurbText_\\\" data-pb-url-field=\\\"custom.blurbUrl_\\\" data-pb-field-type=\\\"text\\\" data-pb-placeholder=\\\"#\\\"&gt;The councilman is in the Dunklin County Jail with no bond and is set to be arraigned on Tuesday.&lt;/div&gt; &lt;div class=\\\"byline-timestamp-container flex-container-column flex-desktop-row spaced spaced-top spaced-xs align-items-start\\\"&gt; &lt;div class=\\\"byline-wrapper flex-container-row \\\"&gt; &lt;span class=\\\"byline\\\"&gt;By&nbsp;&lt;/span&gt; &lt;h3 class=\\\"byline bold\\\"&gt; &lt;span&gt;&lt;span&gt;Region 8 Newsdesk&lt;/span&gt;&lt;/span&gt; &lt;/h3&gt; &lt;span class=\\\"spaced spaced-right spaced-md\\\"&gt;&lt;/span&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=\\\"col desktop-12 tablet-6 mobile-12\\\"&gt; &lt;div class=\\\"card skin\\\"&gt; &lt;div class=\\\"card-media width-full\\\"&gt; &lt;div class=\\\"align-items-center\\\"&gt; &lt;div class=\\\"width-full spaced card-media\\\"&gt; &lt;figure class=\\\"\\\"&gt; &lt;div class=\\\"width-full img-container aspect-ratio-2x1\\\"&gt; &lt;a href=\\\"/2021/02/08/osceola-police-investigate-friday-shooting/\\\"&gt; &lt;img class=\\\"width-full \\\" alt=\\\"Osceola police investigate Friday shooting\\\" src=\\\"https://www.kait8.com/resizer/MwxZfroPz_VTamfctCMR99zL37Q=/1200x600/cloudfront-us-east-1.images.arcpublishing.com/raycom/HPPQUSEVK5GANPXSANIHAFZXAA.jpg\\\"&gt; &lt;/a&gt; &lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=\\\"card card-article-mobile-only width-full flex-container-column\\\"&gt; &lt;h4 class=\\\"promo-header spaced spaced-xs spaced-bottom spaced-top font-normal\\\"&gt;&lt;a class=\\\"unstyled-link \\\" href=\\\"/2021/02/08/osceola-police-investigate-friday-shooting/\\\"&gt; Osceola police investigate Friday shooting&lt;/a&gt;&lt;/h4&gt; &lt;div class=\\\"spaced color-grey-dark font-md\\\" data-pb-field=\\\"custom.blurbText_\\\" data-pb-url-field=\\\"custom.blurbUrl_\\\" data-pb-field-type=\\\"text\\\" data-pb-placeholder=\\\"#\\\"&gt;The Osceola Police Department is investigating a Friday evening shooting that took place at the local Express Mart.&lt;/div&gt; &lt;div class=\\\"byline-timestamp-container flex-container-column flex-desktop-row spaced spaced-top spaced-xs align-items-start\\\"&gt; &lt;div class=\\\"byline-wrapper flex-container-row \\\"&gt; &lt;span class=\\\"byline\\\"&gt;By&nbsp;&lt;/span&gt; &lt;h3 class=\\\"byline bold\\\"&gt; &lt;span&gt;&lt;span&gt;Region 8 Newsdesk&lt;/span&gt;&lt;/span&gt; &lt;/h3&gt; &lt;span class=\\\"spaced spaced-right spaced-md\\\"&gt;&lt;/span&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/section&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=\\\"wrapper clearfix full pb-feature pb-layout-item pb-f-ads-arcads\\\" id=\\\"f09JMK9Xz0ktKr\\\"&gt; &lt;div id=\\\"ad100-sticky\\\" class=\\\"flex-container-column \\\"&gt; &lt;div class=\\\"flex-container-row justify-center \\\"&gt; &lt;div id=\\\"ad100\\\" class=\\\"arcad ad-300x250\\\"&gt;&lt;/div&gt; &lt;script&gt;window.serviceCallbacks.push(function(){window.helpers.includeAd(arcAds,{id:\\\"ad100\\\",slotName:\\\"kait/web/news\\\",adType:\\\"cube\\\",dimensions:\\\"[[[300, 250]], [[300, 250]], [[300, 250]]]\\\",targeting:{position:{\\\"as\\\":\\\"posn\\\"},position_type:\\\"\\\",pt:\\\"a\\\",cid:\\\"QK3SPA45DZEFPGOOKT6HIFCSGQ\\\"},display:\\\"all\\\",sizemap:{breakpoints:\\\"[ [1024, 0], [768, 0], [319, 0] ]\\\",refresh:true},bidding:{prebid:{enabled:true,timeout:1E3,bids:[{bidder:\\\"appnexus\\\",params:{placementId:14567590}}]}}},{lazyload:true})});&lt;/script&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=\\\"col pb-layout-item pb-chain pb-c-single-chain full\\\" id=\\\"c008fu9Xz0ktKr\\\"&gt; &lt;div class=\\\"flex-grid\\\"&gt; &lt;section class=\\\"col desktop-12 tablet-12 mobile-12\\\"&gt; &lt;div class=\\\"flex-grid\\\"&gt; &lt;div class=\\\"wrapper clearfix col full pb-feature pb-layout-item pb-f-homepage-story-feed\\\" id=\\\"fmrOMh1Xz0ktKr\\\"&gt; &lt;div class=\\\"flex-feature \\\"&gt; &lt;div class=\\\"flex-feature height-full flex-grid api-returned-stories\\\" id=\\\"load-more-wrapper-10996\\\" data-content-service=\\\"content-feed\\\"&gt; &lt;div class=\\\"col desktop-12 tablet-6 mobile-12\\\"&gt; &lt;div class=\\\"card skin\\\"&gt; &lt;div class=\\\"card-media width-full\\\"&gt; &lt;div class=\\\"align-items-center flex-container-row justify-space-between\\\"&gt; &lt;div class=\\\"width-full spaced card-media flex-reverse spaced-left spaced-lg flex\\\"&gt; &lt;figure class=\\\"\\\"&gt; &lt;div class=\\\"width-full img-container aspect-ratio-4x3\\\"&gt; &lt;a href=\\\"/2021/02/08/report-terry-mohajir-target-ucf-athletic-director-search/\\\"&gt; &lt;img class=\\\"width-full \\\" alt=\\\"Report: Terry Mohajir a target in UCF athletic director search\\\" src=\\\"https://www.kait8.com/resizer/OhQ0t9syUaQKZTKiI4W49pt8yMw=/1200x900/cloudfront-us-east-1.images.arcpublishing.com/raycom/YK3QUVJ5LZDY3EXMWCRPUP6TVM.jpg\\\"&gt; &lt;/a&gt; &lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=\\\"card card-article-mobile-only width-full flex-container-column flex-2\\\"&gt; &lt;h4 class=\\\"promo-header spaced spaced-xs spaced-bottom spaced-top font-normal\\\"&gt;&lt;a class=\\\"unstyled-link \\\" href=\\\"/2021/02/08/report-terry-mohajir-target-ucf-athletic-director-search/\\\"&gt; Report: Terry Mohajir a target in UCF athletic director search&lt;/a&gt;&lt;/h4&gt; &lt;div class=\\\"byline-timestamp-container flex-container-column flex-desktop-row spaced spaced-top spaced-xs align-items-start\\\"&gt; &lt;div class=\\\"byline-wrapper flex-container-row hidden\\\"&gt; &lt;span class=\\\"byline\\\"&gt;By&nbsp;&lt;/span&gt; &lt;h3 class=\\\"byline bold\\\"&gt; &lt;span&gt;&lt;span&gt;Chris Hudgison&lt;/span&gt;&lt;/span&gt; &lt;/h3&gt; &lt;span class=\\\"spaced spaced-right spaced-md\\\"&gt;&lt;/span&gt; &lt;/div&gt; &lt;div class=\\\"font-sm color-grey byline-wrapper flex-container-row \\\"&gt; Published 1h at 10:45 AM &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=\\\"col desktop-12 tablet-6 mobile-12\\\"&gt; &lt;div class=\\\"card skin\\\"&gt; &lt;div class=\\\"card-media width-full\\\"&gt; &lt;div class=\\\"align-items-center flex-container-row justify-space-between\\\"&gt; &lt;div class=\\\"width-full spaced card-media flex-reverse spaced-left spaced-lg flex\\\"&gt; &lt;figure class=\\\"\\\"&gt; &lt;div class=\\\"width-full img-container aspect-ratio-4x3\\\"&gt; &lt;a href=\\\"/2021/02/08/harding-university-honor-murdered-alumnus/\\\"&gt; &lt;img class=\\\"width-full \\\" alt=\\\"Harding University to honor murdered alumnus\\\" src=\\\"https://www.kait8.com/resizer/aqxTbm7wchoZHFCmoB5F3uv-SvU=/1200x900/cloudfront-us-east-1.images.arcpublishing.com/raycom/4KYQ6HNEIZEKNG2QUHSE4SRBRI.jpg\\\"&gt; &lt;/a&gt; &lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=\\\"card card-article-mobile-only width-full flex-container-column flex-2\\\"&gt; &lt;h4 class=\\\"promo-header spaced spaced-xs spaced-bottom spaced-top font-normal\\\"&gt;&lt;a class=\\\"unstyled-link \\\" href=\\\"/2021/02/08/harding-university-honor-murdered-alumnus/\\\"&gt; Harding University to honor murdered alumnus&lt;/a&gt;&lt;/h4&gt; &lt;div class=\\\"byline-timestamp-container flex-container-column flex-desktop-row spaced spaced-top spaced-xs align-items-start\\\"&gt; &lt;div class=\\\"byline-wrapper flex-container-row hidden\\\"&gt; &lt;span class=\\\"byline\\\"&gt;By&nbsp;&lt;/span&gt; &lt;h3 class=\\\"byline bold\\\"&gt; &lt;span&gt;&lt;span&gt;Region 8 Newsdesk&lt;/span&gt;&lt;/span&gt; &lt;/h3&gt; &lt;span class=\\\"spaced spaced-right spaced-md\\\"&gt;&lt;/span&gt; &lt;/div&gt; &lt;div class=\\\"font-sm color-grey byline-wrapper flex-container-row \\\"&gt; Published 1h at 10:34 AM &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=\\\"col desktop-12 tablet-6 mobile-12\\\"&gt; &lt;div class=\\\"card skin\\\"&gt; &lt;div class=\\\"card-media width-full\\\"&gt; &lt;div class=\\\"align-items-center flex-container-row justify-space-between\\\"&gt; &lt;div class=\\\"width-full spaced card-media flex-reverse spaced-left spaced-lg flex\\\"&gt; &lt;figure class=\\\"\\\"&gt; &lt;div class=\\\"width-full img-container aspect-ratio-4x3\\\"&gt; &lt;a href=\\\"/2021/02/08/lagging-production-driving-up-gas-prices-arkansas/\\\"&gt; &lt;img class=\\\"width-full \\\" alt=\\\"Lagging production driving up gas prices in Arkansas\\\" src=\\\"https://www.kait8.com/resizer/7OLbSYpfhs-xAK7abU8dfrZrfSU=/1200x900/cloudfront-us-east-1.images.arcpublishing.com/raycom/RUYNYT6THBF4FDLIZJCYPUXPBE.png\\\"&gt; &lt;/a&gt; &lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=\\\"card card-article-mobile-only width-full flex-container-column flex-2\\\"&gt; &lt;h4 class=\\\"promo-header spaced spaced-xs spaced-bottom spaced-top font-normal\\\"&gt;&lt;a class=\\\"unstyled-link \\\" href=\\\"/2021/02/08/lagging-production-driving-up-gas-prices-arkansas/\\\"&gt; Lagging production driving up gas prices in Arkansas&lt;/a&gt;&lt;/h4&gt; &lt;div class=\\\"byline-timestamp-container flex-container-column flex-desktop-row spaced spaced-top spaced-xs align-items-start\\\"&gt; &lt;div class=\\\"byline-wrapper flex-container-row hidden\\\"&gt; &lt;span class=\\\"byline\\\"&gt;By&nbsp;&lt;/span&gt; &lt;h3 class=\\\"byline bold\\\"&gt; &lt;span&gt;&lt;span&gt;Region 8 Newsdesk&lt;/span&gt;&lt;/span&gt; &lt;/h3&gt; &lt;span class=\\\"spaced spaced-right spaced-md\\\"&gt;&lt;/span&gt; &lt;/div&gt; &lt;div class=\\\"font-sm color-grey byline-wrapper flex-container-row \\\"&gt; Published 3h at 8:26 AM &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=\\\"col desktop-12 tablet-6 mobile-12\\\"&gt; &lt;div class=\\\"card skin\\\"&gt; &lt;div class=\\\"card-media width-full\\\"&gt; &lt;div class=\\\"align-items-center flex-container-row justify-space-between\\\"&gt; &lt;div class=\\\"width-full spaced card-media flex-reverse spaced-left spaced-lg flex\\\"&gt; &lt;figure class=\\\"\\\"&gt; &lt;div class=\\\"width-full img-container aspect-ratio-4x3\\\"&gt; &lt;a href=\\\"/2021/02/08/police-say-missing-year-old-has-been-found-safe/\\\"&gt; &lt;img class=\\\"width-full \\\" alt=\\\"Police say missing 10-year-old has been found safe\\\" src=\\\"https://www.kait8.com/resizer/e177Sz6cFkOH2rm-RA-DCnARN-Y=/1200x900/cloudfront-us-east-1.images.arcpublishing.com/raycom/MTKXIGEMNVF5BHM2HT3P5Y7J5I.jpg\\\"&gt; &lt;/a&gt; &lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;div class=\\\"card card-article-mobile-only width-full flex-container-column flex-2\\\"&gt; &lt;h4 class=\\\"promo-header spaced spaced-xs spaced-bottom spaced-top font-normal\\\"&gt;&lt;a class=\\\"unstyled-link \\\" href=\\\"/2021/02/08/police-say-missing-year-old-has-been-found-safe/\\\"&gt; Police say missing 10-year-old has been found safe&lt;/a&gt;&lt;/h4&gt; &lt;div class=\\\"byline-timestamp-container flex-container-column flex-desktop-row spaced spaced-top spaced-xs align-items-start\\\"&gt; &lt;div class=\\\"byline-wrapper flex-container-row hidden\\\"&gt; &lt;span class=\\\"byline\\\"&gt;By&nbsp;&lt;/span&gt; &lt;h3 class=\\\"byline bold\\\"&gt; &lt;span&gt;WAFB Staff&lt;/span&gt; &lt;/h3&gt; &lt;span class=\\\"spaced spaced-right spaced-md\\\"&gt;&lt;/span&gt; &lt;/div&gt; &lt;div class=\\\"font-sm color-grey byline-wrapper flex-container-row \\\"&gt; Published 3h at 8:24 AM &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/section&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=\\\"pb-container\\\"&gt; &lt;/div&gt; &lt;/aside&gt; &lt;div id=\\\"recirc\\\" class=\\\"col desktop-12 tablet-12 mobile-12\\\"&gt; &lt;/div&gt; &lt;/div&gt; &lt;section id=\\\"bottom\\\" class=\\\"col desktop-12 tablet-12 mobile-12\\\"&gt; &lt;div class=\\\"wrapper clearfix full pb-feature pb-layout-item pb-f-ads-arcads\\\" id=\\\"f0XShVkXz0ktKr\\\"&gt; &lt;div id=\\\"ad554-sticky\\\" class=\\\"flex-container-column \\\"&gt; &lt;div class=\\\"flex-container-row justify-center \\\"&gt; &lt;div id=\\\"ad554\\\" class=\\\"arcad ad-728x90 zindex-med\\\"&gt;&lt;/div&gt; &lt;script&gt;window.serviceCallbacks.push(function(){window.helpers.includeAd(arcAds,{id:\\\"ad554\\\",slotName:\\\"kait/web/news\\\",adType:\\\"flex-leaderboard\\\",dimensions:\\\"[[[970, 250], [970, 90], [728, 90]], [[728, 90]], [[320, 50]]]\\\",targeting:{position:{\\\"as\\\":\\\"posn\\\"},position_type:\\\"\\\",pt:\\\"a\\\",cid:\\\"QK3SPA45DZEFPGOOKT6HIFCSGQ\\\"},display:\\\"all\\\",sizemap:{breakpoints:\\\"[ [1024, 0], [768, 0], [319, 0] ]\\\",refresh:true},bidding:{prebid:{enabled:true,timeout:1E3,bids:[{bidder:\\\"appnexus\\\",params:{placementId:14567590}}]}}},{lazyload:true})});&lt;/script&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=\\\"wrapper clearfix full pb-feature pb-layout-item pb-f-global-footer\\\" id=\\\"fU2oFF1Xz0ktKr\\\"&gt; &lt;footer class=\\\"card card-dark padded padded-bottom padded-0\\\"&gt; &lt;div class=\\\"container\\\"&gt; &lt;div class=\\\"container spaced spaced-sm spaced-left\\\"&gt; &lt;div class=\\\"flex flex-container-column flex-tablet-row flex-desktop-row width-full color-white\\\"&gt; &lt;div class=\\\"spaced spaced-right spaced-bottom spaced-xl\\\"&gt; &lt;div class=\\\"logo-container flex flex-container-row align-items-center justify-left\\\"&gt; &lt;a aria-label=\\\"Visit homepage\\\" href=\\\"https://www.kait8.com\\\"&gt; &lt;div class=\\\"logo logo-large logo-footer\\\"&gt;&lt;/div&gt; &lt;/a&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=\\\"spaced spaced-right spaced-xl footer-mission\\\"&gt; &lt;p class=\\\"spaced spaced-bottom spaced-xs\\\"&gt; 472 CR 766&lt;br/&gt;Jonesboro, AR 72401&lt;br/&gt;(870) 931-8888 &lt;/p&gt; &lt;a class=\\\"call-to-action uppercase color-white\\\" href=\\\"http://www.kait8.com/about-us\\\"&gt;Contact Us &lt;i class=\\\"fa fa-caret-right\\\" aria-hidden=\\\"true\\\"&gt;&lt;/i&gt;&lt;/a&gt; &lt;div id=\\\"ccpa-link\\\" class=\\\"call-to-action ccpa-dynamic-insertion-container uppercase color-white\\\"&gt;&lt;/div&gt; &lt;div class=\\\"spaced spaced-top spaced-xl hidden-lg\\\"&gt; &lt;ul class=\\\"list-unstyled call-to-action uppercase\\\"&gt; &lt;li&gt;&lt;a href=\\\"http://publicfiles.fcc.gov/tv-profile/kait\\\" class=\\\"color-white\\\"&gt;FCC Public File&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=\\\"mailto:publicfile@kait8.com\\\" class=\\\"color-white\\\"&gt;publicfile@kait8.com &lt;br/&gt; (870) 336-1817&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=\\\"http://webpubcontent.raycommedia.com/kait/PDF/latestKAITEEO.pdf\\\" class=\\\"color-white\\\"&gt;EEO&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=\\\"https://webpubcontent.gray.tv/gdm/fcc/kait-fcc_applications.pdf\\\" class=\\\"color-white\\\"&gt;FCC Applications&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=\\\"http://www.kait8.com/2018/08/28/kait-closed-captioning-service-support/\\\" class=\\\"color-white\\\"&gt;Closed Captioning&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=\\\"https://gray.tv/careers#currentopenings\\\" class=\\\"color-white\\\"&gt;KAIT 8 Careers&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=\\\"/privacy-policy/\\\" class=\\\"color-white\\\"&gt;Privacy Policy&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=\\\"/terms-of-service/\\\" class=\\\"color-white\\\"&gt;Terms of Service&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;div class=\\\"spaced spaced-top spaced-xl hidden-lg\\\"&gt; &lt;span class=\\\"buttonbar app-bar-share flex-container-grid width-full text-align-right\\\"&gt; &lt;a class=\\\"button button-icon button-icon-hover-accent button-lg footer-social-icon\\\" href=\\\"https://twitter.com/Region8News\\\" role=\\\"button\\\" aria-label=\\\"Visit Twitter\\\" target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\"&gt; &lt;i class=\\\"fab fa-twitter \\\"&gt;&lt;/i&gt; &lt;/a&gt; &lt;a class=\\\"button button-icon button-icon-hover-accent button-lg footer-social-icon\\\" href=\\\"https://www.facebook.com/Region8News\\\" role=\\\"button\\\" aria-label=\\\"Visit Facebook\\\" target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\"&gt; &lt;i class=\\\"fab fa-facebook-square \\\"&gt;&lt;/i&gt; &lt;/a&gt; &lt;/span&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=\\\"spaced spaced-right spaced-xl visible-lg\\\"&gt; &lt;ul class=\\\"list-unstyled call-to-action uppercase\\\"&gt; &lt;li&gt;&lt;a href=\\\"http://publicfiles.fcc.gov/tv-profile/kait\\\" class=\\\"color-white\\\"&gt;FCC Public File&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=\\\"mailto:publicfile@kait8.com\\\" class=\\\"color-white\\\"&gt;publicfile@kait8.com &lt;br/&gt; (870) 336-1817&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=\\\"http://webpubcontent.raycommedia.com/kait/PDF/latestKAITEEO.pdf\\\" class=\\\"color-white\\\"&gt;EEO&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=\\\"https://webpubcontent.gray.tv/gdm/fcc/kait-fcc_applications.pdf\\\" class=\\\"color-white\\\"&gt;FCC Applications&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=\\\"http://www.kait8.com/2018/08/28/kait-closed-captioning-service-support/\\\" class=\\\"color-white\\\"&gt;Closed Captioning&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=\\\"https://gray.tv/careers#currentopenings\\\" class=\\\"color-white\\\"&gt;KAIT 8 Careers&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=\\\"/privacy-policy/\\\" class=\\\"color-white\\\"&gt;Privacy Policy&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=\\\"/terms-of-service/\\\" class=\\\"color-white\\\"&gt;Terms of Service&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;div class=\\\"visible-lg\\\"&gt; &lt;span class=\\\"buttonbar app-bar-share flex-container-grid width-full text-align-right\\\"&gt; &lt;a class=\\\"button button-icon button-icon-hover-accent button-lg footer-social-icon\\\" href=\\\"https://twitter.com/Region8News\\\" role=\\\"button\\\" aria-label=\\\"Visit Twitter\\\" target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\"&gt; &lt;i class=\\\"fab fa-twitter \\\"&gt;&lt;/i&gt; &lt;/a&gt; &lt;a class=\\\"button button-icon button-icon-hover-accent button-lg footer-social-icon\\\" href=\\\"https://www.facebook.com/Region8News\\\" role=\\\"button\\\" aria-label=\\\"Visit Facebook\\\" target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\"&gt; &lt;i class=\\\"fab fa-facebook-square \\\"&gt;&lt;/i&gt; &lt;/a&gt; &lt;/span&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=\\\"footer-copyright width-full bordered bordered-top-darken spaced spaced-top spaced-sm\\\"&gt; &lt;div class=\\\"container\\\"&gt; &lt;p class=\\\"font-sm color-white text-align-center spaced spaced-sm spaced-top spaced-bottom\\\"&gt; &lt;span&gt;A Gray Media Group, Inc. Station - &copy; 2002-2021 Gray Television, Inc.&lt;/span&gt; &lt;/p&gt; &lt;div class=\\\"footer-corp-logo width-full spaced spaced-sm spaced-top spaced-bottom\\\"&gt;&lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/footer&gt; &lt;/div&gt; &lt;div class=\\\"pb-container\\\"&gt; &lt;/div&gt; &lt;/section&gt; &lt;/div&gt; &lt;script async=\\\"async\\\" src=\\\"https://widgets.outbrain.com/outbrain.js\\\"&gt;&lt;/script&gt; &lt;script async src='/pb/gr/p/default/rHJ9vP1Xz0ktKr/render.js?v=101'&gt;&lt;/script&gt; &lt;script&gt;(function(){window.pageBuilder=window.pageBuilder||{};var testIdToFeatureId={};var status={};var TIMEOUT=1500;var TIME_NOW=Date.now();var BANDITO_IS_ADMIN=null;var BANDITO_PERF_API=null;var BANDITO_TESTS_API=null;var BANDITO_EVENTS_API=null;var supportsPassive=false;try{var opts=Object.defineProperty({},\\\"passive\\\",{get:function(){supportsPassive=true}});window.addEventListener(\\\"test\\\",null,opts)}catch(e){}var visibilityCallbacks={};function visibilityCallback(){var config=null;var element=null;var callback=\\nnull;var body=document.body;var html=document.documentElement;var height=html.clientHeight||body.clientHeight;for(var key in visibilityCallbacks)if(visibilityCallbacks.hasOwnProperty(key)){config=visibilityCallbacks[key];if(config){element=config[0];callback=config[1];var boundaries=element.getBoundingClientRect();if(boundaries.top&gt;0&&boundaries.top&lt;height){callback();visibilityCallbacks[key]=null}}}}window.addEventListener(\\\"resize\\\",visibilityCallback,supportsPassive?{passive:true}:false);document.addEventListener(\\\"scroll\\\",\\nvisibilityCallback,supportsPassive?{passive:true}:false);function ajax(url,success,failure,timeout){var xhr=new XMLHttpRequest;xhr.open(\\\"GET\\\",url);xhr.onreadystatechange=function(){if(xhr.readyState==4)if(xhr.status===200)try{var response=JSON.parse(xhr.responseText);success(response)}catch(e){failure()}else failure()};if(timeout){xhr.ontimeout=failure;xhr.timeout=timeout}xhr.send()}function perf(status){var elapsed=Date.now()-TIME_NOW;var url=BANDITO_PERF_API+\\\"?status\\\\x3d\\\"+status+\\\"\\\\x26time\\\\x3d\\\"+\\nelapsed;ajax(url,function(){},function(){})}function showDefaultVariants(){for(var id in window._variantFeatureIds)if(window._variantFeatureIds.hasOwnProperty(id)){perf(\\\"failure\\\");showDefaultVariant(window._variantFeatureIds[id],false)}}function iterateFeatures(testId,callback){var id,feature;for(id in window._variantFeatureIds)if(window._variantFeatureIds.hasOwnProperty(id))if(window._variantFeatureIds[id]==testId){feature=document.getElementById(id);if(feature)callback(feature)}}function showDefaultVariant(testId,\\nsuccess){iterateFeatures(testId,function(feature){variantLoaded(testId,testId,feature,success)})}function showOtherVariant(testId,winner,timeLeft){var uri=window._context+\\\"/api/v2/render/feature/variant/\\\"+winner+\\\"?rid\\\\x3d\\\"+window._rid+\\\"\\\\x26uri\\\\x3d\\\"+window._uri+\\\"\\\\x26outputType\\\\x3d\\\"+window._outputType;ajax(uri,function(response){iterateFeatures(testId,function(feature){var resources=response.pageResources||{};var parent=feature.parentNode;var div=document.createElement(\\\"div\\\");div.innerHTML=response.rendering.trim();\\nvar newFeature=div.childNodes[0];parent.replaceChild(newFeature,feature);var scripts=Array.prototype.slice.call(newFeature.getElementsByTagName(\\\"script\\\"));for(var i=0;i&lt;scripts.length;i++)if(scripts[i][\\\"type\\\"]==\\\"\\\"||scripts[i][\\\"type\\\"]==\\\"text/javascript\\\")if(scripts[i].src!=\\\"\\\"){var s=document.createElement(\\\"script\\\");s.setAttribute(\\\"src\\\",scripts[i].src);document.body.appendChild(s)}else eval(scripts[i].innerHTML);for(var file in resources)if(resources.hasOwnProperty(file)&&file.endsWith(\\\".js\\\")){var script=\\ndocument.createElement(\\\"script\\\");script.setAttribute(\\\"src\\\",resources[file]);document.body.appendChild(script)}perf(\\\"success-variant\\\");variantLoaded(testId,winner,newFeature,true)})},function(){iterateFeatures(testId,function(feature){perf(\\\"failure-step2\\\");variantLoaded(testId,testId,feature,false)})},timeLeft)}function variantLoaded(testId,variantId,feature,success){var tag=testId+\\\"-\\\"+feature.id;if(!status[tag]){status[tag]=true;feature.style.visibility=\\\"visible\\\";if(success){feature.addEventListener(\\\"click\\\",\\nfunction(event){var url=null;var target=event.target;while(target&&!(target.tagName.toUpperCase()==\\\"A\\\")&&!(target==feature))target=target.parentNode;if(target.tagName.toUpperCase()==\\\"A\\\"){url=target.href;if(url){var request=new XMLHttpRequest;request.open(\\\"POST\\\",BANDITO_EVENTS_API,true);request.setRequestHeader(\\\"Content-Type\\\",\\\"application/json\\\");request.send(JSON.stringify({event:\\\"clicked\\\",test_id:testId,variant_id:variantId}));if(event.button&gt;0||event.ctrlKey||event.metaKey||event.shiftKey);else{event.preventDefault();\\nsetTimeout(function(){document.location=url},250)}}}});visibilityCallbacks[tag]=[feature,function(){var request=new XMLHttpRequest;request.open(\\\"POST\\\",BANDITO_EVENTS_API,true);request.setRequestHeader(\\\"Content-Type\\\",\\\"application/json\\\");request.send(JSON.stringify({event:\\\"served\\\",test_id:testId,variant_id:variantId}))}];visibilityCallback()}}}function loadVariants(tests){var testsFound={},testId,winner,timeLeft;if(tests&&tests[\\\"length\\\"]){for(var i=0;i&lt;tests.length;i++){winner=tests[i][\\\"winner\\\"];testId=\\ntests[i][\\\"_id\\\"];testsFound[testId]=true;if(testId==winner){perf(\\\"success-default\\\");showDefaultVariant(testId,true)}else{timeLeft=TIMEOUT-(Date.now()-TIME_NOW);if(timeLeft&gt;50)showOtherVariant(testId,winner,timeLeft);else{perf(\\\"timeout-step1\\\");showDefaultVariant(testId,false)}}}for(var id in window._variantFeatureIds)if(window._variantFeatureIds.hasOwnProperty(id)){testId=window._variantFeatureIds[id];if(!testsFound[testId]){perf(\\\"not-found\\\");showDefaultVariant(testId,false)}}}else showDefaultVariants()}\\nwindow.pageBuilder.initVariants=function(opts){opts=opts||{};TIMEOUT=opts[\\\"TIMEOUT\\\"];BANDITO_IS_ADMIN=opts[\\\"IS_ADMIN\\\"];BANDITO_PERF_API=opts[\\\"PERF_API\\\"];BANDITO_TESTS_API=opts[\\\"TESTS_API\\\"];BANDITO_EVENTS_API=opts[\\\"EVENTS_API\\\"]};window.pageBuilder.showVariants=function(){if(BANDITO_IS_ADMIN)return;var tests=[];for(var id in window._variantFeatureIds)if(window._variantFeatureIds.hasOwnProperty(id)){var testId=window._variantFeatureIds[id];testIdToFeatureId[testId]=id;tests.push(testId)}if(tests.length&gt;\\n0){var url=BANDITO_TESTS_API+\\\"?ids\\\\x3d\\\"+tests.join(\\\",\\\");ajax(url,loadVariants,showDefaultVariants)}}})();&lt;/script&gt; &lt;script&gt;window.banditoEnv={TIMEOUT:1500,PERF_API:\\\"https://bandito-events.perso.aws.arc.pub/api/save-perf\\\",TESTS_API:\\\"https://bandito.perso.aws.arc.pub/api/variants\\\",EVENTS_API:\\\"https://bandito-events.perso.aws.arc.pub/api/save-event\\\"};if(window[\\\"pageBuilder\\\"]&&window[\\\"pageBuilder\\\"][\\\"initVariants\\\"])window.pageBuilder.initVariants({IS_ADMIN:false,TIMEOUT:window.banditoEnv.TIMEOUT,PERF_API:window.banditoEnv.PERF_API,TESTS_API:window.banditoEnv.TESTS_API,EVENTS_API:window.banditoEnv.EVENTS_API});&lt;/script&gt; &lt;script async src=\\\"/pb/resources/dist/199071bbaf3b531cbc00/rm/rm-index199071bbaf3b531cbc00.js\\\"&gt;&lt;/script&gt;\"\n\n\n\nWe’ll try to extract the words from the page."
  },
  {
    "objectID": "pstat197a/slides/week5-text.html#scraping-tools",
    "href": "pstat197a/slides/week5-text.html#scraping-tools",
    "title": "Text to data: basic NLP",
    "section": "Scraping tools",
    "text": "Scraping tools\nFirst step: strip HTML and extract text (see rvest [docs]).\n\nlibrary(rvest)\n\n# parse\npage_text &lt;- read_html(page) %&gt;%\n  # extract paragraph elements\n  html_elements('p') %&gt;%\n  # strip html and extract text\n  html_text2()\n\n# print result\npage_text\n\n [1] \"BLYTHEVILLE, AR (KAIT) - One man is in custody after a 4-hour police standoff in Blytheville Sunday afternoon.\"                                                                 \n [2] \"According to Captain Scott Adams with the Blytheville Police Department, Hartzell Watson, 44, was arrested after being forced out of a home in the 500-block of North Division.\"\n [3] \"Captain Adams said police originally responded to the house after a family dispute.\"                                                                                            \n [4] \"When officers arrived, Watson barricaded himself in the house and refused to exit peacefully.\"                                                                                  \n [5] \"After several hours, police fired smoke and a chemical agent through a window of the home. The gas caught on fire within minutes.\"                                              \n [6] \"Officers entered the burning house to find Watson, but he jumped out of an attic window.\"                                                                                       \n [7] \"He was checked by paramedics on the scene and was taken into custody.\"                                                                                                          \n [8] \"The Blytheville Fire Department was on scene and quickly put the fire out.\"                                                                                                     \n [9] \"Watson was then taken to the Mississippi County Jail. He is being held on state and federal warrants.\"                                                                          \n[10] \"The Arkansas State Police, Mississippi County Sheriff's Office, and the Second Judicial Drug Task Force assisted with the standoff.\"                                            \n[11] \"One officer on the scene was treated for dehydration.\"                                                                                                                          \n[12] \"Copyright 2016 KAIT. All rights reserved.\"                                                                                                                                      \n[13] \"Watch Region 8 News On Demand: On your Desktop | On your Mobile device\"                                                                                                         \n[14] \"Region 8 News App - Install or update on your: iPhone | Android\"                                                                                                                \n[15] \"472 CR 766\\nJonesboro, AR 72401\\n(870) 931-8888\"                                                                                                                                \n[16] \"A Gray Media Group, Inc. Station - © 2002-2021 Gray Television, Inc.\""
  },
  {
    "objectID": "pstat197a/slides/week5-text.html#one-long-string",
    "href": "pstat197a/slides/week5-text.html#one-long-string",
    "title": "Text to data: basic NLP",
    "section": "One long string",
    "text": "One long string\nWe can collapse the list into one long character string containing all the paragraph text.\n\nstring &lt;- page_text %&gt;% str_c(collapse = ' ')\n\nstring\n\n[1] \"BLYTHEVILLE, AR (KAIT) - One man is in custody after a 4-hour police standoff in Blytheville Sunday afternoon. According to Captain Scott Adams with the Blytheville Police Department, Hartzell Watson, 44, was arrested after being forced out of a home in the 500-block of North Division. Captain Adams said police originally responded to the house after a family dispute. When officers arrived, Watson barricaded himself in the house and refused to exit peacefully. After several hours, police fired smoke and a chemical agent through a window of the home. The gas caught on fire within minutes. Officers entered the burning house to find Watson, but he jumped out of an attic window. He was checked by paramedics on the scene and was taken into custody. The Blytheville Fire Department was on scene and quickly put the fire out. Watson was then taken to the Mississippi County Jail. He is being held on state and federal warrants. The Arkansas State Police, Mississippi County Sheriff's Office, and the Second Judicial Drug Task Force assisted with the standoff. One officer on the scene was treated for dehydration. Copyright 2016 KAIT. All rights reserved. Watch Region 8 News On Demand: On your Desktop | On your Mobile device Region 8 News App - Install or update on your: iPhone | Android 472 CR 766\\nJonesboro, AR 72401\\n(870) 931-8888 A Gray Media Group, Inc. Station - © 2002-2021 Gray Television, Inc.\""
  },
  {
    "objectID": "pstat197a/slides/week5-text.html#words-neq-strings",
    "href": "pstat197a/slides/week5-text.html#words-neq-strings",
    "title": "Text to data: basic NLP",
    "section": "Words \\(\\neq\\) strings",
    "text": "Words \\(\\neq\\) strings\nnathan , Nathan, and Nathan! are identical words but distinct strings.\n\n'nathan' == 'Nathan'\n\n[1] FALSE\n\n\n\nBut text analysis requires that strings ⟺ words.\n\n\nQuestion. What are the steps to get from [1] to [2] ?\n\n\n[1] \"For more information, call @Alfred | (201) 744 5050\"\n[2] \"for more information call alfred\""
  },
  {
    "objectID": "pstat197a/slides/week5-text.html#string-manipulation",
    "href": "pstat197a/slides/week5-text.html#string-manipulation",
    "title": "Text to data: basic NLP",
    "section": "String manipulation",
    "text": "String manipulation\nSee stringr [docs] for string manipulation via pattern matching.\n\nlibrary(stringr)\n\nc('example-string') %&gt;% str_replace('[[:punct:]]', ' ')\n\n[1] \"example string\"\n\n\n\nSee qdapRegex [docs] for shorthand wrappers tor removing common but complex patterns.\n\nlibrary(qdapRegex)\n\nc('email Mildred mildred@mildred.info') %&gt;% rm_email() \n\n[1] \"email Mildred\""
  },
  {
    "objectID": "pstat197a/slides/week5-text.html#page-text-processing",
    "href": "pstat197a/slides/week5-text.html#page-text-processing",
    "title": "Text to data: basic NLP",
    "section": "Page text processing",
    "text": "Page text processing\nOur strategy will be:\n\nRemove URLs and email addresses\nRemove non-letters:\n\nline breaks \\n and &nbsp\npunctuation, numbers, and special characters\n\nAdd spaces before capital letters then remove extra whitespace\nReplace all capital letters with lower case letters"
  },
  {
    "objectID": "pstat197a/slides/week5-text.html#example",
    "href": "pstat197a/slides/week5-text.html#example",
    "title": "Text to data: basic NLP",
    "section": "Example",
    "text": "Example\nHere’s what that looks like for one page.\n\nremove &lt;- c('\\n', \n            '[[:punct:]]', \n            'nbsp', \n            '[[:digit:]]', \n            '[[:symbol:]]') %&gt;%\n  paste(collapse = '|')\n\nstring %&gt;%\n  rm_url() %&gt;%\n  rm_email() %&gt;%\n  str_remove_all('\\'') %&gt;%\n  str_replace_all(remove, ' ') %&gt;%\n  str_replace_all(\"([a-z])([A-Z])\", \"\\\\1 \\\\2\") %&gt;%\n  tolower() %&gt;%\n  str_replace_all(\"\\\\s+\", \" \")\n\n[1] \"blytheville ar kait one man is in custody after a hour police standoff in blytheville sunday afternoon according to captain scott adams with the blytheville police department hartzell watson was arrested after being forced out of a home in the block of north division captain adams said police originally responded to the house after a family dispute when officers arrived watson barricaded himself in the house and refused to exit peacefully after several hours police fired smoke and a chemical agent through a window of the home the gas caught on fire within minutes officers entered the burning house to find watson but he jumped out of an attic window he was checked by paramedics on the scene and was taken into custody the blytheville fire department was on scene and quickly put the fire out watson was then taken to the mississippi county jail he is being held on state and federal warrants the arkansas state police mississippi county sheriffs office and the second judicial drug task force assisted with the standoff one officer on the scene was treated for dehydration copyright kait all rights reserved watch region news on demand on your desktop on your mobile device region news app install or update on your i phone android cr jonesboro ar a gray media group inc station gray television inc \""
  },
  {
    "objectID": "pstat197a/slides/week5-text.html#output-quality",
    "href": "pstat197a/slides/week5-text.html#output-quality",
    "title": "Text to data: basic NLP",
    "section": "Output quality",
    "text": "Output quality\nComments:\n\nconsistent input format (i.e. sampling and collection) is really important for consistent scraping and text processing\n\ntricky with HTML because webpages may use different elements to display content\n\nthorough quality tests are recommended: inspect random subsamples for errors in processing"
  },
  {
    "objectID": "pstat197a/slides/week5-text.html#quality-checks",
    "href": "pstat197a/slides/week5-text.html#quality-checks",
    "title": "Text to data: basic NLP",
    "section": "Quality checks",
    "text": "Quality checks\n\n\n[[1]]\n[1] \"a melrose woman has been sentenced to seven years in prison for exploiting an elderly neighbor while duping her own live in girlfriend a melrose woman has been sentenced to seven years in prison for exploiting an elderly neighbor while duping her own live in girlfriend rhonda jo lay pleaded no contest late friday to exploitation of the elderly in connection with thefts from a year old woman circuit court judge james p nilon sentenced lay to seven years in prison to be followed by eight years of probation nilon also ordered lay to pay the victim in restitution the case against lay began as a case against her roommate lisa a feindt had been charged in the case but assistant state attorney deborah hunt said all charges against feindt were dropped dec when it became clear that lay had been lying to feindt when the women were arrested feindt was a physician s assistant at the university of florida feindt s attorney gilbert schaffnit said feindt had to resign from her job while waiting for the charges against her to be resolved my client was another victim schaffnit said she had to resign or be terminated over the criminal charges and getting her job back looks unlikely the case began when the elderly neighbor s daughter came for an extended visit and discovered that her mother was not getting any mail investigators said the daughter discovered that lay had arranged for the mail to be held at the post office presumably so that the daughter would not see any of the bank statements or other financial documents once the daughter collected the mail and financial information she called the alachua county sheriff s office to report money was missing from the woman s bank account sgt tom wetherington determined that more than two dozen checks worth more than had been fraudulently written on the woman s account wetherington noted that most of the money was taken from the account in checks made out to lay or feindt the rest of the money was taken from the account by writing checks to others who apparently were unaware they may have been written fraudulently during the investigation which ultimately cleared feindt investigators discovered that lay had been convicted of similar crimes in south carolina and michigan lay was being held at the alachua county jail on monday awaiting transfer to the florida department of corrections gannett co inc all rights reserved original content available for non commercial use under a creative commons license except where noted gainesville sun sw th st gainesville fl do not sell my personal information cookie policy do not sell my personal information privacy policy terms of service your california privacy rights privacy policy gannett usa today network choose the plan that s right for you digital access or digital and print delivery \"\n\n[[2]]\n[1] \"neighbors please be mindful of social distancing guidelines while you do your part to slow the spread of the new coronavirus see the latest guidance from the cdc here this post was contributed by a community member the views expressed here are the authors own small joys art exhibit at artworks gallery on the green st paul s on the green in norwalk will host a retrospective art exhibit small joys thursday december through saturday december in the artworks gallery in the historic chittim howell house next to the church at east avenue in norwalk the exhibit features small original artworks including mixed media painting and photography from regional artists whose work has previously appeared at the gallery they include merion frolich marcy juran elisa keogh kendall klingbeil tom kretsch scott kuykendall susan leggitt kerwin kipp mayers wendy moore gregg welz and nancy woodward the public is invited to the opening reception which includes light refreshments on thursday december from p m visitors can also shop for unique holiday gifts decorations and collectibles at the st paul s holiday market on the green boutique in the chittim howell house for more information please visit the st paul s website at thursday pm bedford katonah ny friday am new canaan ct friday pm new york city ny friday pm weston redding easton ct saturday am norwalk ct saturday am new canaan ct saturday am paramus nj\"\n\n[[3]]\n[1] \"order flowers for the family show your sympathy to the family november september robert irwin bob worzalla died peacefully at life path hospice care on sep in tampa florida at the age of he was born on nov in stevens point to the late irwin and hildegard worzalla and he was married to dorothy kiefer in at sacred heart catholic church in nekoosa presided by his uncle father dennis worzalla bob started a career in television repair in wisconsin rapids at miller tv over the next years the couple welcomed two children into their home robert michael worzalla and david john worzalla and set about teaching them life s lesson his children would remember him as a driven father who encouraged them to pursue their goals in he started a potato farm golden boy farms followed by a career with n e isaacson associates a lake developer in various positions with the company in he started a seafood distributorship central wisconsin seafoods with his youngest son david in he opened a health food store in brandon florida and ran this business until he retired in he was a generous witty and dedicated individual who loved the health care field and who was passionate about helping people improve their health by using natural herbal remedies bob is survived by son david j karen worzalla of pawleys island south carolina sisters patricia david olson of stevens point yvonne david kopperud of stevens point and bonnie bill wright of madison grandchildren eric worzalla of misawa japan curtis worzalla of colorado springs colorado sophie worzalla of camp douglas and nash worzalla of pawleys island south carolina and great granddaughter laney worzalla of paden city virginia he is preceded in death by son robert m tammy worzalla of camp douglas and brother john whitey worzalla of stevens point no memorial service will be held we ask instead that those of you who knew him to take a moment to remember him and celebrate his life for online condolences to the family please visit to send a flower arrangement or to plant trees in memory of robert irwin worzalla please click here to visit our sympathy store copyright pisarski funeral homes all rights reserved funeral home website by batesville inc funeral planning and grief resources terms of use privacy\""
  },
  {
    "objectID": "pstat197a/slides/week5-text.html#processed-data",
    "href": "pstat197a/slides/week5-text.html#processed-data",
    "title": "Text to data: basic NLP",
    "section": "Processed data",
    "text": "Processed data\n\nclean &lt;- rawdata_relabeled %&gt;%\n  filter(str_detect(text_tmp, '&lt;!')) %&gt;%\n  rowwise() %&gt;%\n  mutate(text_clean = parse_fn(text_tmp)) %&gt;%\n  select(-text_tmp) %&gt;%\n  unnest(text_clean)\n\nclean %&gt;% head()\n\n# A tibble: 6 × 3\n  .id   bclass     text_clean                                                   \n  &lt;chr&gt; &lt;fct&gt;      &lt;chr&gt;                                                        \n1 url1  relevant   \"blytheville ar kait one man is in custody after a hour poli…\n2 url2  irrelevant \"\"                                                           \n3 url3  irrelevant \"email password dont have an account sign up now forgot your…\n4 url4  relevant   \"username or email address password remember me presque isle…\n5 url5  relevant   \" this may take a moment this may take a moment \"            \n6 url6  relevant   \"\""
  },
  {
    "objectID": "pstat197a/slides/week5-text.html#about-nlp",
    "href": "pstat197a/slides/week5-text.html#about-nlp",
    "title": "Text to data: basic NLP",
    "section": "About NLP",
    "text": "About NLP\nNatural language processing(NLP) refers to techniques for processing and analyzing speech and text. Although a specialized subfield, it comprises a broad range of problems and methods, including:\n\ntext and speech processing and representation\nautomated summarization\nspeech recognition\nmachine translation\nsentiment analysis"
  },
  {
    "objectID": "pstat197a/slides/week5-text.html#text-processing-techniques",
    "href": "pstat197a/slides/week5-text.html#text-processing-techniques",
    "title": "Text to data: basic NLP",
    "section": "Text processing techniques",
    "text": "Text processing techniques\nWe will focus here on NLP techniques for processing text, i.e., converting text into data.\n\ntokenization: breaking a string of text into smaller units\nlemmatization: converting tokens into common forms\ncalculating frequency measures"
  },
  {
    "objectID": "pstat197a/slides/week5-text.html#tokenization",
    "href": "pstat197a/slides/week5-text.html#tokenization",
    "title": "Text to data: basic NLP",
    "section": "Tokenization",
    "text": "Tokenization\nBreaking a string of text into subunits is called tokenization.\n\nConsider this string:\n\n\n[1] \"if you are a dreamer come in if you are a dreamer a wisher a liar a hope er a pray er a magic bean buyer if youre a pretender come sit by my fire for we have some flax golden tales to spin come in come in\""
  },
  {
    "objectID": "pstat197a/slides/week5-text.html#word-tokenization",
    "href": "pstat197a/slides/week5-text.html#word-tokenization",
    "title": "Text to data: basic NLP",
    "section": "Word tokenization",
    "text": "Word tokenization\nThe tokenizers package [docs] contains various tokenization functions. The most elementary method is to treat each word as a token.\n\nlibrary(tokenizers)\n\ninvitation_text %&gt;% tokenize_words()\n\n[[1]]\n [1] \"if\"        \"you\"       \"are\"       \"a\"         \"dreamer\"   \"come\"     \n [7] \"in\"        \"if\"        \"you\"       \"are\"       \"a\"         \"dreamer\"  \n[13] \"a\"         \"wisher\"    \"a\"         \"liar\"      \"a\"         \"hope\"     \n[19] \"er\"        \"a\"         \"pray\"      \"er\"        \"a\"         \"magic\"    \n[25] \"bean\"      \"buyer\"     \"if\"        \"youre\"     \"a\"         \"pretender\"\n[31] \"come\"      \"sit\"       \"by\"        \"my\"        \"fire\"      \"for\"      \n[37] \"we\"        \"have\"      \"some\"      \"flax\"      \"golden\"    \"tales\"    \n[43] \"to\"        \"spin\"      \"come\"      \"in\"        \"come\"      \"in\""
  },
  {
    "objectID": "pstat197a/slides/week5-text.html#n-gram-tokens",
    "href": "pstat197a/slides/week5-text.html#n-gram-tokens",
    "title": "Text to data: basic NLP",
    "section": "N-gram tokens",
    "text": "N-gram tokens\nAlternatively, one could tokenize by n-grams: unique combinations of \\(n\\) adjacent words.\n\ninvitation_text %&gt;% tokenize_ngrams(n = 2)\n\n[[1]]\n [1] \"if you\"         \"you are\"        \"are a\"          \"a dreamer\"     \n [5] \"dreamer come\"   \"come in\"        \"in if\"          \"if you\"        \n [9] \"you are\"        \"are a\"          \"a dreamer\"      \"dreamer a\"     \n[13] \"a wisher\"       \"wisher a\"       \"a liar\"         \"liar a\"        \n[17] \"a hope\"         \"hope er\"        \"er a\"           \"a pray\"        \n[21] \"pray er\"        \"er a\"           \"a magic\"        \"magic bean\"    \n[25] \"bean buyer\"     \"buyer if\"       \"if youre\"       \"youre a\"       \n[29] \"a pretender\"    \"pretender come\" \"come sit\"       \"sit by\"        \n[33] \"by my\"          \"my fire\"        \"fire for\"       \"for we\"        \n[37] \"we have\"        \"have some\"      \"some flax\"      \"flax golden\"   \n[41] \"golden tales\"   \"tales to\"       \"to spin\"        \"spin come\"     \n[45] \"come in\"        \"in come\"        \"come in\""
  },
  {
    "objectID": "pstat197a/slides/week5-text.html#stopwords",
    "href": "pstat197a/slides/week5-text.html#stopwords",
    "title": "Text to data: basic NLP",
    "section": "Stopwords",
    "text": "Stopwords\nSome tokens are thought to contain little semantic information, such as logical connectives, pronouns, and the like.\n\nIn NLP these are treated as stopwords: words that are stopped in text processing.\n\nlibrary(stopwords)\n\n# display 10 random stopwords from the 'snowball' dictionary\nset.seed(102422)\nstopwords(language = 'en', source = 'snowball') %&gt;% \n  sample(size = 10)\n\n [1] \"but\"      \"off\"      \"didn't\"   \"nor\"      \"yourself\" \"they\"    \n [7] \"ought\"    \"when's\"   \"wouldn't\" \"they'll\""
  },
  {
    "objectID": "pstat197a/slides/week5-text.html#stopword-removal",
    "href": "pstat197a/slides/week5-text.html#stopword-removal",
    "title": "Text to data: basic NLP",
    "section": "Stopword removal",
    "text": "Stopword removal\nIn the default stopword list, some stopwords include punctuation. Since this was removed from our string, it should also be removed from the stopword list for effective string matching.\n\nstopwords_nopunct &lt;- stopwords() %&gt;% \n  str_remove_all('[[:punct:]]')\n\ninvitation_text %&gt;%\n  tokenize_words(stopwords = stopwords_nopunct)\n\n[[1]]\n [1] \"dreamer\"   \"come\"      \"dreamer\"   \"wisher\"    \"liar\"      \"hope\"     \n [7] \"er\"        \"pray\"      \"er\"        \"magic\"     \"bean\"      \"buyer\"    \n[13] \"pretender\" \"come\"      \"sit\"       \"fire\"      \"flax\"      \"golden\"   \n[19] \"tales\"     \"spin\"      \"come\"      \"come\""
  },
  {
    "objectID": "pstat197a/slides/week5-text.html#lemmatization",
    "href": "pstat197a/slides/week5-text.html#lemmatization",
    "title": "Text to data: basic NLP",
    "section": "Lemmatization",
    "text": "Lemmatization\nLemmatization refers to grouping word inflections into a single form. See textstem [docs].\n\nlibrary(textstem)\n\nlocomotion &lt;- c('run', 'running', 'ran', \n                'boating', 'boat', \n                'swim', 'swam', 'swimming', 'swum') \n\nlocomotion %&gt;% lemmatize_words()\n\n[1] \"run\"  \"run\"  \"run\"  \"boat\" \"boat\" \"swim\" \"swim\" \"swim\" \"swim\"\n\n\n\nAlso works by pattern matching and replacement using a source dictionary."
  },
  {
    "objectID": "pstat197a/slides/week5-text.html#tidytextunnest_tokens",
    "href": "pstat197a/slides/week5-text.html#tidytextunnest_tokens",
    "title": "Text to data: basic NLP",
    "section": "tidytext::unnest_tokens()",
    "text": "tidytext::unnest_tokens()\nThe tidytext package [docs] contains wrappers around tokenizers and other functions for use in tidyverse-style programming.\n\n\n# A tibble: 36 × 3\n   doc        token   token.lem\n   &lt;chr&gt;      &lt;chr&gt;   &lt;chr&gt;    \n 1 invitation dreamer dreamer  \n 2 invitation come    come     \n 3 invitation dreamer dreamer  \n 4 invitation wisher  wisher   \n 5 invitation liar    liar     \n 6 invitation hope    hope     \n 7 invitation er      er       \n 8 invitation pray    pray     \n 9 invitation er      er       \n10 invitation magic   magic    \n# … with 26 more rows"
  },
  {
    "objectID": "pstat197a/slides/week5-text.html#quality-check",
    "href": "pstat197a/slides/week5-text.html#quality-check",
    "title": "Text to data: basic NLP",
    "section": "Quality check",
    "text": "Quality check\nText processing is error-prone. The functions shown here are handy but imperfect. You should always perform quality checks to identify bugs!\n\nSee any problems?\n\n\n [1] \"dreamer\"   \"come\"      \"wisher\"    \"liar\"      \"hope\"      \"er\"       \n [7] \"pray\"      \"magic\"     \"bean\"      \"buyer\"     \"pretender\" \"sit\"      \n[13] \"fire\"      \"flax\"      \"golden\"    \"tale\"      \"spin\"      \"boat\"     \n[19] \"just\"      \"build\"     \"fine\"      \"try\"       \"tell\"      \"us\"       \n[25] \"side\"      \"back\"      \"divine\"    \"bottom\"    \"guess\"     \"forget\""
  },
  {
    "objectID": "pstat197a/slides/week5-text.html#corpora",
    "href": "pstat197a/slides/week5-text.html#corpora",
    "title": "Text to data: basic NLP",
    "section": "Corpora",
    "text": "Corpora\nWe can summarize a text corpus (collection of texts) as:\n\n\\(T = \\{t_1, \\dots, t_p\\}\\) set of \\(p\\) unique tokens\n\\(D = \\{d_1, \\dots, d_n\\}\\) set of \\(n\\) documents comprising some corpus\n\n\\(d_i = \\{t_{i1}, \\dots, t_{iL_i}: t_{ij} \\in T\\}\\) is the \\(i\\)th document"
  },
  {
    "objectID": "pstat197a/slides/week5-text.html#frequency-measures",
    "href": "pstat197a/slides/week5-text.html#frequency-measures",
    "title": "Text to data: basic NLP",
    "section": "Frequency measures",
    "text": "Frequency measures\nThen we can define the following:\n\ndocumentwise token counts \\(n_{ti} = \\sum_j \\mathbf{1}\\{t_{ij} = t\\}\\)\n\nnumber of times token \\(t\\) appears in document \\(i\\)\n\ncorpuswise token counts \\(n_t = \\sum_i \\mathbf{1}\\{t \\in d_i\\}\\)\n\nnumber of documents containing token \\(t\\)\n\nterm frequency: \\(\\text{tf}(t, i) = n_{ti}/n_i\\)\ndocument frequency: \\(\\text{df}(t, i) = n_t/n\\)\ninverse document frequency: \\(-\\log\\left(\\text{df}(t, i)\\right)\\)"
  },
  {
    "objectID": "pstat197a/slides/week5-text.html#computing-tf-idf",
    "href": "pstat197a/slides/week5-text.html#computing-tf-idf",
    "title": "Text to data: basic NLP",
    "section": "Computing TF-IDF",
    "text": "Computing TF-IDF\nContinuing with our toy example:\n\n\n# A tibble: 4 × 6\n  doc        token.lem  n_ti     tf   idf tf_idf\n  &lt;chr&gt;      &lt;chr&gt;     &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 invitation come          4 0.182  0.693 0.126 \n2 boat       just          2 0.143  0.693 0.0990\n3 invitation dreamer       2 0.0909 0.693 0.0630\n4 invitation er            2 0.0909 0.693 0.0630\n\n\n\nQuestion: what does it mean that IDF is the same for all terms?"
  },
  {
    "objectID": "pstat197a/slides/week5-text.html#tf-idf",
    "href": "pstat197a/slides/week5-text.html#tf-idf",
    "title": "Text to data: basic NLP",
    "section": "TF-IDF",
    "text": "TF-IDF\nThe last column was the product of term frequency and inverse document frequency, known as TF-IDF:\n\\[\n\\text{tf-idf}(t, i) = \\text{tf}(t, i) \\times \\text{idf}(t, i)\n\\]\n\nInterpretation:\n\nhigher values indicate rare words used often in a document\nlower values indicate common words used infrequently in a document"
  },
  {
    "objectID": "pstat197a/slides/week5-text.html#document-term-matrix",
    "href": "pstat197a/slides/week5-text.html#document-term-matrix",
    "title": "Text to data: basic NLP",
    "section": "Document term matrix",
    "text": "Document term matrix\nFinally, we can pivot a selected frequency measure into a data frame in which:\n\neach row is a document\neach column is a token\neach value is a frequency measure\n\n\n\n\n# A tibble: 2 × 31\n  doc        back   boat bottom  build divine   fine forget  guess   just   side\n  &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 boat     0.0495 0.0495 0.0495 0.0495 0.0495 0.0495 0.0495 0.0495 0.0990 0.0495\n2 invitat… 0      0      0      0      0      0      0      0      0      0     \n# … with 20 more variables: tell &lt;dbl&gt;, try &lt;dbl&gt;, us &lt;dbl&gt;, bean &lt;dbl&gt;,\n#   buyer &lt;dbl&gt;, come &lt;dbl&gt;, dreamer &lt;dbl&gt;, er &lt;dbl&gt;, fire &lt;dbl&gt;, flax &lt;dbl&gt;,\n#   golden &lt;dbl&gt;, hope &lt;dbl&gt;, liar &lt;dbl&gt;, magic &lt;dbl&gt;, pray &lt;dbl&gt;,\n#   pretender &lt;dbl&gt;, sit &lt;dbl&gt;, spin &lt;dbl&gt;, tale &lt;dbl&gt;, wisher &lt;dbl&gt;\n\n\n\n\nQuestion: how would you check whether the ‘documents’ have words in common?"
  },
  {
    "objectID": "pstat197a/slides/week5-text.html#processed-fraud-claim-data",
    "href": "pstat197a/slides/week5-text.html#processed-fraud-claim-data",
    "title": "Text to data: basic NLP",
    "section": "Processed fraud claim data",
    "text": "Processed fraud claim data\nTF-IDF document term matrix for word tokens:\n\n\n# A tibble: 3 × 15,870\n  .id    bclass     adams afternoon  agent android    app arkansas arrest arrive\n  &lt;chr&gt;  &lt;fct&gt;      &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 url1   relevant  0.0692    0.0300 0.0365  0.0450 0.0330   0.0390 0.0140 0.0305\n2 url10  irreleva… 0         0      0       0      0        0      0      0     \n3 url100 irreleva… 0         0      0       0      0        0      0      0     \n# … with 15,860 more variables: assist &lt;dbl&gt;, attic &lt;dbl&gt;, barricade &lt;dbl&gt;,\n#   block &lt;dbl&gt;, blytheville &lt;dbl&gt;, burn &lt;dbl&gt;, captain &lt;dbl&gt;, catch &lt;dbl&gt;,\n#   check &lt;dbl&gt;, chemical &lt;dbl&gt;, copyright &lt;dbl&gt;, county &lt;dbl&gt;, custody &lt;dbl&gt;,\n#   dehydration &lt;dbl&gt;, demand &lt;dbl&gt;, department &lt;dbl&gt;, desktop &lt;dbl&gt;,\n#   device &lt;dbl&gt;, dispute &lt;dbl&gt;, division &lt;dbl&gt;, drug &lt;dbl&gt;, enter &lt;dbl&gt;,\n#   exit &lt;dbl&gt;, family &lt;dbl&gt;, federal &lt;dbl&gt;, fire &lt;dbl&gt;, force &lt;dbl&gt;,\n#   gas &lt;dbl&gt;, gray &lt;dbl&gt;, hartzell &lt;dbl&gt;, hold &lt;dbl&gt;, home &lt;dbl&gt;, …\n\n\n\n\\(n = 552\\) rows/observations (one per page)\ncolumns comprising \\(p = 15,742\\) variables and \\(1\\) class label"
  },
  {
    "objectID": "pstat197a/slides/week5-text.html#next-time",
    "href": "pstat197a/slides/week5-text.html#next-time",
    "title": "Text to data: basic NLP",
    "section": "Next time",
    "text": "Next time\nNext time we’ll discuss the capstone group’s analysis strategy:\n\ndimension reduction\nstatistical modeling"
  },
  {
    "objectID": "pstat197a/slides/week4-logistic.html#announcementsreminders",
    "href": "pstat197a/slides/week4-logistic.html#announcementsreminders",
    "title": "Classification with logistic regression",
    "section": "Announcements/reminders",
    "text": "Announcements/reminders\n\nNext assignment and groups posted on class website\n\nDue Friday, October 28, 11:59pm\n\nAny feedback on the first group assignment?"
  },
  {
    "objectID": "pstat197a/slides/week4-logistic.html#from-last-time",
    "href": "pstat197a/slides/week4-logistic.html#from-last-time",
    "title": "Classification with logistic regression",
    "section": "From last time",
    "text": "From last time\nIn groups you made \\(T = 7\\) trees ‘by hand’. To make each tree:\n\nrandomly resample observations and choose two predictors at random\nchoose a variable and a split by manual inspection, then repeat\n\nthis method of tree construction is called recursive partitioning\n\n\n\nThen each of you classified a new observation. We took a majority vote to decide on the final classification.\n\n\nThis is a random forest consisting of \\(T = 7\\) trees."
  },
  {
    "objectID": "pstat197a/slides/week4-logistic.html#random-forests",
    "href": "pstat197a/slides/week4-logistic.html#random-forests",
    "title": "Classification with logistic regression",
    "section": "Random forests",
    "text": "Random forests\nTo implement random forests algorithmically, one can control:\n\nnumber of trees \\(T\\)\nnumber of predictors \\(m\\) to choose at random for each tree\nbootstrap sample size and method\ntree depth as specified by…\n\nminimum number of observations per split (‘node size’)\nmaximum number of terminal nodes"
  },
  {
    "objectID": "pstat197a/slides/week4-logistic.html#variable-importance",
    "href": "pstat197a/slides/week4-logistic.html#variable-importance",
    "title": "Classification with logistic regression",
    "section": "Variable importance",
    "text": "Variable importance\nSuppose we had a random forest of three trees.\n\n\n\n\n\n\n\n\n\nG\n\n \n\ncluster3\n\n tree #3  \n\ncluster2\n\n tree #2  \n\ncluster1\n\n tree #1   \n\nage\n\n age   \n\nedu\n\n edu   \n\nage-&gt;edu\n\n   \n\nl2\n\n &gt;50k   \n\nage-&gt;l2\n\n   \n\nl1\n\n &gt;50k   \n\nedu-&gt;l1\n\n   \n\nl3\n\n &lt;50k   \n\nedu-&gt;l3\n\n   \n\nedu2\n\n edu   \n\nl4\n\n &gt;50k   \n\nedu2-&gt;l4\n\n   \n\nl5\n\n &lt;50k   \n\nedu2-&gt;l5\n\n   \n\ngain\n\n gain   \n\nage2\n\n age   \n\ngain-&gt;age2\n\n   \n\nl7\n\n &lt;50k   \n\ngain-&gt;l7\n\n   \n\nsex\n\n sex   \n\nsex-&gt;gain\n\n   \n\nl6\n\n &gt;50k   \n\nsex-&gt;l6\n\n   \n\nl8\n\n &lt;50k   \n\nage2-&gt;l8\n\n   \n\nl9\n\n &gt;50k   \n\nage2-&gt;l9\n\n  \n\n\n\n\n\n\n\nWhich variables seem most important?\nHow can you tell?"
  },
  {
    "objectID": "pstat197a/slides/week4-logistic.html#how-to-measure-importance",
    "href": "pstat197a/slides/week4-logistic.html#how-to-measure-importance",
    "title": "Classification with logistic regression",
    "section": "How to measure importance?",
    "text": "How to measure importance?\nA natural thought is to measure importance by the use frequency of each variable.\n\nBut use frequency doesn’t capture the quality of splits. Imagine:\n\nsplitting often on education but with little improvement in classifications\nand splitting infrequently on captial gain but with dramatic improvement\ncapital gain is probably more important for classification even though it’s used less"
  },
  {
    "objectID": "pstat197a/slides/week4-logistic.html#quality-of-splits",
    "href": "pstat197a/slides/week4-logistic.html#quality-of-splits",
    "title": "Classification with logistic regression",
    "section": "Quality of splits",
    "text": "Quality of splits\nWhen you were building trees, you had to choose which variable to split on.\n\nHow did you pick?\nDid you have a principle or goal in mind?\nWhat would make one split better than another?"
  },
  {
    "objectID": "pstat197a/slides/week4-logistic.html#measuring-quality-node-homogeneity",
    "href": "pstat197a/slides/week4-logistic.html#measuring-quality-node-homogeneity",
    "title": "Classification with logistic regression",
    "section": "Measuring quality: node homogeneity",
    "text": "Measuring quality: node homogeneity\nOne approach is to tree construction is to choose splits that optimize quantitative measures of node homogeneity. If \\(p_k\\) is the proportion of observations in class \\(k\\):\n\n(Gini index) \\(1 - \\sum_{k = 1}^K p_k^2\\)\n(entropy) \\(-\\sum_{k = 1}^K p_k \\log_2 p_k\\)\n\n\nSmaller values indicate greater homogeneity."
  },
  {
    "objectID": "pstat197a/slides/week4-logistic.html#variable-importance-scores",
    "href": "pstat197a/slides/week4-logistic.html#variable-importance-scores",
    "title": "Classification with logistic regression",
    "section": "Variable importance scores",
    "text": "Variable importance scores\nThe change in node homogeneity can be calculated for every split:\n\\[\nh(\\text{before}) - \\underbrace{\\Big[(p_L \\times h(\\text{after}_L) - p_R \\times h(\\text{after}_R)\\Big]}_{\\text{weighted avg. of homogeneity in child nodes}}\n\\]\n\nThe average change across all nodes associated with a given predictor in all trees gives an easy measure of importance.\n\nfavors high-quality splits over splitting frequency"
  },
  {
    "objectID": "pstat197a/slides/week4-logistic.html#proteomics-application",
    "href": "pstat197a/slides/week4-logistic.html#proteomics-application",
    "title": "Classification with logistic regression",
    "section": "Proteomics application",
    "text": "Proteomics application\nBack to the proteomics data, the variable importance scores from a random forest provide another means of identifying proteins.\n\nfit a random forest\ncompute importance scores\nrank predictors and choose top \\(n\\)"
  },
  {
    "objectID": "pstat197a/slides/week4-logistic.html#results",
    "href": "pstat197a/slides/week4-logistic.html#results",
    "title": "Classification with logistic regression",
    "section": "Results",
    "text": "Results\n\nFittingImportance scoresAccuracy\n\n\n\n# reproducibility\nset.seed(101422)\n\n# fit rf\nrf_out &lt;- randomForest(x = asd_preds, # predictors\n                       y = asd_resp, # response\n                       ntree = 1000, # number of trees\n                       importance = T) # compute importance\n\nBy default, randomForest():\n\nuses \\(\\sqrt{p}\\) predictors for each tree\ntrees grown until exact classification accuracy is achieved\nbootstrap sample size equal to number of observations\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRows show true classes, columns show predicted classes.\n\n\n\n\n\n\nASD\nTD\nclass.error\n\n\n\n\nASD\n48\n28\n0.3684211\n\n\nTD\n17\n61\n0.2179487"
  },
  {
    "objectID": "pstat197a/slides/week4-logistic.html#putting-things-together",
    "href": "pstat197a/slides/week4-logistic.html#putting-things-together",
    "title": "Classification with logistic regression",
    "section": "Putting things together",
    "text": "Putting things together\nLet \\(\\hat{S}_j\\) indicate the set of proteins selected by method \\(j\\) . Then the final estimate is\n\\[\n\\hat{S}^* = \\bigcap_j \\hat{S}_j\n\\]\n\nIn other words, those proteins that are selected by all three methods. Remarks:\n\nprobably fairly high selection variance\nprobably pretty conservative"
  },
  {
    "objectID": "pstat197a/slides/week4-logistic.html#core-panel",
    "href": "pstat197a/slides/week4-logistic.html#core-panel",
    "title": "Classification with logistic regression",
    "section": "“Core” panel",
    "text": "“Core” panel\n\nMultiple testingRandom forestIntersection\n\n\n\ntt_fn &lt;- function(.df){\n  infer::t_test(.df,\n         formula = level ~ group,\n         alternative = 'two-sided',\n         order = c('ASD', 'TD'),\n         var.equal = F)\n}\n\ns1 &lt;- read_csv('data/biomarker-clean.csv') %&gt;% \n  mutate(across(.cols = -c(group, ados), log10)) %&gt;%\n  mutate(across(.cols = -c(group, ados), ~ scale(.x)[, 1])) %&gt;%\n  mutate(across(.cols = -c(group, ados), trim_fn)) %&gt;%\n  select(-ados) %&gt;%\n  pivot_longer(-group,\n               names_to = \"protein\",\n               values_to = \"level\") %&gt;%\n  nest(data = c(group, level)) %&gt;%\n  mutate(test = map(data, tt_fn)) %&gt;%\n  unnest(test) %&gt;%\n  arrange(p_value) %&gt;%\n  mutate(m = n(),\n         hm = log(m) + 1/(2*m) - digamma(1),\n         rank = row_number(),\n         p.adj = m*hm*p_value/rank) %&gt;%\n  slice_min(p.adj, n = 10) %&gt;%\n  pull(protein)\n\n\n\n\n# reproducibility\nset.seed(101422)\n\n# fit rf\nrf_out &lt;- randomForest(x = asd_preds,\n                       y = asd_resp,                    \n                       ntree = 1000, \n                       importance = T) \n\n# select most important predictors\ns2 &lt;- rf_out$importance %&gt;% \n  as_tibble() %&gt;%\n  mutate(protein = rownames(rf_out$importance)) %&gt;%\n  slice_max(MeanDecreaseGini, n = 10) %&gt;%\n  pull(protein)\n\n\n\n\ns_star &lt;- intersect(s1, s2)\ns_star\n\n[1] \"DERM\"  \"RELT\"  \"IgD\"   \"FSTL1\""
  },
  {
    "objectID": "pstat197a/slides/week4-logistic.html#how-accurate-is-the-panel",
    "href": "pstat197a/slides/week4-logistic.html#how-accurate-is-the-panel",
    "title": "Classification with logistic regression",
    "section": "How accurate is the panel?",
    "text": "How accurate is the panel?\nGoal: use a statistical model to evaluate classification accuracy using the ‘core’ panel of proteins \\(\\hat{S}^*\\).\n\nThe logistic regression model is the most widely-used statistical model for binary data."
  },
  {
    "objectID": "pstat197a/slides/week4-logistic.html#the-bernoulli-distribution",
    "href": "pstat197a/slides/week4-logistic.html#the-bernoulli-distribution",
    "title": "Classification with logistic regression",
    "section": "The Bernoulli distribution",
    "text": "The Bernoulli distribution\nThe Bernoulli distribution describes the probability of a binary outcome (think coin toss). Mathematically:\n\\[\nY \\sim \\text{bernoulli}(p)\n\\quad\\Longleftrightarrow\\quad\nP(Y = y) = p^y (1 - p)^{1 - y}\n\\quad\\text{for}\\quad\ny \\in \\{0, 1\\}\n\\]\n\nThis just says that \\(P(Y = 1) = p\\) and \\(P(Y = 0) = 1 - p\\).\n\n\nProperties:\n\n\\(\\mathbb{E}Y = p\\)\n\\(\\text{var}Y = p(1 - p)\\)"
  },
  {
    "objectID": "pstat197a/slides/week4-logistic.html#the-logistic-regression-model",
    "href": "pstat197a/slides/week4-logistic.html#the-logistic-regression-model",
    "title": "Classification with logistic regression",
    "section": "The logistic regression model",
    "text": "The logistic regression model\nThe logistic regression model for a response \\(Y\\in\\{0, 1\\}\\) and covariates \\(X\\in\\mathbb{R}^p\\) is:\n\\[\n\\begin{cases}\nY_i|X_i = x_i \\stackrel{iid}{\\sim} \\text{bernoulli}(p_i)\n\\quad\\text{for}\\quad i = 1, \\dots, n\\\\\n\\log\\left(\\frac{p_i}{1 - p_i}\\right) = \\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_p x_{ip}\n\\end{cases}\n\\]\n\nThis is a generalized linear model because \\(g\\left(\\mathbb{E}Y\\right) = X\\beta\\) and \\(Y\\sim EF\\).\n\n\nParameters are estimated by maximum likelihood."
  },
  {
    "objectID": "pstat197a/slides/week4-logistic.html#the-model-visually",
    "href": "pstat197a/slides/week4-logistic.html#the-model-visually",
    "title": "Classification with logistic regression",
    "section": "The model, visually",
    "text": "The model, visually\n\\[\n\\log\\left(\\frac{p_i}{1 - p_i}\\right) = x_i^T\\beta\n\\quad\\Longleftrightarrow\\quad\np_i = \\frac{1}{1 + e^{-x_i^T\\beta}}\n\\]\n\nPlotting the right hand side for one predictor with \\(\\beta^T = [0 \\; 1]\\):"
  },
  {
    "objectID": "pstat197a/slides/week4-logistic.html#assumptions",
    "href": "pstat197a/slides/week4-logistic.html#assumptions",
    "title": "Classification with logistic regression",
    "section": "Assumptions",
    "text": "Assumptions\n\nObservations are independent\nProbability of event is monotonic in each predictor\nMean-variance relationship following Bernoulli distribution"
  },
  {
    "objectID": "pstat197a/slides/week4-logistic.html#parameter-interpretation",
    "href": "pstat197a/slides/week4-logistic.html#parameter-interpretation",
    "title": "Classification with logistic regression",
    "section": "Parameter interpretation",
    "text": "Parameter interpretation\nAccording to the model, the log-odds are linear in the predictors:\n\\[\n\\log\\underbrace{\\left(\\frac{p_i}{1 - p_i}\\right)}_{\\text{odds}} = \\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_p x_{ip}\n\\]\n\nSo a unit increase in the \\(j\\)th predictor \\(x_{ij} \\rightarrow x_{ij} + 1\\) is associated with a change in log-odds of \\(\\beta_j\\).\n\n\nTherefore the same unit increase is associated with a change in the odds by a factor of \\(e^\\beta_j\\)."
  },
  {
    "objectID": "pstat197a/slides/week4-logistic.html#fitting-with-one-predictor",
    "href": "pstat197a/slides/week4-logistic.html#fitting-with-one-predictor",
    "title": "Classification with logistic regression",
    "section": "Fitting with one predictor",
    "text": "Fitting with one predictor\n\nMLEEstimatesVisualization\n\n\nMaximum likelihood: find the parameter values for which the joint probability of the data is greatest according to the model.\n\nWritten as an optimization problem in terms of the negative log-likelihood:\n\\[\n\\hat{\\beta} = \\arg\\min_\\beta \\left\\{ -\\ell(\\beta; x, y) \\right\\}\n\\]\nComputed by iteratively re-weighted least squares (IRLS).\n\n\n\n\nasd_sub &lt;- asd_clean %&gt;% \n  select(group, any_of(s_star)) %&gt;%\n  mutate(group = (group == 'ASD'))\n\nfit &lt;- glm(group ~ DERM, family = 'binomial', data = asd_sub)\n\nfit %&gt;% broom::tidy() %&gt;% knitr::kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-0.016712\n0.1804471\n-0.0926145\n0.9262099\n\n\nDERM\n-1.116996\n0.2239268\n-4.9882173\n0.0000006\n\n\n\n\n\n\n\n\n\n\n\n\nProportion of subjects in ASD group after binning by DERM level (points) with estimated probability (curve)."
  },
  {
    "objectID": "pstat197a/slides/week4-logistic.html#fitting-with-several-predictors",
    "href": "pstat197a/slides/week4-logistic.html#fitting-with-several-predictors",
    "title": "Classification with logistic regression",
    "section": "Fitting with several predictors",
    "text": "Fitting with several predictors\nThe fitting procedure is identical.\n\n\nfit &lt;- glm(group ~ ., \n           family = 'binomial', \n           data = asd_sub)\n\nfit %&gt;%\n  broom::tidy() %&gt;%\n  knitr::kable()\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-0.0872143\n0.1984416\n-0.439496\n0.6603022\n\n\nDERM\n-0.6909559\n0.2634663\n-2.622559\n0.0087272\n\n\nRELT\n-0.4564311\n0.2863507\n-1.593958\n0.1109453\n\n\nIgD\n-0.6603222\n0.2117471\n-3.118447\n0.0018181\n\n\nFSTL1\n-0.4278022\n0.2460913\n-1.738388\n0.0821425"
  },
  {
    "objectID": "pstat197a/slides/week4-logistic.html#measuring-accuracy",
    "href": "pstat197a/slides/week4-logistic.html#measuring-accuracy",
    "title": "Classification with logistic regression",
    "section": "Measuring accuracy",
    "text": "Measuring accuracy\nThere are two types of errors:\n\n\n\n\nPredicted 0\nPredicted 1\n\n\n\n\nClass 0\ntrue negative (TN)\nfalse positive (FP)\n\n\nClass 1\nfalse negative (FN)\ntrue positive (TP)\n\n\n\n\n\nasd_sub %&gt;%\n  modelr::add_predictions(fit, type = 'response') %&gt;%\n  mutate(pred_class = pred &gt; 0.5) %&gt;%\n  select(group, pred_class) %&gt;%\n  mutate_all(~factor(.x, labels = c('TD', 'ASD'))) %&gt;%\n  table()\n\n     pred_class\ngroup TD ASD\n  TD  58  20\n  ASD 18  58"
  },
  {
    "objectID": "pstat197a/slides/week4-logistic.html#accuracy-rates",
    "href": "pstat197a/slides/week4-logistic.html#accuracy-rates",
    "title": "Classification with logistic regression",
    "section": "Accuracy rates",
    "text": "Accuracy rates\nThe most basic accuracy rates are:\n\nSensitivity/recall: \\(\\frac{TP}{P}\\) , proportion of positives that are correctly classified\nSpecificity: \\(\\frac{TN}{N}\\) , proportion of negatives that are correctly classified\nAccuracy: proportion of observations that are correctly classified"
  },
  {
    "objectID": "pstat197a/slides/week4-logistic.html#your-turn",
    "href": "pstat197a/slides/week4-logistic.html#your-turn",
    "title": "Classification with logistic regression",
    "section": "Your turn",
    "text": "Your turn\nTry calculating sensitivity, specificity, and accuracy for the logistic regression using the core proteins selected.\n\n\n    TD ASD    \nTD  58  20  78\nASD 18  58  76\n    76  78 154"
  },
  {
    "objectID": "pstat197a/slides/week4-logistic.html#using-yardstickmetric_set",
    "href": "pstat197a/slides/week4-logistic.html#using-yardstickmetric_set",
    "title": "Classification with logistic regression",
    "section": "Using yardstick::metric_set()",
    "text": "Using yardstick::metric_set()\n\nlibrary(yardstick)\n\nclass_metrics &lt;- metric_set(sensitivity, specificity, accuracy)\n\nasd_sub %&gt;%\n  modelr::add_predictions(fit, type = 'response') %&gt;%\n  class_metrics(estimate = factor(pred &gt; 0.5),\n                truth = factor(group), \n                event_level = 'second') %&gt;%\n  knitr::kable()\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nsensitivity\nbinary\n0.7631579\n\n\nspecificity\nbinary\n0.7435897\n\n\naccuracy\nbinary\n0.7532468"
  },
  {
    "objectID": "pstat197a/slides/week4-logistic.html#roc-analysis",
    "href": "pstat197a/slides/week4-logistic.html#roc-analysis",
    "title": "Classification with logistic regression",
    "section": "ROC analysis",
    "text": "ROC analysis\nThe error rates you just calculated are based on classifying a subject as ASD whenever \\(\\hat{p}_i &gt; 0.5\\).\n\nif we wanted a more sensitive classifier, could use \\(\\hat{p}_i &gt; 0.4\\);\nfor a more specific classifier, use \\(\\hat{p}_i &gt; 0.6\\).\n\n\n\n\nA receiver operating characteristic (ROC) curve shows this tradeoff between sensitivity and specificity."
  },
  {
    "objectID": "pstat197a/slides/week4-logistic.html#other-accuracy-metrics",
    "href": "pstat197a/slides/week4-logistic.html#other-accuracy-metrics",
    "title": "Classification with logistic regression",
    "section": "Other accuracy metrics",
    "text": "Other accuracy metrics\nSome other metrics that are useful to know:\n\nprecision \\(\\frac{TP}{TP + FP}\\), proportion of estimated positives that are correct\nfalse discovery rate \\(\\frac{FP}{TP + FP}\\), proportion of estimated positives that are incorrect\nF1 score \\(\\frac{2TP}{2TP + FP + FN}\\), harmonic mean of precision and recall\nAUROC area under ROC curve"
  },
  {
    "objectID": "pstat197a/slides/week4-logistic.html#next-time",
    "href": "pstat197a/slides/week4-logistic.html#next-time",
    "title": "Classification with logistic regression",
    "section": "Next time",
    "text": "Next time\n\nvariable selection via regularized estimation\ndesign assessment of Hewitson analysis\n\nmini assignment: sketch a diagram representing the data analysis in the paper; come prepared to share"
  },
  {
    "objectID": "pstat197a/slides/week7-forecasting.html#from-last-time",
    "href": "pstat197a/slides/week7-forecasting.html#from-last-time",
    "title": "Building a forecasting model",
    "section": "From last time",
    "text": "From last time\n\nPooled data from all sites with at least a year of continuous observation between 2017 and 2020 at 0.2m depth\nModeled seasonal trend based on elevation and day of the year using curve fitting techniques"
  },
  {
    "objectID": "pstat197a/slides/week7-forecasting.html#seasonal-trend-model",
    "href": "pstat197a/slides/week7-forecasting.html#seasonal-trend-model",
    "title": "Building a forecasting model",
    "section": "Seasonal trend model",
    "text": "Seasonal trend model\nFrom last time: using 4 Fourier bases and elevation.\n\n\\(Y_{i, t} = \\beta_0 + \\beta_1 \\text{elev}_i + \\sum_{j = 1}^4 \\gamma_j \\phi_j(t) + \\epsilon_{i, t}\\)"
  },
  {
    "objectID": "pstat197a/slides/week7-forecasting.html#a-small-adjustment",
    "href": "pstat197a/slides/week7-forecasting.html#a-small-adjustment",
    "title": "Building a forecasting model",
    "section": "A small adjustment",
    "text": "A small adjustment\nAdding elevation x seasonality interaction terms.\n\n\\(Y_{i, t} = \\beta_0 + \\beta_1 \\text{elev}_i + \\sum_{j = 1}^4 \\left(\\gamma_j \\phi_j(t) + \\color{maroon}{\\delta_j \\text{elev}\\times\\phi_j(t)}\\right) + \\epsilon_{i, t}\\)"
  },
  {
    "objectID": "pstat197a/slides/week7-forecasting.html#seasonal-forecast",
    "href": "pstat197a/slides/week7-forecasting.html#seasonal-forecast",
    "title": "Building a forecasting model",
    "section": "Seasonal forecast",
    "text": "Seasonal forecast\nSeasonal forecasts ignore recent data.\n\n10-day forecast based on seasonal mean only."
  },
  {
    "objectID": "pstat197a/slides/week7-forecasting.html#forecasting-error",
    "href": "pstat197a/slides/week7-forecasting.html#forecasting-error",
    "title": "Building a forecasting model",
    "section": "Forecasting error",
    "text": "Forecasting error\nThis leads to greater error.\n\nSeasonal forecast vs. observed temperature."
  },
  {
    "objectID": "pstat197a/slides/week7-forecasting.html#a-better-forecast",
    "href": "pstat197a/slides/week7-forecasting.html#a-better-forecast",
    "title": "Building a forecasting model",
    "section": "A better forecast",
    "text": "A better forecast\nIdea: the present temperature at time \\(t\\) contains useful information about the expected temperature at time \\(t + 1\\).\n\nOur model for site \\(i\\) at time \\(t\\) is:\n\\[\n\\underbrace{Y_{i, t}}_{\\text{temperature}} = \\underbrace{\\mu(i, t)}_{\\text{seasonal mean}} + \\underbrace{\\epsilon_{i, t}}_{\\text{random deviation}}\n\\]\n\n\nThe conditional mean forecast \\(\\mathbb{E}(Y_{i, t + 1}| Y_{i, t})\\) should be better than the seasonal forecast \\(\\mathbb{E}Y_{i, t + 1} = \\mu(i, t + 1)\\)…\n\n\n… because it incorporates information about the recent past."
  },
  {
    "objectID": "pstat197a/slides/week7-forecasting.html#conditional-mean-forecast",
    "href": "pstat197a/slides/week7-forecasting.html#conditional-mean-forecast",
    "title": "Building a forecasting model",
    "section": "Conditional mean forecast",
    "text": "Conditional mean forecast\nThe conditional mean of the next temp given the present is:\n\\[\n\\begin{aligned}\n\\mathbb{E}(Y_{i, t + 1}|Y_{i, t} = y_{i, t})\n&= \\mathbb{E}[\\underbrace{\\mu(i, t + 1)}_{\\text{nonrandom}}|Y_{i, t} = y_{i, t}] + \\mathbb{E}(\\epsilon_{i, t + 1}|Y_{i, t} = y_{i, t}) \\\\\\\\\n&= \\mu(i, t + 1) + \\mathbb{E}(\\epsilon_{i, t + 1}|\\epsilon_{i, t} = \\underbrace{y_{i, t} - \\mu(i, t)}_{\\text{residual}}) \\\\\\\\\n&= \\underbrace{\\mu(i, t + 1)}_{\\text{seasonal forecast}} + \\underbrace{\\mathbb{E}(\\epsilon_{i, t + 1}|\\epsilon_{i, t} = e_{i, t})}_{\\text{forecasted deviation}}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "pstat197a/slides/week7-forecasting.html#modeling-the-residuals",
    "href": "pstat197a/slides/week7-forecasting.html#modeling-the-residuals",
    "title": "Building a forecasting model",
    "section": "Modeling the residuals",
    "text": "Modeling the residuals\nTo forecast the deviation from the seasonal mean, we should model the residuals.\n\\[\ne_{i, t} = Y_{i, t} - \\hat{\\mu}(i, t)\n\\qquad\\text{(residual)}\n\\]"
  },
  {
    "objectID": "pstat197a/slides/week7-forecasting.html#residual-autocorrelation",
    "href": "pstat197a/slides/week7-forecasting.html#residual-autocorrelation",
    "title": "Building a forecasting model",
    "section": "Residual autocorrelation",
    "text": "Residual autocorrelation\n\n\n\n\n\n\n\nResidual vs. lagged residual.\n\n\n\n\n\nResiduals are strongly correlated with their immediately previous value.\nThis is called autocorrelation (think: self-correlation)."
  },
  {
    "objectID": "pstat197a/slides/week7-forecasting.html#an-intuitive-approach-slr",
    "href": "pstat197a/slides/week7-forecasting.html#an-intuitive-approach-slr",
    "title": "Building a forecasting model",
    "section": "An intuitive approach: SLR",
    "text": "An intuitive approach: SLR\n(1) Lag the residuals: \\(\\texttt{resid} = e_{i, t}\\) and \\(\\texttt{resid.lag} = e_{i, t - 1}\\)\n\n\n# A tibble: 6 × 5\n# Groups:   site [1]\n  site   date        temp resid resid.lag\n  &lt;chr&gt;  &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n1 B21K-1 2017-08-15  3.72 -6.18     -6.14\n2 B21K-1 2017-08-16  3.31 -6.52     -6.18\n3 B21K-1 2017-08-17  4.95 -4.81     -6.52\n4 B21K-1 2017-08-18  5.46 -4.24     -4.81\n5 B21K-1 2017-08-19  5.80 -3.82     -4.24\n6 B21K-1 2017-08-20  5.68 -3.87     -3.82\n\n\n\n(2) Fit SLR at one time lag: \\(e_{i, t} = \\beta_0 + \\beta_1 e_{i, t - 1} + \\xi_{i, t}\\).\n\nfit_resid &lt;- lm(resid ~ resid.lag - 1, data = resid_df)"
  },
  {
    "objectID": "pstat197a/slides/week7-forecasting.html#computing-one-step-forecasts",
    "href": "pstat197a/slides/week7-forecasting.html#computing-one-step-forecasts",
    "title": "Building a forecasting model",
    "section": "Computing one-step forecasts",
    "text": "Computing one-step forecasts\n\nSeasonal forecastsResidual forecastsFinal forecasts\n\n\n\\(\\texttt{pred.mean} = \\hat{\\mu}(i, t)\\)\n\n\n# A tibble: 6 × 4\n  date        elev  temp pred.mean\n  &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n1 2019-03-12   396 -2.85     -6.83\n2 2019-03-13   396 -2.86     -6.76\n3 2019-03-14   396 -2.92     -6.69\n4 2019-03-15   396 -3.16     -6.62\n5 2019-03-16   396 -3.34     -6.55\n6 2019-03-17   396 -3.28     -6.47\n\n\n\n\n\\(\\texttt{pred.resid} = \\hat{e}_{i, t} = \\mathbb{E}(e_{i, t}|e_{i, t - 1})\\)\n\n\n# A tibble: 6 × 6\n  date        elev  temp pred.mean resid pred.resid\n  &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;\n1 2019-03-12   396 -2.85     -6.83  3.98      NA   \n2 2019-03-13   396 -2.86     -6.76  3.90       3.90\n3 2019-03-14   396 -2.92     -6.69  3.77       3.83\n4 2019-03-15   396 -3.16     -6.62  3.46       3.69\n5 2019-03-16   396 -3.34     -6.55  3.20       3.39\n6 2019-03-17   396 -3.28     -6.47  3.18       3.14\n\n\n\n\n\\(\\texttt{pred} = \\texttt{pred.mean} + \\texttt{pred.resid} = \\hat{\\mu}(i, t) + \\hat{e}_{i, t}\\)\n\n\n# A tibble: 6 × 7\n  date        elev  temp pred.mean resid pred.resid  pred\n  &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;\n1 2019-03-12   396 -2.85     -6.83  3.98      NA    NA   \n2 2019-03-13   396 -2.86     -6.76  3.90       3.90 -2.86\n3 2019-03-14   396 -2.92     -6.69  3.77       3.83 -2.86\n4 2019-03-15   396 -3.16     -6.62  3.46       3.69 -2.93\n5 2019-03-16   396 -3.34     -6.55  3.20       3.39 -3.15\n6 2019-03-17   396 -3.28     -6.47  3.18       3.14 -3.33"
  },
  {
    "objectID": "pstat197a/slides/week7-forecasting.html#one-step-forecasts",
    "href": "pstat197a/slides/week7-forecasting.html#one-step-forecasts",
    "title": "Building a forecasting model",
    "section": "One-step forecasts",
    "text": "One-step forecasts\n\nSeasonal forecast (blue curve), residual forecasts (red lines), one-step forecasts (dotted line), and observed temperatures (solid black line)."
  },
  {
    "objectID": "pstat197a/slides/week7-forecasting.html#forecasting-error-1",
    "href": "pstat197a/slides/week7-forecasting.html#forecasting-error-1",
    "title": "Building a forecasting model",
    "section": "Forecasting error",
    "text": "Forecasting error\nOne the site we’ve been examining, the one-step forecasting error is:\n\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nccc\nstandard\n0.9981049\n\n\nmsd\nstandard\n0.0536997\n\n\nrmse\nstandard\n0.3494716\n\n\n\n\n\n\nccc is the concordance correlation coefficient\nmsd is the mean signed deviation\nrmse is the root mean squared error"
  },
  {
    "objectID": "pstat197a/slides/week7-forecasting.html#average-forecasting-error-across-sites",
    "href": "pstat197a/slides/week7-forecasting.html#average-forecasting-error-across-sites",
    "title": "Building a forecasting model",
    "section": "Average forecasting error across sites",
    "text": "Average forecasting error across sites\nMeans and standard deviations across all 29 sites of error metrics for one-step forecasts computed across entire observation window:\n\n\n\n\n\n.metric\naverage\nsd\nmin\nmax\nn\n\n\n\n\nccc\n0.9949698\n0.0032392\n0.9869467\n0.9993684\n29\n\n\nmsd\n-0.0062058\n0.0510766\n0.0005584\n0.1186212\n29\n\n\nrmse\n0.6460647\n0.3593531\n0.1801040\n1.4875005\n29"
  },
  {
    "objectID": "pstat197a/slides/week7-forecasting.html#one-step-forecasts-mathematically",
    "href": "pstat197a/slides/week7-forecasting.html#one-step-forecasts-mathematically",
    "title": "Building a forecasting model",
    "section": "One-step forecasts (mathematically)",
    "text": "One-step forecasts (mathematically)\nThe one-step forecasts are the predicted conditional means at the next time step given the present :\n\\[\n\\hat{Y}_{i, t} = \\mathbb{E}(Y_{i, t + 1}|Y_{i, t} = \\color{blue}{y_{i, t}})\n\\]\n\nConditional expectation gives optimal prediction under squared error loss (assuming the model is correct).\n\n\nAccording to our model:\n\\[\n\\hat{Y}_{i, t + 1}\n=  \\hat{\\mu}(i, t + 1) + \\hat{\\alpha}_0 + \\hat{\\alpha}_1 \\left(\\color{blue}{y_{i, t}} - \\hat{\\mu}(i, t)\\right)\n\\]"
  },
  {
    "objectID": "pstat197a/slides/week7-forecasting.html#multi-step-forecasts",
    "href": "pstat197a/slides/week7-forecasting.html#multi-step-forecasts",
    "title": "Building a forecasting model",
    "section": "Multi-step forecasts",
    "text": "Multi-step forecasts\nMultistep forecasts must be computed recursively:\n\\[\n\\begin{aligned}\n\\color{maroon}{\\hat{Y}_{i, t + 1}}\n&= \\mathbb{E}(Y_{i, t + 1}|Y_{i, t} = y_{i, t}) \\\\\n\\color{teal}{\\hat{Y}_{i, t + 2}}\n&= \\mathbb{E}(Y_{i, t + 2}|Y_{i, t + 1} = \\color{maroon}{\\hat{Y}_{i, t + 1}}) \\\\\n\\hat{Y}_{i, t + 3}\n&= \\mathbb{E}(Y_{i, t + 3}|Y_{i, t + 2} = \\color{teal}{\\hat{Y}_{i, t + 2}}) \\\\\n&\\vdots\n\\end{aligned}\n\\]\n\nWhat do you think will happen the farther out we forecast??"
  },
  {
    "objectID": "pstat197a/slides/week7-forecasting.html#multistep-forecasts-on-one-site",
    "href": "pstat197a/slides/week7-forecasting.html#multistep-forecasts-on-one-site",
    "title": "Building a forecasting model",
    "section": "Multistep forecasts on one site",
    "text": "Multistep forecasts on one site"
  },
  {
    "objectID": "pstat197a/slides/week7-forecasting.html#comments",
    "href": "pstat197a/slides/week7-forecasting.html#comments",
    "title": "Building a forecasting model",
    "section": "Comments",
    "text": "Comments\nThis approach pooled data across sites to estimate model quantities.\n\nseasonal mean (using Fourier basis approximation)\nresidual autocorrelation at one lag (using SLR)\n\n\nWorks pretty well for one-step forecasts; not very well for longer-term forecasts."
  },
  {
    "objectID": "pstat197a/slides/week7-forecasting.html#site-specific-approach",
    "href": "pstat197a/slides/week7-forecasting.html#site-specific-approach",
    "title": "Building a forecasting model",
    "section": "Site-specific approach",
    "text": "Site-specific approach\nAn alternative to pooling data together to estimate the seasonal trend and residual autocorrelation is to do so individually for every site.\n\nupshot: more flexibility on approaches; can use time series techniques\ndownside: many models \\(\\longrightarrow\\) more total uncertainty\n\n\nFor now we’ll leave the seasonality alone and revise the residual autocorrelation approach."
  },
  {
    "objectID": "pstat197a/slides/week7-forecasting.html#autoregression",
    "href": "pstat197a/slides/week7-forecasting.html#autoregression",
    "title": "Building a forecasting model",
    "section": "Autoregression",
    "text": "Autoregression\nAn autoregressive model of order \\(D\\) is\n\\[\nX_t = \\nu + \\alpha_1 X_{t - 1} + \\cdots + \\alpha_D X_{t - D} + \\epsilon_t\n\\]\n\n‘innovations’ \\(\\epsilon_t\\) are \\(iid\\) with mean zero\nprocess mean linear in \\(D\\) lags\n\\(\\mathbb{E}X_t\\) and \\(\\text{var}X_t\\) are constant in time (‘weak stationarity’)"
  },
  {
    "objectID": "pstat197a/slides/week7-forecasting.html#technical-asides",
    "href": "pstat197a/slides/week7-forecasting.html#technical-asides",
    "title": "Building a forecasting model",
    "section": "Technical asides",
    "text": "Technical asides\nAbout the AR parameters:\n\nconstraints on \\(\\alpha_j\\)’s needed for a well-defined process in infinite time\nestimates \\(\\hat{\\alpha}_j\\) found by:\n\n(moment estimator) solving a recursive system of equations (known as the Yule-Walker equations)\n(mle) maximum likelihood assuming \\(\\epsilon_{t} \\sim N(0, \\sigma^2)\\)"
  },
  {
    "objectID": "pstat197a/slides/week7-forecasting.html#site-specific-model",
    "href": "pstat197a/slides/week7-forecasting.html#site-specific-model",
    "title": "Building a forecasting model",
    "section": "Site-specific model",
    "text": "Site-specific model\nSo let’s revise:\n\\[\n\\begin{aligned}\nY_{i, t} &= f_i (t) + \\epsilon_{i, t} \\quad\\text{(nonlinear regression)} \\\\\n\\epsilon_{i, t} &= \\sum_{d = 1}^D \\alpha_{i,d}\\epsilon_{i, t - d} + \\xi_{i, t} \\quad\\text{(AR(D) errors)}\n\\end{aligned}\n\\]\n\nNote:\n\nseasonal mean \\(f_i(t)\\) is site-dependent (hence subscript)\nAR process is site-dependent (hence \\(\\alpha_{i, d}\\) subscript)\nno elevation included, since it is constant for each site"
  },
  {
    "objectID": "pstat197a/slides/week7-forecasting.html#fit-comparison",
    "href": "pstat197a/slides/week7-forecasting.html#fit-comparison",
    "title": "Building a forecasting model",
    "section": "Fit comparison",
    "text": "Fit comparison\n\nAR fitComparision of estimates\n\n\n\nfit_ar2 &lt;- arima(y_train, \n      order = c(2, 0, 0), \n      xreg = x_train, \n      include.mean = T, \n      method = 'ML')\n\ntidy(fit_ar2) %&gt;% knitr::kable()\n\n\n\n\nterm\nestimate\nstd.error\n\n\n\n\nar1\n1.5389488\n0.0310331\n\n\nar2\n-0.5872016\n0.0310424\n\n\nintercept\n1.7120661\n0.2661139\n\n\nsin1\n-57.4478541\n4.9296869\n\n\ncos1\n-97.6093126\n5.1741646\n\n\nsin2\n9.5264551\n4.9135875\n\n\ncos2\n15.0945042\n5.0136827\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate.ar\nestimate.pooled\n\n\n\n\nar1\n1.5389488\nNA\n\n\nar2\n-0.5872016\nNA\n\n\nintercept\n1.7120661\n1.7428057\n\n\nsin1\n-57.4478541\n-60.4797059\n\n\ncos1\n-97.6093126\n-84.9711125\n\n\nsin2\n9.5264551\n7.0360375\n\n\ncos2\n15.0945042\n15.0271089\n\n\nelev\nNA\n-0.0038422\n\n\nelevsin1\nNA\n0.0094721\n\n\nelevsin2\nNA\n0.0063869\n\n\nelevcos1\nNA\n-0.0627032\n\n\nelevcos2\nNA\n-0.0032467\n\n\nresidlag\nNA\n0.9804500"
  },
  {
    "objectID": "pstat197a/slides/week7-forecasting.html#forecast-comparison",
    "href": "pstat197a/slides/week7-forecasting.html#forecast-comparison",
    "title": "Building a forecasting model",
    "section": "Forecast comparison",
    "text": "Forecast comparison"
  },
  {
    "objectID": "pstat197a/slides/week7-forecasting.html#next-time",
    "href": "pstat197a/slides/week7-forecasting.html#next-time",
    "title": "Building a forecasting model",
    "section": "Next time",
    "text": "Next time\nSpatial prediction…\n\nbased on observations\nbased on forecasts"
  },
  {
    "objectID": "pstat197a/slides/week7-curvefitting.html#word-clouds",
    "href": "pstat197a/slides/week7-curvefitting.html#word-clouds",
    "title": "Curve fitting using basis approximations",
    "section": "Word clouds",
    "text": "Word clouds\n\nFrom self-assessments: identify a helpful feature that improved groupwork."
  },
  {
    "objectID": "pstat197a/slides/week7-curvefitting.html#word-clouds-1",
    "href": "pstat197a/slides/week7-curvefitting.html#word-clouds-1",
    "title": "Curve fitting using basis approximations",
    "section": "Word clouds",
    "text": "Word clouds\n\nFrom self-assessments: identify an area of improvement"
  },
  {
    "objectID": "pstat197a/slides/week7-curvefitting.html#above",
    "href": "pstat197a/slides/week7-curvefitting.html#above",
    "title": "Curve fitting using basis approximations",
    "section": "ABoVE",
    "text": "ABoVE\nThe Arctic-Boreal Vulnerability Experiment (ABoVE) is a NASA Terrestrial Ecology Program field campaign in Alaska and western Canada from 2016 to 2021.\n\nResearch for ABoVE will link field-based, process-level studies with geospatial data products derived from airborne and satellite sensors, providing a foundation for improving the analysis, and modeling capabilities needed to understand and predict ecosystem responses and societal implications.\n\n\nFind ABoVE data here."
  },
  {
    "objectID": "pstat197a/slides/week7-curvefitting.html#above-soil-temperatures",
    "href": "pstat197a/slides/week7-curvefitting.html#above-soil-temperatures",
    "title": "Curve fitting using basis approximations",
    "section": "ABoVE Soil Temperatures",
    "text": "ABoVE Soil Temperatures\nWe’ll work with soil temperatures.\n\nNicolsky, D.J., V.E. Romanovsky, A.L. Kholodov, K. Dolgikh, and N. Hasson. 2022. ABoVE: Soil Temperature Profiles, USArray Seismic Stations, 2016-2021. ORNL DAAC, Oak Ridge, Tennessee, USA. https://doi.org/10.3334/ORNLDAAC/1680\n\n\nObservations of soil temperatures (centigrade)\nMeasured at 63 locations in Alaska\nRecorded four times daily at multiple depths"
  },
  {
    "objectID": "pstat197a/slides/week7-curvefitting.html#site-locations",
    "href": "pstat197a/slides/week7-curvefitting.html#site-locations",
    "title": "Curve fitting using basis approximations",
    "section": "Site locations",
    "text": "Site locations\nWe’ll use 57 of the 63 sites for now.\n\nLocations of selected monitoring stations."
  },
  {
    "objectID": "pstat197a/slides/week7-curvefitting.html#example-rows",
    "href": "pstat197a/slides/week7-curvefitting.html#example-rows",
    "title": "Curve fitting using basis approximations",
    "section": "Example rows",
    "text": "Example rows\n\n\n# A tibble: 6 × 7\n  site   latitude longitude elevation date_time           depth  temp\n  &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dttm&gt;              &lt;dbl&gt; &lt;dbl&gt;\n1 B21K-1     69.6     -155.        96 2017-08-14 12:00:00   0    3.46\n2 B21K-1     69.6     -155.        96 2017-08-14 12:00:00   0.2  3.48\n3 B21K-1     69.6     -155.        96 2017-08-14 12:00:00   1    3.38\n4 B21K-1     69.6     -155.        96 2017-08-14 12:00:00   1.5  3.31\n5 B21K-1     69.6     -155.        96 2017-08-14 18:00:00   0    3.92\n6 B21K-1     69.6     -155.        96 2017-08-14 18:00:00   0.2  4.16"
  },
  {
    "objectID": "pstat197a/slides/week7-curvefitting.html#temperature-profiles",
    "href": "pstat197a/slides/week7-curvefitting.html#temperature-profiles",
    "title": "Curve fitting using basis approximations",
    "section": "Temperature profiles",
    "text": "Temperature profiles\n\nObservations for a single site at four depths (one path per depth).\nWhat is happening over time?\nWhat is happening across depth?\nAny other observations?"
  },
  {
    "objectID": "pstat197a/slides/week7-curvefitting.html#comparing-sites",
    "href": "pstat197a/slides/week7-curvefitting.html#comparing-sites",
    "title": "Curve fitting using basis approximations",
    "section": "Comparing sites",
    "text": "Comparing sites\n\nProfiles at two sites.\nHow do the sites differ?\nHow are they similar?"
  },
  {
    "objectID": "pstat197a/slides/week7-curvefitting.html#comparing-sites-1",
    "href": "pstat197a/slides/week7-curvefitting.html#comparing-sites-1",
    "title": "Curve fitting using basis approximations",
    "section": "Comparing sites",
    "text": "Comparing sites\n\n\n\n\n\n\n\n\nHere are the locations of the sites just compared.\n\nwhat factors might account for some of the differences in temperature profiles between the sites?\nare any of them recorded in our data?"
  },
  {
    "objectID": "pstat197a/slides/week7-curvefitting.html#goals",
    "href": "pstat197a/slides/week7-curvefitting.html#goals",
    "title": "Curve fitting using basis approximations",
    "section": "Goals",
    "text": "Goals\nOur overall goal this week is to build a forecasting model.\n\nStrategy:\n\nTo start, approximate the seasonal trend.\nDe-trend the data and model the correlation structure of deviations from seasonal trend.\nForecast as: \\(\\text{trend} + \\mathbb{E}(\\text{future}| \\text{present})\\)"
  },
  {
    "objectID": "pstat197a/slides/week7-curvefitting.html#annual-cycles",
    "href": "pstat197a/slides/week7-curvefitting.html#annual-cycles",
    "title": "Curve fitting using basis approximations",
    "section": "Annual cycles",
    "text": "Annual cycles\nThe seasonality is annual – let’s examine the annual cycle instead of the usual time course plot.\n\n\n\n\n\n\n\nAnnual cycle for site H17K-1 at 0.2m depth.\n\n\n\n\n\n\nCan you see the start and stop dates in the plot?\nAny other observations?"
  },
  {
    "objectID": "pstat197a/slides/week7-curvefitting.html#pooling-sites",
    "href": "pstat197a/slides/week7-curvefitting.html#pooling-sites",
    "title": "Curve fitting using basis approximations",
    "section": "Pooling sites",
    "text": "Pooling sites\nHow would you estimate the annual cycle based on data at each site?\n\nDaily average temperatures at 0.2m depth for 37 sites, 2017-2019."
  },
  {
    "objectID": "pstat197a/slides/week7-curvefitting.html#as-an-estimation-problem",
    "href": "pstat197a/slides/week7-curvefitting.html#as-an-estimation-problem",
    "title": "Curve fitting using basis approximations",
    "section": "As an estimation problem",
    "text": "As an estimation problem\nModeling the trend can be formulated as estimating the model:\n\\[\nY_{i, t} = f(t) + \\epsilon_{i, t}\n\\]\n\nWhere:\n\n\\(Y_{i, t}\\) is the temperature at site \\(i\\) and time \\(t\\)\n\\(f(t)\\) is the mean temperature at time \\(t\\)\n\\(\\epsilon_{i, t}\\) is a random error\n\n\n\nBut how do you estimate an arbitrary function?"
  },
  {
    "objectID": "pstat197a/slides/week7-curvefitting.html#basis-functions",
    "href": "pstat197a/slides/week7-curvefitting.html#basis-functions",
    "title": "Curve fitting using basis approximations",
    "section": "Basis functions",
    "text": "Basis functions\nA basis function is an element of a basis for a function space.\n\nIf \\(\\{f_j\\}\\) form a basis for a function space \\(C\\) then\n\\[\nf \\in C \\quad\\Longleftrightarrow f = \\sum_j c_j f_j\n\\]\n\n\nA finite subset of basis functions can be used to approximate functions in the space:\n\\[\nf \\approx \\sum_{j = 1}^J c_j f_j\n\\]"
  },
  {
    "objectID": "pstat197a/slides/week7-curvefitting.html#basis-approximation",
    "href": "pstat197a/slides/week7-curvefitting.html#basis-approximation",
    "title": "Curve fitting using basis approximations",
    "section": "Basis approximation",
    "text": "Basis approximation\nA nifty trick is to estimate \\(f\\) using a suitable basis approximation:\n\\[\nY_{i, t} = \\beta_0 + \\color{maroon}{\\underbrace{\\sum_{j = 1}^J \\beta_j f_j(t)}_{\\tilde{f}(t) \\approx f(t)}} + \\epsilon_{i, t}\n\\]\n\nThis model can be fit using standard linear regression. (Think of the \\(f_j(t)\\)’s as \\(J\\) ‘new’ predictors.)"
  },
  {
    "objectID": "pstat197a/slides/week7-curvefitting.html#spline-basis",
    "href": "pstat197a/slides/week7-curvefitting.html#spline-basis",
    "title": "Curve fitting using basis approximations",
    "section": "Spline basis",
    "text": "Spline basis\nThe spline basis is a basis for piecewise polynomials of a specified order.\n\nBases for piecewise polynomials of order 1 through 4 joined at evenly-spaced knot points.\nGenerated recursively based on ‘knots’ – joining locations"
  },
  {
    "objectID": "pstat197a/slides/week7-curvefitting.html#knot-spacing",
    "href": "pstat197a/slides/week7-curvefitting.html#knot-spacing",
    "title": "Curve fitting using basis approximations",
    "section": "Knot spacing",
    "text": "Knot spacing\nKnot spacing will affect how densely basis functions are concentrated around particular regions of data.\n\nHere are bases generated on some unevenly-spaced knots:\n\n\n\n\n\n\n\n\n\n\n\nCheck your understanding: where would this spline basis have the most flexible approximation capability?"
  },
  {
    "objectID": "pstat197a/slides/week7-curvefitting.html#knot-placement",
    "href": "pstat197a/slides/week7-curvefitting.html#knot-placement",
    "title": "Curve fitting using basis approximations",
    "section": "Knot placement",
    "text": "Knot placement\nAppropriate placement of knots is essential for quality function approximation.\n\ndefault: place at data quantiles\nbetter: concentrated in regions with irregular trend\n\n\n\n\n\n\n\n\n\n\nWhere would you put them for our data?"
  },
  {
    "objectID": "pstat197a/slides/week7-curvefitting.html#a-first-attempt-spline-basis",
    "href": "pstat197a/slides/week7-curvefitting.html#a-first-attempt-spline-basis",
    "title": "Curve fitting using basis approximations",
    "section": "A first attempt: spline basis",
    "text": "A first attempt: spline basis\nModel: \\(Y_{i,t} = \\beta_0 + \\beta_1\\cdot\\text{elev}_i + \\sum_{j = 1}^7 \\gamma_j \\cdot f_j(t) + \\epsilon_{i, t}\\)\n\nKnot placementFitted curve\n\n\n\n\n\n\n\nKnots placed at vertical lines.\n\n\n\n\n\n\n\n\n\n\n\nEstimated mean with 95% prediction interval at median site elevation."
  },
  {
    "objectID": "pstat197a/slides/week7-curvefitting.html#a-problem",
    "href": "pstat197a/slides/week7-curvefitting.html#a-problem",
    "title": "Curve fitting using basis approximations",
    "section": "A problem",
    "text": "A problem\n\n\n\n\n\n\n\nSpline bases produce discontinuities\n\n\n\n\n\nThe choice of basis must match problem context.\n\nhere, need boundaries to meet\nin other words, need a harmonic function"
  },
  {
    "objectID": "pstat197a/slides/week7-curvefitting.html#fourier-basis",
    "href": "pstat197a/slides/week7-curvefitting.html#fourier-basis",
    "title": "Curve fitting using basis approximations",
    "section": "Fourier basis",
    "text": "Fourier basis\nThe Fourier basis is a basis for square-integrable functions on closed intervals consisting of sine-cosine pairs.\n\n\n\n\n\n\n4 Fourier basis functions on the interval [1, 365]."
  },
  {
    "objectID": "pstat197a/slides/week7-curvefitting.html#second-try",
    "href": "pstat197a/slides/week7-curvefitting.html#second-try",
    "title": "Curve fitting using basis approximations",
    "section": "Second try",
    "text": "Second try\n\nSeasonal meanTime plot\n\n\n\n\n\n\n\nSeasonal mean approximation using 4 Fourier basis functions.\n\n\n\n\n\n\n\n\n\n\n\nSeasonal mean approxiamtion using 4 Fourier basis functions."
  },
  {
    "objectID": "pstat197a/slides/week7-curvefitting.html#forecasting",
    "href": "pstat197a/slides/week7-curvefitting.html#forecasting",
    "title": "Curve fitting using basis approximations",
    "section": "Forecasting",
    "text": "Forecasting\nDoes this forecast make sense? Why or why not?"
  },
  {
    "objectID": "pstat197a/slides/week7-curvefitting.html#next-time",
    "href": "pstat197a/slides/week7-curvefitting.html#next-time",
    "title": "Curve fitting using basis approximations",
    "section": "Next time",
    "text": "Next time\n\nFit a time series model to the residuals\n\\[\ne_{i, t} = Y_{i, t} - \\underbrace{\\left(\\hat{\\beta_0} + \\hat{\\beta_1}\\text{elev}_i + \\hat{f}(t)\\right)}_{\\text{mean function } \\hat{\\mu}(i, t)}\n\\]\nForecast \\(\\hat{e}_{i, t} = \\mathbb{E}\\left(e_{i, t}|e_{i, t - 1}\\right)\\) using the residual model\n“Feed forward” residual forecasts to obtain temperature forecasts\n\\[\n\\hat{Y}_{i, t} = \\hat{\\mu}(i, t) + \\hat{e}_{i, t}\n\\]"
  },
  {
    "objectID": "pstat197a/slides/week3-randomforest.html#announcementsreminders",
    "href": "pstat197a/slides/week3-randomforest.html#announcementsreminders",
    "title": "Random forests",
    "section": "Announcements/reminders",
    "text": "Announcements/reminders\nMake your final commits for the first group assignment by Friday 10/14 11:59pm PST\n\nshould include an updated report.qmd and report.html with your write-up\n\n\nNext group assignment to be distributed Tuesday 10/18.\n\nGroups will be randomly assigned\nTask: replicate and redesign proteomic analysis"
  },
  {
    "objectID": "pstat197a/slides/week3-randomforest.html#last-time",
    "href": "pstat197a/slides/week3-randomforest.html#last-time",
    "title": "Random forests",
    "section": "Last time",
    "text": "Last time\n\nintroduced ASD proteomic dataset\nused multiple testing corrections to identify proteins whose serum levels differ significantly between ASD/TD groups\ndiscussed the difference between controlling familywise error vs. false discovery"
  },
  {
    "objectID": "pstat197a/slides/week3-randomforest.html#today",
    "href": "pstat197a/slides/week3-randomforest.html#today",
    "title": "Random forests",
    "section": "Today",
    "text": "Today\nWe’ll talk about two more approaches to identifying proteins of interest:\n\ncorrelation-based identification of proteins\nrandom forest classifier"
  },
  {
    "objectID": "pstat197a/slides/week3-randomforest.html#ados-score",
    "href": "pstat197a/slides/week3-randomforest.html#ados-score",
    "title": "Random forests",
    "section": "ADOS score",
    "text": "ADOS score\nAutism Diagnostic Observation Schedule (ADOS) scores are determined by psychological assessment and measure ASD severity.\n\nasd %&gt;% \n  select(group, ados) %&gt;% \n  group_by(group) %&gt;% \n  sample_n(size = 2)\n\n# A tibble: 4 × 2\n# Groups:   group [2]\n  group  ados\n  &lt;chr&gt; &lt;dbl&gt;\n1 ASD      20\n2 ASD      10\n3 TD       NA\n4 TD       NA\n\n\n\nados is only measured for the ASD group\nnumerical score between 6 and 23 (at least in this sample)"
  },
  {
    "objectID": "pstat197a/slides/week3-randomforest.html#correlation-approach",
    "href": "pstat197a/slides/week3-randomforest.html#correlation-approach",
    "title": "Random forests",
    "section": "Correlation approach",
    "text": "Correlation approach\nSo here’s an idea:\n\ncompute correlations of each protein with ADOS;\npick the 10 proteins with the strongest correlation"
  },
  {
    "objectID": "pstat197a/slides/week3-randomforest.html#computation",
    "href": "pstat197a/slides/week3-randomforest.html#computation",
    "title": "Random forests",
    "section": "Computation",
    "text": "Computation\nThis is a simple aggregation operation and can be executed in R with summarize() :\n\n# compute correlations\nasd_clean %&gt;%\n  pivot_longer(cols = -ados,\n               names_to = 'protein',\n               values_to = 'level') %&gt;%\n  group_by(protein) %&gt;%\n  summarize(correlation = cor(ados, level))\n\n# A tibble: 1,317 × 2\n   protein                   correlation\n   &lt;chr&gt;                           &lt;dbl&gt;\n 1 14-3-3                         0.0970\n 2 14-3-3 protein beta/alpha      0.0481\n 3 14-3-3 protein theta           0.122 \n 4 14-3-3 protein zeta/delta      0.0735\n 5 14-3-3E                       -0.127 \n 6 17-beta-HSD 1                  0.0969\n 7 3HAO                           0.0773\n 8 3HIDH                          0.0452\n 9 4-1BB                          0.0290\n10 4-1BB ligand                   0.0799\n# … with 1,307 more rows"
  },
  {
    "objectID": "pstat197a/slides/week3-randomforest.html#visual-assessment",
    "href": "pstat197a/slides/week3-randomforest.html#visual-assessment",
    "title": "Random forests",
    "section": "Visual assessment",
    "text": "Visual assessment"
  },
  {
    "objectID": "pstat197a/slides/week3-randomforest.html#sort-and-slice",
    "href": "pstat197a/slides/week3-randomforest.html#sort-and-slice",
    "title": "Random forests",
    "section": "Sort and slice",
    "text": "Sort and slice\n\nListVisual\n\n\n\n\n# A tibble: 10 × 4\n   protein          correlation abs.corr  rank\n   &lt;chr&gt;                  &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt;\n 1 CO8A1                  0.362    0.362  1317\n 2 C5b, 6 Complex        -0.337    0.337     1\n 3 Thrombospondin-1       0.310    0.310  1316\n 4 ILT-2                 -0.309    0.309     2\n 5 TRAIL R4               0.296    0.296  1315\n 6 HCE000414             -0.296    0.296     3\n 7 C1r                   -0.290    0.290     4\n 8 GM-CSF                 0.290    0.290  1314\n 9 Angiogenin            -0.284    0.284     5\n10 HCG                    0.278    0.278  1313"
  },
  {
    "objectID": "pstat197a/slides/week3-randomforest.html#slr-coefficients-instead",
    "href": "pstat197a/slides/week3-randomforest.html#slr-coefficients-instead",
    "title": "Random forests",
    "section": "SLR coefficients instead?",
    "text": "SLR coefficients instead?\nFact: the simple linear regression coefficient estimate is proportional to the correlation coefficient.\n\nSo it should give similar results to sort the SLR coefficients by significance.\n\n\n\n\n# A tibble: 1,317 × 3\n   protein          estimate p.value\n   &lt;chr&gt;               &lt;dbl&gt;   &lt;dbl&gt;\n 1 CO8A1               0.391 0.00131\n 2 C5b, 6 Complex     -0.401 0.00290\n 3 Thrombospondin-1    0.317 0.00635\n 4 ILT-2              -0.546 0.00664\n 5 TRAIL R4            0.353 0.00933\n 6 HCE000414          -0.278 0.00950\n 7 C1r                -0.341 0.0110 \n 8 GM-CSF              0.345 0.0110 \n 9 Angiogenin         -0.314 0.0130 \n10 HCG                 0.306 0.0149 \n# … with 1,307 more rows"
  },
  {
    "objectID": "pstat197a/slides/week3-randomforest.html#fdr-control",
    "href": "pstat197a/slides/week3-randomforest.html#fdr-control",
    "title": "Random forests",
    "section": "FDR control",
    "text": "FDR control\nIf we do the correlation analysis this way, do the identified proteins pass multiple testing significance thresholds?\n\n\n# A tibble: 10 × 3\n   protein          p.value  p.adj\n   &lt;chr&gt;              &lt;dbl&gt;  &lt;dbl&gt;\n 1 CO8A1            0.00131 0.0145\n 2 C5b, 6 Complex   0.00290 0.0342\n 3 Thrombospondin-1 0.00635 0.112 \n 4 ILT-2            0.00664 0.0641\n 5 TRAIL R4         0.00933 1.29  \n 6 HCE000414        0.00950 0.916 \n 7 C1r              0.0110  0.242 \n 8 GM-CSF           0.0110  0.124 \n 9 Angiogenin       0.0130  0.142 \n10 HCG              0.0149  0.159 \n\n\n\nprobably just introducing selection noise\nbut also, this result diverges considerably from the paper (?)"
  },
  {
    "objectID": "pstat197a/slides/week3-randomforest.html#aside-correlation-test",
    "href": "pstat197a/slides/week3-randomforest.html#aside-correlation-test",
    "title": "Random forests",
    "section": "Aside: correlation test",
    "text": "Aside: correlation test\nThe SLR approach is equivalent to sorting the correlations. (Take a moment to check the results.)\n\nWe could use inference on the population correlation to obtain a \\(p\\)-value associated with each sample correlation coefficient. These match the ones from SLR.\n\ncor_test &lt;- function(x, y){\n  cor_out &lt;- cor.test(x, y)\n  tibble(estimate = cor_out$estimate,\n         p.value = cor_out$p.value)\n}\n\nasd_clean %&gt;%\n  pivot_longer(cols = -ados,\n               names_to = 'protein',\n               values_to = 'level') %&gt;%\n  group_by(protein) %&gt;%\n  summarize(correlation = cor_test(ados, level)) %&gt;%\n  unnest(correlation) %&gt;%\n  arrange(p.value)\n\n# A tibble: 1,317 × 3\n   protein          estimate p.value\n   &lt;chr&gt;               &lt;dbl&gt;   &lt;dbl&gt;\n 1 CO8A1               0.362 0.00131\n 2 C5b, 6 Complex     -0.337 0.00290\n 3 Thrombospondin-1    0.310 0.00635\n 4 ILT-2              -0.309 0.00664\n 5 TRAIL R4            0.296 0.00933\n 6 HCE000414          -0.296 0.00950\n 7 C1r                -0.290 0.0110 \n 8 GM-CSF              0.290 0.0110 \n 9 Angiogenin         -0.284 0.0130 \n10 HCG                 0.278 0.0149 \n# … with 1,307 more rows"
  },
  {
    "objectID": "pstat197a/slides/week3-randomforest.html#background",
    "href": "pstat197a/slides/week3-randomforest.html#background",
    "title": "Random forests",
    "section": "Background",
    "text": "Background\nA binary tree is a directed graph in which:\n\nthere is at most one path between any two nodes\neach node has at most two outward-directed edges"
  },
  {
    "objectID": "pstat197a/slides/week3-randomforest.html#classification-tree",
    "href": "pstat197a/slides/week3-randomforest.html#classification-tree",
    "title": "Random forests",
    "section": "Classification tree",
    "text": "Classification tree\nA classification tree is a binary tree in which the paths represent classification rules.\n\nA goofy classification tree."
  },
  {
    "objectID": "pstat197a/slides/week3-randomforest.html#example-classifying-high-earners",
    "href": "pstat197a/slides/week3-randomforest.html#example-classifying-high-earners",
    "title": "Random forests",
    "section": "Example: classifying high earners",
    "text": "Example: classifying high earners\nSay we want to predict income based on captial gains and education level using census data.\n\n\n# A tibble: 6 × 3\n# Groups:   income [2]\n  income educ     capital_gain\n  &lt;fct&gt;  &lt;fct&gt;           &lt;dbl&gt;\n1 &lt;=50K  hs                  0\n2 &lt;=50K  advanced            0\n3 &lt;=50K  college             0\n4 &gt;50K   hs               7688\n5 &gt;50K   hs               7688\n6 &gt;50K   college          5178\n\n\n\nWe could construct a classification tree by ‘splitting’ based on the values of predictor variables."
  },
  {
    "objectID": "pstat197a/slides/week3-randomforest.html#activity-building-trees",
    "href": "pstat197a/slides/week3-randomforest.html#activity-building-trees",
    "title": "Random forests",
    "section": "Activity: building trees",
    "text": "Activity: building trees\nTo get a sense of the process of tree construction, we’ll do an activity in groups: each group will build a tree ‘by hand’.\n\nfirst let’s look at the instructions together\nthen take about 15-20 minutes to do it"
  },
  {
    "objectID": "pstat197a/slides/week3-randomforest.html#random-forests",
    "href": "pstat197a/slides/week3-randomforest.html#random-forests",
    "title": "Random forests",
    "section": "Random forests",
    "text": "Random forests\nA random forest is a classifier based on many trees. It is constructed by:\n\nbuilding some large number of \\(T\\) trees using bootstrap samples and random subsets of predictors (what you just did, repeated many times)\ntaking a majority vote across all trees to determine the classification\n\n\nSo let’s take a vote using your trees!"
  },
  {
    "objectID": "pstat197a/slides/week3-randomforest.html#variable-importance-scores",
    "href": "pstat197a/slides/week3-randomforest.html#variable-importance-scores",
    "title": "Random forests",
    "section": "Variable importance scores",
    "text": "Variable importance scores\nIf the number of trees \\(T\\) is large (as it should be):\n\ntrees are built using lots of random subsets of predictors\ncan keep track of which ones are used most often to define splits\n\n\nVariable importance scores provide a measure of how influential each predictor is in a random forest."
  },
  {
    "objectID": "pstat197a/slides/week3-randomforest.html#results",
    "href": "pstat197a/slides/week3-randomforest.html#results",
    "title": "Random forests",
    "section": "Results",
    "text": "Results\nBack to the proteomics data, the variable importance scores from a random forest provide another means of identifying proteins.\n\n# grow RF\nset.seed(101222)\nrf_out &lt;- randomForest(x = asd_preds, y = asd_resp,\n                       mtry = 100, ntree = 1000, \n                       importance = T)\n\n# variable importance\nrf_out$importance %&gt;% \n  as_tibble() %&gt;%\n  mutate(protein = rownames(rf_out$importance)) %&gt;%\n  slice_max(MeanDecreaseGini, n = 10) %&gt;%\n  select(protein)\n\n# A tibble: 10 × 1\n   protein    \n   &lt;chr&gt;      \n 1 DERM       \n 2 IgD        \n 3 TGF-b R III\n 4 TSP4       \n 5 Notch 1    \n 6 MAPK14     \n 7 RELT       \n 8 ERBB1      \n 9 CK-MB      \n10 SOST"
  },
  {
    "objectID": "pstat197a/slides/week3-randomforest.html#errors",
    "href": "pstat197a/slides/week3-randomforest.html#errors",
    "title": "Random forests",
    "section": "Errors",
    "text": "Errors\nBut how accurate is the predictor?\n\n\n\n\n\n\nASD\nTD\nclass.error\n\n\n\n\nASD\n53\n23\n0.3026316\n\n\nTD\n19\n59\n0.2435897"
  },
  {
    "objectID": "pstat197a/slides/week3-randomforest.html#next-week",
    "href": "pstat197a/slides/week3-randomforest.html#next-week",
    "title": "Random forests",
    "section": "Next week",
    "text": "Next week\n\nlogistic regression\nvariable selection\na design view of the proteomic analysis"
  },
  {
    "objectID": "pstat197a/labs/lab4-logistic/lab4-logistic.html",
    "href": "pstat197a/labs/lab4-logistic/lab4-logistic.html",
    "title": "Measuring classification accuracy",
    "section": "",
    "text": "In class you saw how to fit a logistic regression model using glm() and some basic classification accuracy measures.\nObjectives\nIn this lab you’ll carry out a more rigorous quantification of predictive accuracy by data partitioning. You’ll learn to use:\nPrerequisites\nFollow the action item below to get set up for the lab. You may need to install one or more packages if the library() calls return an error.\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(modelr)\nlibrary(rsample)\nlibrary(yardstick)\n\n# read data\nurl &lt;- 'https://raw.githubusercontent.com/pstat197/pstat197a/main/materials/labs/lab4-logistic/data/biomarker_clean.csv'\n\ns_star &lt;- c(\"DERM\", \"RELT\", \"IgD\", \"PTN\", \"FSTL1\")\nbiomarker &lt;- read_csv(url) %&gt;%\n  # subset to proteins of interest and group\n  select(group, any_of(s_star)) %&gt;%\n  # convert group (chr) to binary (lgl)\n  mutate(class = (group == 'ASD')) %&gt;%\n  select(-group)"
  },
  {
    "objectID": "pstat197a/labs/lab4-logistic/lab4-logistic.html#data-partitioning",
    "href": "pstat197a/labs/lab4-logistic/lab4-logistic.html#data-partitioning",
    "title": "Measuring classification accuracy",
    "section": "Data partitioning",
    "text": "Data partitioning\nIn class we fit a logistic regression model to the data and evaluated classification accuracy on the very same data.\nBecause the parameter estimates optimize errors on the data used to fit the model, evaluating accuracy on the same data gives an overly optimistic assessment, because the errors have been made as small as possible.\nData partitioning consists in setting aside a random subset of observations that will be used only to assess predictive accuracy and not to fit any models. The held out data is treated as a ‘test’ set of observations to try to predict. This provides a more realistic assessment of predictive accuracy that is closer to what can be expected if genuinely new data is collected.\nPartitioning is easy to do using rsample::initial_split and specifying the proportion of data that should be retained for model fitting (the ‘training’ set).\nPartitions are computed at random, so for reproducibility it is necessary to set the RNG seed at a fixed value.\n\n\n\n\n\n\nAction\n\n\n\nPartition the biomarker data into training and test sets\nCopy-paste the code chunk below into your script and execute once.\nRemarks:\n\nset.seed() needs to be executed together with the lines that follow to ensure the same result is rendered every time\nthe output simply summarizes the partitions\n\n\n# for reproducibility\nset.seed(102022)\n\n# partition data\npartitions &lt;- biomarker %&gt;%\n  initial_split(prop = 0.8)\n\n# examine\npartitions\n\n&lt;Training/Testing/Total&gt;\n&lt;123/31/154&gt;\n\n\n\n\nTo retrieve the data partitions, one needs to use the helper functions training() and testing() :\n\n# training set\ntraining(partitions) %&gt;% head(4)\n\n# A tibble: 4 × 6\n    DERM   RELT    IgD     PTN  FSTL1 class\n   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;lgl&gt;\n1 -0.409  0.108  1.82  -0.457  -1.31  TRUE \n2  1.25   0.802 -0.461  0.910   0.973 FALSE\n3 -0.697 -0.921 -1.26   0.0444 -1.46  TRUE \n4 -0.591  1.34   2.31   0.269   0.802 TRUE \n\n# testing set\ntesting(partitions) %&gt;% head(4)\n\n# A tibble: 4 × 6\n      DERM   RELT    IgD     PTN  FSTL1 class\n     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;lgl&gt;\n1  0.140   -0.586 -1.53  -1.72   -1.96  TRUE \n2 -0.276    0.410  0.454 -0.0375 -1.41  TRUE \n3  0.927   -1.12   1.15  -0.290   0.190 TRUE \n4 -0.00743 -0.319 -0.762 -1.27   -1.05  TRUE"
  },
  {
    "objectID": "pstat197a/labs/lab4-logistic/lab4-logistic.html#model-fitting",
    "href": "pstat197a/labs/lab4-logistic/lab4-logistic.html#model-fitting",
    "title": "Measuring classification accuracy",
    "section": "Model fitting",
    "text": "Model fitting\nFitting a logistic regression model is, as you saw in class, a one-line affair:\n\n# fit glm\nfit &lt;- glm(class ~ ., \n           data = biomarker, \n           family = binomial(link = \"logit\"))\n\nThe glm() function can fit many kinds of generalized linear models. Logistic regression is just one of this class of models in which:\n\nthe response follows a binomial distribution (the Bernoulli distribution is the binomial with \\(n = 1\\))\nthe log-odds or logit transformation of the event/class probability \\(p_i\\) is linear in the predictors\n\nThe parameter estimates reported in tabular form are:\n\ntidy(fit)\n\n# A tibble: 6 × 5\n  term        estimate std.error statistic p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)  -0.0933     0.199    -0.468 0.640  \n2 DERM         -0.603      0.280    -2.16  0.0311 \n3 RELT         -0.438      0.286    -1.53  0.126  \n4 IgD          -0.662      0.213    -3.11  0.00189\n5 PTN          -0.234      0.273    -0.857 0.392  \n6 FSTL1        -0.360      0.259    -1.39  0.165  \n\n\n\n\n\n\n\n\nAction\n\n\n\nFit the model and interpret a parameter\n\nCopy-paste the code above in your script and execute to fit the logistic regression model.\nConfer with your neighbor and interpret one of the parameters of your choosing.\n\nBy interpret, we mean: what is the estimated change in P(ASD) associated with a +1SD change in log protein level?"
  },
  {
    "objectID": "pstat197a/labs/lab4-logistic/lab4-logistic.html#predictions",
    "href": "pstat197a/labs/lab4-logistic/lab4-logistic.html#predictions",
    "title": "Measuring classification accuracy",
    "section": "Predictions",
    "text": "Predictions\nThe modelr package makes it relatively easy to compute predictions for a wide range of model objects in R. Its pipe-friendly add_predictions(.df, .mdl, type) function will add a column of predictions of type type using model .mdl to data frame .df .\n\n# compute predictions on the test set\ntesting(partitions) %&gt;%\n  add_predictions(fit)\n\n# A tibble: 31 × 7\n       DERM    RELT    IgD     PTN  FSTL1 class    pred\n      &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;lgl&gt;   &lt;dbl&gt;\n 1  0.140   -0.586  -1.53  -1.72   -1.96  TRUE   2.20  \n 2 -0.276    0.410   0.454 -0.0375 -1.41  TRUE   0.108 \n 3  0.927   -1.12    1.15  -0.290   0.190 TRUE  -0.925 \n 4 -0.00743 -0.319  -0.762 -1.27   -1.05  TRUE   1.23  \n 5 -0.484   -0.612  -0.625 -0.990  -1.16  TRUE   1.53  \n 6  0.111   -0.489  -1.15   0.0483 -1.12  TRUE   1.21  \n 7 -0.972   -0.535   0.502 -0.594  -0.427 TRUE   0.687 \n 8 -0.344    0.0337  0.998 -2.00   -0.309 TRUE   0.0179\n 9 -1.13    -0.385  -0.929  0.892  -0.597 TRUE   1.38  \n10  0.398    0.325  -1.17  -0.0114 -0.415 TRUE   0.450 \n# … with 21 more rows\n\n\nInspect the pred column. Notice that the predictions are not classes or probabilities. The default type of predictions are log-odds. One could back-transform:\n\n# manually transform to probabilities\ntesting(partitions) %&gt;%\n  add_predictions(fit) %&gt;%\n  mutate(probs = 1/(1 + exp(-pred))) %&gt;%\n  select(class, pred, probs) %&gt;%\n  head(5)\n\n# A tibble: 5 × 3\n  class   pred probs\n  &lt;lgl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 TRUE   2.20  0.900\n2 TRUE   0.108 0.527\n3 TRUE  -0.925 0.284\n4 TRUE   1.23  0.774\n5 TRUE   1.53  0.822\n\n\nOr simply change the type of predictions to response in order to obtain predicted probabilities:\n\n# predict on scale of response\ntesting(partitions) %&gt;%\n  add_predictions(fit, type = 'response') %&gt;%\n  select(class, pred) %&gt;%\n  head(5)\n\n# A tibble: 5 × 2\n  class  pred\n  &lt;lgl&gt; &lt;dbl&gt;\n1 TRUE  0.900\n2 TRUE  0.527\n3 TRUE  0.284\n4 TRUE  0.774\n5 TRUE  0.822\n\n\nIf we want to convert these predicted class probabilities into predicted classes, we can simply define a new variable based on whether the probabilities exceed 0.5 (or any other threshold):\n\n# predict classes\ntesting(partitions) %&gt;%\n  add_predictions(fit, type = 'response') %&gt;%\n  mutate(pred.class = (pred &gt; 0.5)) %&gt;%\n  select(class, pred, pred.class) %&gt;%\n  head(5)\n\n# A tibble: 5 × 3\n  class  pred pred.class\n  &lt;lgl&gt; &lt;dbl&gt; &lt;lgl&gt;     \n1 TRUE  0.900 TRUE      \n2 TRUE  0.527 TRUE      \n3 TRUE  0.284 FALSE     \n4 TRUE  0.774 TRUE      \n5 TRUE  0.822 TRUE"
  },
  {
    "objectID": "pstat197a/labs/lab4-logistic/lab4-logistic.html#accuracy-measures",
    "href": "pstat197a/labs/lab4-logistic/lab4-logistic.html#accuracy-measures",
    "title": "Measuring classification accuracy",
    "section": "Accuracy measures",
    "text": "Accuracy measures\nThe classification accuracy measures we discussed in class are based on tabulating observation counts when grouping by predicted and observed classes.\nThis tabulation can be done in a base-R way by piping a data frame of the predicted and observed classes into table() :\n\n# tabulate\ntesting(partitions) %&gt;%\n  add_predictions(fit, type = 'response') %&gt;%\n  mutate(pred.class = (pred &gt; 0.5)) %&gt;%\n  select(class, pred.class) %&gt;%\n  table()\n\n       pred.class\nclass   FALSE TRUE\n  FALSE    10    6\n  TRUE      3   12\n\n\nHowever, the metrics discussed in class are somewhat painful to compute from the output above. Luckily, yardstick makes that process easier: the package has specialized functions that compute each metric. One need only:\n\nprovide the predicted and true labels as factors\nindicate which factor is the truth and which is the prediction\nindicate which factor level is considered a ‘positive’\n\n\n# store predictions as factors\npred_df &lt;- testing(partitions) %&gt;%\n  add_predictions(fit, type = 'response') %&gt;%\n  mutate(pred.class = (pred &gt; 0.5),\n         group = factor(class, labels = c('TD', 'ASD')),\n         pred.group = factor(pred.class, labels = c('TD', 'ASD'))) \n\n# check order of factor levels\npred_df %&gt;% pull(group) %&gt;% levels()\n\n[1] \"TD\"  \"ASD\"\n\n# compute specificity\npred_df %&gt;%\n  specificity(truth = group, \n              estimate = pred.group,\n              event_level = 'second')\n\n# A tibble: 1 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 specificity binary         0.625\n\n\nThe second level is ASD, which in this context is a positive. We knew since we supplied the labels in defining the factors, and the order of levels will match the order of labels. However, we can also check as above using levels() . Hence, event_level = 'second' .\nSensitivity, accuracy, and other metrics can be computed similarly:\n\n# sensitivity\npred_df %&gt;%\n  sensitivity(truth = group,\n              estimate = pred.group,\n              event_level = 'second')\n\n# A tibble: 1 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 sensitivity binary           0.8\n\n\nYou can check the package documentation for a complete list of available metrics.\n\n\n\n\n\n\nAction\n\n\n\nCompute the accuracy\nFind the appropriate function from the package documentation (link immediately above) and use it to compute the accuracy.\nRemark: from the table, you know the result should be \\(\\frac{10 + 12}{10 + 12 + 6 + 3} \\approx 0.7097\\) .\n\n\nThe package also has a helper function that allows you to define a panel of metrics so that you can compute several simultaneously. If we want a panel of specificity and sensitivity, the following will do the trick:\n\n# define panel (arguments must be yardstick metric function names)\npanel_fn &lt;- metric_set(sensitivity, specificity)\n\n# compute\npred_df %&gt;%\n  panel_fn(truth = group,\n           estimate = pred.group,\n           event_level = 'second')\n\n# A tibble: 2 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 sensitivity binary         0.8  \n2 specificity binary         0.625\n\n\n\n\n\n\n\n\nAction\n\n\n\nCompute a panel of precision, recall, and F1 score\n\nFind the appropriate yardstick functions and define a metric panel\nCompute on the test data\n\n\n\nAs a final comment, the table of classifications can be obtained in yardstick using the conf_mat() function. (The cross-classification table of predicted versus actual classes is called a confusion matrix.)\n\npred_df %&gt;% conf_mat(truth = group, estimate = pred.group)\n\n          Truth\nPrediction TD ASD\n       TD  10   3\n       ASD  6  12"
  },
  {
    "objectID": "pstat197a/labs/lab4-logistic/lab4-logistic.html#checklist",
    "href": "pstat197a/labs/lab4-logistic/lab4-logistic.html#checklist",
    "title": "Measuring classification accuracy",
    "section": "Checklist",
    "text": "Checklist\n\nYou partitioned the data into training and test sets\nYou fit a logistic regression model using the training set\nYou evaluated accuracy on the test set"
  },
  {
    "objectID": "pstat197a/labs/lab4-logistic/lab4-logistic.html#extras",
    "href": "pstat197a/labs/lab4-logistic/lab4-logistic.html#extras",
    "title": "Measuring classification accuracy",
    "section": "Extras",
    "text": "Extras\nIf there is extra time, or you’re interested in exploring a bit further on your own, read on.\n\nROC curves\nThe yardstick package also supplies functions for computing class probability metrics based on the estimated class probabilities rather than the estimated classes. ROC curves and AUROC are examples.\nroc_curve() will find all unique probability thresholds and, for each threshold, calculate sensitivity and specificity:\n\npred_df %&gt;%\n  roc_curve(truth = group, estimate = pred)\n\n# A tibble: 33 × 3\n   .threshold specificity sensitivity\n        &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n 1  -Inf           0            1    \n 2     0.0664      0            1    \n 3     0.117       0            0.938\n 4     0.158       0            0.875\n 5     0.186       0.0667       0.875\n 6     0.191       0.0667       0.812\n 7     0.197       0.0667       0.75 \n 8     0.262       0.0667       0.688\n 9     0.284       0.0667       0.625\n10     0.299       0.133        0.625\n# … with 23 more rows\n\n\nThis can be used to plot the ROC curve:\n\npred_df %&gt;%\n  roc_curve(truth = group, \n            estimate = pred,\n            event_level = 'second') %&gt;%\n  ggplot(aes(y = sensitivity, x = 1 - specificity)) +\n  geom_path() +\n  geom_abline(slope = 1, intercept = 0)\n\n\n\n\nIn this case the ROC curve is so choppy because the test set only includes 31 observations. In general, for a collection of \\(n\\) observations there are at most \\(n + 1\\) unique thresholds and usually considerably fewer.\nThe area under the ROC curve is also easy to compute:\n\npred_df %&gt;% roc_auc(truth = group, \n                    estimate = pred,\n                    event_level = 'second')\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.712\n\n\n\n\nCombined metric types\nIf you wish to compute both classification metrics based on a class prediction and class probability metrics based on a probability prediction in a metric panel, the classification should be provided as the argument to estimate = ... and class probability column can be provided as an unnamed argument following the estimate argument.\nFor example:\n\npanel &lt;- metric_set(roc_auc, accuracy) \n\npred_df %&gt;% panel(truth = group,\n                  estimate = pred.group,\n                  pred,\n                  event_level = 'second')\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.710\n2 roc_auc  binary         0.712\n\n\n\n\nExploring partitions\nReview this section if you want a deeper understanding of data partitioning.\nThe rationale for partitioning is that held out data will give a more honest assessment. Conversely, evaluating accuracy on data used to fit a model will provide an overly optimistic assessment.\nWe can experimentally confirm this intuition by:\n\nrepeatedly partitioning the data at random\nevaluating accuracy on both partitions\naveraging across partitions\n\nThis procedure will reveal that on average the accuracy is better on training data. Don’t worry too much about the computations; focus on the output and the concepts.\nWe’ll use the tidyverse iteration strategy from lab 3. First we’ll need some helper functions that are basically wrappers around each step we went through in this lab:\n\nfitting a model\nadding predictions\nevaluating metrics\n\n\n# define some helper functions\nfit_fn &lt;- function(.df){\n  glm(class ~ ., family = 'binomial', data = .df)\n}\n\npred_fn &lt;- function(.df, .mdl){\n  .df %&gt;% add_predictions(.mdl, type = 'response')\n}\n\npanel &lt;- metric_set(sensitivity, specificity, accuracy, roc_auc)\n\neval_fn &lt;- function(.df){\n  .df %&gt;%\n    mutate(group = factor(class, labels = c('TD', 'ASD')),\n           pred.group = factor(pred &gt; 0.5, labels = c('TD', 'ASD'))) %&gt;%\n    panel(truth = group,\n          estimate = pred.group,\n          pred,\n          event_level = 'second')\n  \n}\n\nNow let’s create 400 random partitions of the data and perform the steps in this lab for every partition. In addition, we’ll compute predictions and evaluate accuracy on the training set, which we did not do above and is generally not done.\n\nset.seed(101922)\nn_splits &lt;- 400\nout &lt;- tibble(partition = 1:n_splits,\n       split = map(partition, ~ initial_split(biomarker, prop = 0.8)),\n       train = map(split, training),\n       test = map(split, testing),\n       fit = map(train, fit_fn),\n       pred_test = map2(test, fit, pred_fn),\n       pred_train = map2(train, fit, pred_fn),\n       eval_test = map(pred_test, eval_fn),\n       eval_train = map(pred_train, eval_fn))\n\nout %&gt;% head(4)\n\n# A tibble: 4 × 9\n  partition split            train    test     fit    pred_test pred_train\n      &lt;int&gt; &lt;list&gt;           &lt;list&gt;   &lt;list&gt;   &lt;list&gt; &lt;list&gt;    &lt;list&gt;    \n1         1 &lt;split [123/31]&gt; &lt;tibble&gt; &lt;tibble&gt; &lt;glm&gt;  &lt;tibble&gt;  &lt;tibble&gt;  \n2         2 &lt;split [123/31]&gt; &lt;tibble&gt; &lt;tibble&gt; &lt;glm&gt;  &lt;tibble&gt;  &lt;tibble&gt;  \n3         3 &lt;split [123/31]&gt; &lt;tibble&gt; &lt;tibble&gt; &lt;glm&gt;  &lt;tibble&gt;  &lt;tibble&gt;  \n4         4 &lt;split [123/31]&gt; &lt;tibble&gt; &lt;tibble&gt; &lt;glm&gt;  &lt;tibble&gt;  &lt;tibble&gt;  \n# … with 2 more variables: eval_test &lt;list&gt;, eval_train &lt;list&gt;\n\n\nWe can extract the accuracy of predictions on each of the training and test sets as follows:\n\ntest_accuracy &lt;- out %&gt;% \n  select(partition, contains('eval')) %&gt;%\n  unnest(eval_test) %&gt;%\n  select(partition, .metric, .estimate) %&gt;%\n  pivot_wider(names_from = .metric, values_from = .estimate)\n\ntrain_accuracy &lt;- out %&gt;% \n  select(partition, contains('eval')) %&gt;%\n  unnest(eval_train) %&gt;%\n  select(partition, .metric, .estimate) %&gt;%\n  pivot_wider(names_from = .metric, values_from = .estimate)\n\ntest_accuracy %&gt;% head(4)\n\n# A tibble: 4 × 5\n  partition sensitivity specificity accuracy roc_auc\n      &lt;int&gt;       &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1         1       0.6         0.562    0.581   0.675\n2         2       0.778       0.538    0.677   0.645\n3         3       0.684       0.917    0.774   0.846\n4         4       0.684       0.833    0.742   0.842\n\ntrain_accuracy %&gt;% head(4)\n\n# A tibble: 4 × 5\n  partition sensitivity specificity accuracy roc_auc\n      &lt;int&gt;       &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1         1       0.803       0.806    0.805   0.858\n2         2       0.741       0.831    0.789   0.858\n3         3       0.737       0.742    0.740   0.825\n4         4       0.719       0.773    0.748   0.825\n\n\nLastly, we can average the metrics over all partitions and also check the variability across partitions. We’ll start with the training set:\n\ntrain_summaries &lt;- train_accuracy %&gt;%\n  rename(roc.auc = roc_auc) %&gt;%\n  select(-partition) %&gt;%\n  summarize_all(.funs = list(mean = mean, sd = sd)) %&gt;%\n  gather() %&gt;%\n  separate(key, into = c('metric', 'stat'), sep = '_') %&gt;%\n  spread(stat, value)\n\nNow compute the average and variability on the test set:\n\ntest_summaries &lt;- test_accuracy %&gt;%\n  rename(roc.auc = roc_auc) %&gt;%\n  select(-partition) %&gt;%\n  summarize_all(.funs = list(mean = mean, sd = sd)) %&gt;%\n  gather() %&gt;%\n  separate(key, into = c('metric', 'stat'), sep = '_') %&gt;%\n  spread(stat, value)\n\nNow let’s put them side-by-side:\n\nleft_join(train_summaries, \n          test_summaries,\n          by = 'metric',\n          suffix = c('.train', '.test')) %&gt;%\n  select(metric, contains('mean'), contains('sd')) %&gt;%\n  knitr::kable()\n\n\n\n\nmetric\nmean.train\nmean.test\nsd.train\nsd.test\n\n\n\n\naccuracy\n0.7573374\n0.7225806\n0.0207045\n0.0702816\n\n\nroc.auc\n0.8348238\n0.7994956\n0.0158188\n0.0706822\n\n\nsensitivity\n0.7565383\n0.7093282\n0.0308425\n0.1073645\n\n\nspecificity\n0.7569019\n0.7416470\n0.0282898\n0.1148716\n\n\n\n\n\nNotice that:\n\nThe apparent accuracy according to all metrics is higher on average on the training data across partitionings\nThe accuracy metrics on the test data are more variable across partitionings\n\nThis behavior is the justification for data partitioning: evaluating predictions on the same data that was used to fit the model overestimates the accuracy compared with data that was not used in fitting."
  },
  {
    "objectID": "pstat197a/labs/lab5-text/lab5-text.html",
    "href": "pstat197a/labs/lab5-text/lab5-text.html",
    "title": "Text processing",
    "section": "",
    "text": "In this lab you’ll learn some basic text processing following what was presented in class and do a little exploratory analysis using token frequency measures.\nObjectives\n# setup\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(tokenizers)\nlibrary(textstem)\nlibrary(stopwords)\nurl &lt;- 'https://raw.githubusercontent.com/pstat197/pstat197a/main/materials/labs/lab5-text/data/drseuss.txt'\n\n# read data\nseuss_lines &lt;- read_lines(url, skip_empty_rows = T)\nThe text we’ll work with comprises four Dr. Seuss books. The raw data are read in line-by-line, so that seuss_lines is a vector in which each element is a line from one of the four books. Lines are rendered in order.\nseuss_lines %&gt;% head()\n\n[1] \"The Cat in the Hat\"            \"By Dr. Seuss\"                 \n[3] \"The sun did not shine.\"        \"It was too wet to play.\"      \n[5] \"So we sat in the house\"        \"All that cold, cold, wet day.\""
  },
  {
    "objectID": "pstat197a/labs/lab5-text/lab5-text.html#text-preprocessing",
    "href": "pstat197a/labs/lab5-text/lab5-text.html#text-preprocessing",
    "title": "Text processing",
    "section": "Text preprocessing",
    "text": "Text preprocessing\nFor us, ‘preprocessing’ operations will refer to coercing a document into one long uniformly-formatted string.\n\nDistinguishing documents\nTo start, we have all four books lumped together. A quick visual scan of the text file will confirm that each book is set off by the title on one line followed by ‘By Dr. Seuss’ on the next line.\nWe can leverage this structure to distinguish the books: the chunk below\n\ncreates a ‘flag’ by pattern-matching each line with Dr. Seuss,\nthen shifts the lines down by one so that the flag matches the title line instead of the author line\nthen assigns a document ID to each line by computing the total number of flags in all preceding lines.\n\nThe last two commands correct for having ‘lagged’ the lines.\n\n# flag lines with a document id\nseuss_lines_df &lt;- tibble(line_lag = c(seuss_lines, NA)) %&gt;%\n  mutate(flag = str_detect(line_lag, 'Dr. Seuss'),\n         line = lag(line_lag, n = 1),\n         doc = cumsum(flag)) %&gt;% \n  select(doc, line) %&gt;%\n  slice(-1) %&gt;%\n  fill(doc)\n\nWe may as well assign labels to the document IDs.\n\n# grab titles\ntitles &lt;- seuss_lines_df %&gt;% \n  group_by(doc) %&gt;%\n  slice_head() %&gt;%\n  pull(line) %&gt;%\n  tolower()\n\n# label docs\nseuss_lines_df &lt;- seuss_lines_df %&gt;%\n  mutate(doc = factor(doc, labels = titles))\n\nFinally, we’ll strip the title and author information, because all books are by the same author and the title is now recorded in the document ID.\nThe chunk below adds a document-specific line number and removes the first two lines from every document. Since each row is a line, this amounts to a simple row numbering and filtering.\n\n# remove header lines (title/author)\nseuss_lines_clean &lt;- seuss_lines_df %&gt;%\n  group_by(doc) %&gt;%\n  mutate(line_num = row_number() - 2) %&gt;%\n  filter(line_num &gt; 0)\n\n\n\n\n\n\n\nAction\n\n\n\nLine summaries\nSee if you can answer the following questions:\n\nHow many lines are in each book?\nHow many lines in each book contain the word ‘bump’?\n\nWork with a neighbor. Hint: you might find it handy to use str_detect() , and grouped operations and/or summaries.\n\n\n\n\nCollapsing lines and cleaning text\nFirst, concatenate all the lines using str_c() .\n\n# collapse lines into one long string\nseuss_text &lt;- seuss_lines_clean %&gt;% \n  summarize(text = str_c(line, collapse = ' '))\n\nIn this case the resulting text strings for each document don’t contain too many elements in need of removal: just punctuation and capital letters.\n\ncat_in_hat &lt;- seuss_text %&gt;% slice(1) %&gt;% pull(text)\n\nTo strip these elements, we can exclude matching patterns from the collection of punctuation marks and then use tolower() to replace upper-case letters with lower-case letters. Shorthand for punctuation in stringr is '[[:punct:]]' .\n\ncat_in_hat %&gt;%\n  str_remove_all('[[:punct:]]') %&gt;%\n  tolower()\n\n[1] \"the sun did not shine it was too wet to play so we sat in the house all that cold cold wet day i sat there with sally we sat there we two and i said how i wish we had something to do too wet to go out and too cold to play ball so we sat in the house we did nothing at all so all we could do was to sit sit sit sit and we did not like it not one little bit bump and then something went bump how that bump made us jump we looked then we saw him step in on the mat we looked and we saw him the cat in the hat and he said to us why do you sit there like that i know it is wet and the sun is not sunny but we can have lots of good fun that is funny i know some good games we could play said the cat i know some new tricks said the cat in the hat a lot of good tricks i will show them to you your mother will not mind at all if i do then sally and i did not know what to say our mother was out of the house for the day but our fish said no no make that cat go away tell that cat in the hat you do not want to play he should not be here he should not be about he should not be here when your mother is out now now have no fear have no fear said the cat my tricks are not bad said the cat in the hat why we can have lots of good fun if you wish with a game that i call upupup with a fish put me down said the fish this is no fun at all put me down said the fish i do not wish to fall have no fear said the cat i will not let you fall i will hold you up high as i stand on a ball with a book on one hand and a cup on my hat but that is not all i can do said the cat look at me look at me now said the cat with a cup and a cake on the top of my hat i can hold up two books i can hold up the fish and a litte toy ship and some milk on a dish and look i can hop up and down on the ball but that is not all oh no that is not all look at me look at me look at me now it is fun to have fun but you have to know how i can hold up the cup and the milk and the cake i can hold up these books and the fish on a rake i can hold the toy ship and a little toy man and look with my tail i can hold a red fan i can fan with the fan as i hop on the ball but that is not all oh no that is not all that is what the cat said then he fell on his head he came down with a bump from up there on the ball and sally and i we saw all the things fall and our fish came down too he fell into a pot he said do i like this oh no i do not this is not a good game said our fish as he lit no i do not like it not one little bit now look what you did said the fish to the cat now look at this house look at this look at that you sank our toy ship sank it deep in the cake you shook up our house and you bent our new rake you should not be here when our mother is not you get out of this house said the fish in the pot but i like to be here oh i like it a lot said the cat in the hat to the fish in the pot i will not go away i do not wish to go and so said the cat in the hat so so so i will show you another good game that i know and then he ran out and then fast as a fox the cat in the hat came back in with a box a big red wood box it was shut with a hook now look at this trick said the cat take a look then he got up on top with a tip of his hat i call this game funinabox said the cat in this box are two things i will show to you now you will like these two things said the cat with a bow i will pick up the hook you will see something new two things and i call them thing one and thing two these things will not bite you they want to have fun then out of the box came thing two and thing one and they ran to us fast they said how do you do would you like to shake hands with thing one and thing two and sally and i did not know what to do so we had to shake hands with thing one and thing two we shook their two hands but our fish said no no those things should not be in this house make them go they should not be here when your mother is not put them out put them out said the fish in the pot have no fear little fish said the cat in the hat these things are good things and he gave them a pat they are tame oh so tame they have come here to play they will give you some fun on this wet wet wet day now here is a game that they like said the cat they like to fly kites said the cat in the hat no not in the house said the fish in the pot they should not fly kites in a house they should not oh the things they will bump oh the things they will hit oh i do not like it not one little bit then sally and i saw them run down the hall we saw those two things bump their kites on the wall bump thump thump bump down the wall in the hall thing two and thing one they ran up they ran down on the string of one kite we saw mothers new gown her gown with the dots that are pink white and red then we saw one kite bump on the head of her bed then those things ran about with big bumps jumps and kicks and with hops and big thumps and all kinds of bad tricks and i said i do not like the way that they play if mother could see this oh what would she say then our fish said look look and our fish shook with fear your mother is on her way home do you hear oh what will she do to us what will she say oh she will not like it to find us this way so do something fast said the fish do you hear i saw her your mother your mother is near so as fast as you can think of something to do you will have to get rid of thing one and thing two so as fast as i could i went after my net and i said with my net i can get them i bet i bet with my net i can get those things yet then i let down my net it came down with a plop and i had them at last thoe two things had to stop then i said to the cat now you do as i say you pack up those things and you take them away oh dear said the cat you did not like our game oh dear what a shame what a shame what a shame then he shut up the things in the box with the hook and the cat went away with a sad kind of look that is good said the fish he has gone away yes but your mother will come she will find this big mess and this mess is so big and so deep and so tall we ca not pick it up there is no way at all and then who was back in the house why the cat have no fear of this mess said the cat in the hat i always pick up all my playthings and so i will show you another good trick that i know then we saw him pick up all the things that were down he picked up the cake and the rake and the gown and the milk and the strings and the books and the dish and the fan and the cup and the ship and the fish and he put them away then he said that is that and then he was gone with a tip of his hat then our mother came in and she said to us two did you have any fun tell me what did you do and sally and i did not know what to say should we tell her the things that went on there that day should we tell her about it now what should we do well what would you do if your mother asked you\"\n\n\nTo apply this to all four texts, simply create a function wrapper for the processing commands and then use dplyr to pass the text through the processing function.\n\nclean_fn &lt;- function(.text){\n  str_remove_all(.text, '[[:punct:]]') %&gt;% tolower()\n}\n\nseuss_text_clean &lt;- seuss_text %&gt;%\n  mutate(text = clean_fn(text))\n\nYou could also create a manual list of punctuation to remove.\n\n\n\n\n\n\nAction\n\n\n\nThe regular expression for matching a or b is a | b . Write an alternative to the previous code chunk that lists the punctuation to remove explicitly and does not use '[[:punct:]]' ."
  },
  {
    "objectID": "pstat197a/labs/lab5-text/lab5-text.html#basic-nlp",
    "href": "pstat197a/labs/lab5-text/lab5-text.html#basic-nlp",
    "title": "Text processing",
    "section": "Basic NLP",
    "text": "Basic NLP\nAs you saw in class, once we have a string of clean text for each document, tokenization and lemmatization are largely automated.\n\nTokenization\nunnest_tokens() will tokenize and return the result in tidy format; lemmatize_words() can be applied to the resulting column of tokens using dplyr commands.\n\nstpwrd &lt;- stop_words %&gt;%\n  pull(word) %&gt;%\n  str_remove_all('[[:punct:]]')\n\nseuss_tokens_long &lt;- seuss_text_clean %&gt;%\n  unnest_tokens(output = token, # specifies new column name\n                input = text, # specifies column containing text\n                token = 'words', # how to tokenize\n                stopwords = stpwrd) %&gt;% # optional stopword removal\n  mutate(token = lemmatize_words(token)) \n\n\n\n\n\n\n\nAction\n\n\n\nBased on the data frame above, use row counting (count() ) to answer the following questions:\n\nWhat’s the most frequently used word in each book?\nWhat’s the most frequently used word in all books?\n\nCompare with your neighbor to check your answers.\nIf there’s time: refer to the documentation ?unnest_tokens to determine how to tokenize as bigrams. Find the most frequent bigrams in each book.\n\n\n\n\nFrequency measures\nThe frequency measures discussed in class – term frequency (TF), inverse document frequency (IDF), and their product (TF-IDF) – can be computed from token counts using tidytext::bind_tf_idf() .\n\nseuss_tfidf &lt;- seuss_tokens_long %&gt;%\n  count(doc, token) %&gt;%\n  bind_tf_idf(term = token,\n              document = doc,\n              n = n) \n\nseuss_df &lt;- seuss_tfidf %&gt;%\n  pivot_wider(id_cols = doc, \n              names_from = token,\n              values_from = tf_idf,\n              values_fill = 0)\n\nseuss_df\n\n# A tibble: 4 × 246\n  doc         bad    ball     bed    bend     bet    bite   book     bow     box\n  &lt;fct&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 \"the c… 0.00396 0.00990 0.00198 0.00198 0.00792 0.00792 0.0158 0.00396 0.00411\n2 \"fox i… 0       0       0       0.00891 0       0       0      0       0.00431\n3 \"green… 0       0       0       0       0       0       0      0       0.0139 \n4 \"hop o… 0.00788 0.0118  0.0118  0       0       0.00394 0      0       0      \n# … with 236 more variables: bump &lt;dbl&gt;, ca &lt;dbl&gt;, cake &lt;dbl&gt;, call &lt;dbl&gt;,\n#   cat &lt;dbl&gt;, cold &lt;dbl&gt;, cup &lt;dbl&gt;, day &lt;dbl&gt;, dear &lt;dbl&gt;, deep &lt;dbl&gt;,\n#   dish &lt;dbl&gt;, dot &lt;dbl&gt;, fall &lt;dbl&gt;, fan &lt;dbl&gt;, fast &lt;dbl&gt;, fear &lt;dbl&gt;,\n#   fish &lt;dbl&gt;, fly &lt;dbl&gt;, fox &lt;dbl&gt;, fun &lt;dbl&gt;, funinabox &lt;dbl&gt;, funny &lt;dbl&gt;,\n#   game &lt;dbl&gt;, gown &lt;dbl&gt;, hall &lt;dbl&gt;, hand &lt;dbl&gt;, hat &lt;dbl&gt;, head &lt;dbl&gt;,\n#   hear &lt;dbl&gt;, hit &lt;dbl&gt;, hold &lt;dbl&gt;, home &lt;dbl&gt;, hook &lt;dbl&gt;, hop &lt;dbl&gt;,\n#   house &lt;dbl&gt;, jump &lt;dbl&gt;, kick &lt;dbl&gt;, kind &lt;dbl&gt;, kite &lt;dbl&gt;, light &lt;dbl&gt;, …\n\n\nWe can use this data to compute a variety of summaries of the text. For example, the two words that distinguish each book most from the other books are, by book:\n\nseuss_tfidf %&gt;%\n  group_by(doc) %&gt;%\n  slice_max(tf_idf, n = 2)\n\n# A tibble: 10 × 6\n# Groups:   doc [4]\n   doc                  token      n     tf   idf tf_idf\n   &lt;fct&gt;                &lt;chr&gt;  &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1 \"the cat in the hat\" cat       26 0.0743 0.693 0.0515\n 2 \"the cat in the hat\" fish      20 0.0571 0.693 0.0396\n 3 \"fox in socks \"      sir       37 0.0792 1.39  0.110 \n 4 \"fox in socks \"      sock      19 0.0407 1.39  0.0564\n 5 \"green eggs and ham\" samiam    13 0.0897 1.39  0.124 \n 6 \"green eggs and ham\" egg       10 0.0690 1.39  0.0956\n 7 \"green eggs and ham\" green     10 0.0690 1.39  0.0956\n 8 \"green eggs and ham\" ham       10 0.0690 1.39  0.0956\n 9 \"hop on pop\"         brown     10 0.0568 1.39  0.0788\n10 \"hop on pop\"         pup        8 0.0455 1.39  0.0630\n\n\nBut the two most common words in each book are:\n\nseuss_tfidf %&gt;%\n  group_by(doc) %&gt;%\n  slice_max(tf, n = 2)\n\n# A tibble: 8 × 6\n# Groups:   doc [4]\n  doc                  token      n     tf   idf tf_idf\n  &lt;fct&gt;                &lt;chr&gt;  &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 \"the cat in the hat\" cat       26 0.0743 0.693 0.0515\n2 \"the cat in the hat\" fish      20 0.0571 0.693 0.0396\n3 \"fox in socks \"      sir       37 0.0792 1.39  0.110 \n4 \"fox in socks \"      sock      19 0.0407 1.39  0.0564\n5 \"green eggs and ham\" eat       24 0.166  0.288 0.0476\n6 \"green eggs and ham\" samiam    13 0.0897 1.39  0.124 \n7 \"hop on pop\"         brown     10 0.0568 1.39  0.0788\n8 \"hop on pop\"         pat       10 0.0568 0.693 0.0394\n\n\n\n\n\n\n\n\nAction\n\n\n\nDiscuss with your neighbor how you might determine how ‘different’ any two books are using an appropriate frequency measure and comparison between rows.\n\nCompute your difference measure for all pairs of books. Which pair is most distinct?\nUse the same idea to compute difference from the ‘average’ Dr. Seuss book. Which book is most different from the rest?"
  },
  {
    "objectID": "pstat197a/labs/lab7-curvefitting/lab7-curvefitting.html",
    "href": "pstat197a/labs/lab7-curvefitting/lab7-curvefitting.html",
    "title": "Curve fitting",
    "section": "",
    "text": "The objective of this lab is to:\n\nreplicate the estimation of seasonal trend in the soil data as shown in class;\nexplore curve-fitting methods in greater depth.\n\n\n\n\n\n\n\nAction\n\n\n\nSetup: open RStudio, create a new script for this lab, and copy-paste the code chunk below into the first several lines of the script. Execute once to load and preview the data.\n\n\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(modelr)\nlibrary(fda)\n\nurl &lt;- 'https://raw.githubusercontent.com/pstat197/pstat197a/main/materials/labs/lab7-curvefitting/data/soiltemp.csv'\n\nsoil &lt;- read_csv(url)\nsoil %&gt;% head()\n\n# A tibble: 6 × 6\n  site    year   day date        elev  temp\n  &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 B21K-1  2017   226 2017-08-14    96  3.82\n2 B21K-1  2017   227 2017-08-15    96  3.72\n3 B21K-1  2017   228 2017-08-16    96  3.31\n4 B21K-1  2017   229 2017-08-17    96  4.95\n5 B21K-1  2017   230 2017-08-18    96  5.46\n6 B21K-1  2017   231 2017-08-19    96  5.80\n\n\nOur overall objective is to model temp as a function of day. As discussed in class, this amounts to estimating an unknown function \\(f(\\text{day})\\) in the model:\n\\[\n\\text{temp}_i = f(\\text{day}_i) + \\text{error}_i\n\\]\n(Here we’re using \\(i\\) to index observations and not sites – so we will use 16,799 observations to estimate \\(f\\). Note that there are only 57 unique sites.)\nAlthough visually it makes more sense to plot paths by site, since we’re pooling all the observations together to estimate \\(f\\) we’re essentially treating the data as a collection of unrelated points. We’ll make use of the following visualization throughout to underscore this point.\n\n# scatterplot of temperatures against day of year\ntemp_pts &lt;- soil %&gt;%\n  ggplot(aes(x = day, y = temp)) +\n  geom_point(alpha = 0.1)\n\ntemp_pts\n\n\n\n\n\nLOESS curves\nIt’s worth being aware that ggplot has a built-in ability to compute local nonparametric conditional mean estimates using LOESS (LOcally Estimated Scatterplot Smoothing). This can often be a quick-and-dirty strategy for visualizing trends; it is less often (but sometimes!) used as a model for prediction and the like.\n\ntemp_pts + \n  geom_smooth(formula = 'y ~ x',\n              method = 'loess',\n              span = 0.5,\n              se = F)\n\n\n\n\nThis technique consists in computing a weighted average temp for every value of day that up-weights nearby points.\nThe span parameter controls the size of the neighborhood that contributes meaningfully to the weighted average – a smaller span, and thus a smaller neighborhood, produces a more erratic curve.\n\n\n\n\n\n\nAction\n\n\n\nTry changing the smoothing span. Note that the units for span are standard deviations, e.g., span = 1 says adjust the smoothing neighborhood in proportion to \\(\\pm 1 \\text{SD}\\) .\n\nPick a larger value.\nPick a smaller value.\nFind a value that you think captures the trend best.\n\n\n\nThe LOESS curve is fully nonparametric, meaning no assumptions are made about the form of \\(f\\).\nThis is an advantage in some senses – it’s a model-free method – but a disadvantage in others – it doesn’t provide any meaningful data reduction since the curve is just a bunch of averages.\n\n\nPolynomial regression\nIf we want an estimate that does yield some data reduction, a straightforward approach might be to fit a polynomial in day .\nThis can also be done through ggplot using a regression smooth (geom_smooth(..., method = 'lm', ...)). The following superimposes a quadratic fit (order 2 polynomial) to the data:\n\n# quadratic fit\ntemp_pts + \n  geom_smooth(formula = 'y ~ poly(x, 2)',\n              method = 'lm')\n\n\n\n\nAs an aside, poly is a handy R function that will compute polynomial terms in is argument, as illustrated below. (The argument raw = T ensures that ‘raw’ polynomials are returned rather than orthogonal polynomials. Orthogonal polynomials are the Gram-Schmidt orthognalization of the matrix of raw polynomials.)\n\npoly(1:5, degree = 3, raw = T, simple = T)\n\n     1  2   3\n[1,] 1  1   1\n[2,] 2  4   8\n[3,] 3  9  27\n[4,] 4 16  64\n[5,] 5 25 125\n\n\nSo the quadratic model is:\n\\[\n\\text{temp}_i = \\beta_0 + \\beta_1 \\text{day}_i + \\beta_1 \\text{day}^2_i + \\epsilon_i\n\\]\nThis could be fit explicitly as a regression model:\n\n# fit a polynomial model\nfit_poly &lt;- lm(temp ~ poly(day, degree = 2, raw = T),\n               data = soil)\n\n# compute predictions\npred_df &lt;- tibble(day = 1:365) %&gt;%\n  add_predictions(fit_poly)\n\n# visualize\ntemp_pts + \n  geom_path(data = pred_df, \n            aes(y = pred),\n            color = 'blue')\n\n\n\n\n\n\n\n\n\n\nAction\n\n\n\nExperiment with the polynomial degree and fit a model that you think best approximates the curve.\nSuggestion: see what happens if you overfit by choosing a polynomial of unrealistically large order, e.g., a polynomial of degree 25.\n\n\n\n\nSpline regression\nSpline regression refers to fitting piecewise models constrained to join together at the “knot” points that divide the pieces.\nThis can be accomplished by some creative manipulation of the model formula with indicator variables. An example is shown below of a linear spline with one knot at day 200. It’s not so important that you understand the formula manipulation as much as the idea.\n\n# linear spline with a knot at day 200\nfit_spline &lt;- lm(temp ~ day + I((day - 200)*(day &gt; 200)) - 1,\n                 data = soil)\n\n# compute predictions\npred_df &lt;- tibble(day = 1:365) %&gt;%\n  add_predictions(fit_spline)\n\n# plot it\ntemp_pts + \n  geom_path(data = pred_df, \n            aes(y = pred),\n            color = 'blue')\n\n\n\n\nThe formulae would get a little more complicated, but the principle would be the same for (a) introducing additional knots and/or (b) fitting polynomials to each segment.\n\n\nSpline basis (a.k.a. “b-spline”)\nA better solution than working out the formula each time is to use a basis expansion for the set of all possible regression splines of a given order.\nThis is what we discussed in class, and is illustrated in the code chunk below. The bs() function works similarly to poly() in the sense that it returns a matrix of values of the basis functions (cf. polynomial functions) for each value of an input vector.\n\n# define knot points\nknotpts &lt;- c(100, 200, 300)\n\n# fit an order 3 regression spline with three internal knots\nfit_bs &lt;- lm(temp ~ bs(day, degree = 1, knots = knotpts),\n             data = soil) \n\n# compute predictions\npred_df &lt;- tibble(day = 1:365) %&gt;%\n  add_predictions(fit_bs)\n\n# plot it\ntemp_pts + \n  geom_path(data = pred_df, \n            aes(y = pred),\n            color = 'blue') +\n  geom_vline(xintercept = knotpts, \n             linetype = 'dashed')\n\n\n\n\n\n\n\n\n\n\nAction\n\n\n\n\nExperiment with the polynomial order and the placement of knot points.\nFind a fitted curve that you feel captures the pattern well.\nCompare with your neighbor. What did you choose similarly? What did you choose differently?\n\n\n\nFor the curious: you can also inspect the basis functions used to generate the regression spline and their dependence on the knot placement using the code chunk below.\n\n# define knots\nknotpts &lt;- c(100, 200, 300)\n\n# input variable\nx &lt;- 1:365\n\n# calculate basis expansion and plot it\nbs(x, knots = knotpts, degree = 3) %&gt;%\n  as_tibble() %&gt;%\n  bind_cols(x = x) %&gt;%\n  pivot_longer(-x, names_to = 'basis') %&gt;%\n  ggplot(aes(x = x, y = value)) +\n  geom_path(aes(group = basis, color = basis)) +\n  geom_vline(xintercept = knotpts, linetype = 'dashed')\n\n\n\nFourier basis\nLastly, it was noted in class that spline regression does not produce a harmonic function and so is not well-suited to approximating a cyclical trend. Using a Fourier basis solves this problem.\nThe Fourier basis functions (expressed as real-valued functions) are the sequence of sine and cosine pairs characterized by\n\\[\n\\phi_{2n}(x) = k\\cos\\left(\\frac{2\\pi nx}{\\lambda}\\right)\n\\quad\\text{and}\\quad\n\\phi_{2n + 1}(x) = k\\sin\\left(\\frac{2 \\pi n x}{\\lambda}\\right) \\qquad\nn = 1, 2, \\dots\n\\]\nwhere \\(k\\) is a scaling factor. The first basis function is a constant \\(\\phi_1 (x) = c\\).\nThis basis can be used to approximate any continuous function. One need only choose a number of pairs to employ; by default, the initial wavelength \\(\\lambda\\) is set to the span of the data.\nA nice consequence for us in the current context is that the resulting estimate, as a linear combination of harmonic functions, is itself harmonic.\n\n# fit the model with the fourier basis expansion\nfit_fbs &lt;- lm(temp ~ fourier(day, nbasis = 4, period = 365) - 1,\n              data = soil)\n\n# compute predictions\npred_df &lt;- tibble(day = 1:365) %&gt;%\n  add_predictions(fit_fbs)\n\n# plot it\ntemp_pts + \n  geom_path(data = pred_df, \n            aes(y = pred),\n            color = 'blue')\n\n\n\n\nThe model is fit without an intercept (temp ~ ... - 1 ) because fourier(...) returns a constant term.\nNotice also that there is no need to specify knots in this case. The Fourier basis is generated using the period specified – each additional pair of sine and cosine basis functions simply halves the frequency of the previous pair.\n\n\n\n\n\n\nAction\n\n\n\n\nExperiment with the number of bases (note nbasis must be an even number).\nUsing a single pair of basis functions, plot estimates of the mean temperature over a period of 2 years. What happens when the input variable is outside the interval [1, 365]?\nMake a time course plot of the temperature data with the estimated mean from the fit using the Fourier basis expansion overlaid."
  },
  {
    "objectID": "pstat197a/labs/lab6-nn/lab6-nn.html",
    "href": "pstat197a/labs/lab6-nn/lab6-nn.html",
    "title": "Training neural networks",
    "section": "",
    "text": "Please complete this activity before arriving at your section meeting. It should only take you about 15 minutes.\n\n\nWe will use packages that interface to python from R to train neural networks in this lab. For this a current python installation is needed.\nSelect the download appropriate for your operating system here and follow installation instructions.\n\n\n\nOpen RStudio and execute the following commands in the console. You should have already installed keras with other packages in the first lab; however, if library(keras) returns an error execute install.packages('keras') and then try again. This will install Tensorflow (for python) in a manner suitable for use in R, along with a few other packages.\n\nlibrary(keras)\ninstall_keras()\n\nTo confirm the installation worked, try:\n\nlibrary(tensorflow)\ntf$constant('Hello world')\n\ntf.Tensor(b'Hello world', shape=(), dtype=string)\n\n\nYou may see a long message related to CUDA libraries in addition to the output shown above, but if you see this output at the end, your installation was successful.\nIf you did not see the expected output, try configuring a virtual environment for the installation explicitly as shown here.\nIf you are unable to troubleshoot after a short period of time, partner with a classmate for the lab activity and then ask for help from course staff."
  },
  {
    "objectID": "pstat197a/labs/lab6-nn/lab6-nn.html#pre-lab-activity",
    "href": "pstat197a/labs/lab6-nn/lab6-nn.html#pre-lab-activity",
    "title": "Training neural networks",
    "section": "",
    "text": "Please complete this activity before arriving at your section meeting. It should only take you about 15 minutes.\n\n\nWe will use packages that interface to python from R to train neural networks in this lab. For this a current python installation is needed.\nSelect the download appropriate for your operating system here and follow installation instructions.\n\n\n\nOpen RStudio and execute the following commands in the console. You should have already installed keras with other packages in the first lab; however, if library(keras) returns an error execute install.packages('keras') and then try again. This will install Tensorflow (for python) in a manner suitable for use in R, along with a few other packages.\n\nlibrary(keras)\ninstall_keras()\n\nTo confirm the installation worked, try:\n\nlibrary(tensorflow)\ntf$constant('Hello world')\n\ntf.Tensor(b'Hello world', shape=(), dtype=string)\n\n\nYou may see a long message related to CUDA libraries in addition to the output shown above, but if you see this output at the end, your installation was successful.\nIf you did not see the expected output, try configuring a virtual environment for the installation explicitly as shown here.\nIf you are unable to troubleshoot after a short period of time, partner with a classmate for the lab activity and then ask for help from course staff."
  },
  {
    "objectID": "pstat197a/labs/lab6-nn/lab6-nn.html#lab-activity",
    "href": "pstat197a/labs/lab6-nn/lab6-nn.html#lab-activity",
    "title": "Training neural networks",
    "section": "Lab activity",
    "text": "Lab activity\n\nSetup\n\n\n\n\n\n\nAction\n\n\n\nSetup\nOpen a new script for this lab, copy-paste the code chunk below at the top of the script, and execute once.\n\n\n\n# packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(tidytext)\nlibrary(keras)\nlibrary(tensorflow)\n\n# data location\nurl &lt;- 'https://raw.githubusercontent.com/pstat197/pstat197a/main/materials/labs/lab6-nn/data/claims-clean.csv'\n\n# read in data\nclean &lt;- read_csv(url)\n\nNow partition the data into training and test sets.\n\n\n\n\n\n\nAction\n\n\n\nData partitioning\n\nCopy the code chunk below into your script but do not run the resulting lines.\nCoordinate with your neighbor: choose a new RNG seed and split proportion and input the same values in each of your scripts.\nExecute lines to partition the data.\n\n\n\n\n# partition\nset.seed(102722)\npartitions &lt;- clean %&gt;%\n  mutate(text_clean = str_trim(text_clean)) %&gt;%\n  filter(str_length(text_clean) &gt; 5) %&gt;%\n  initial_split(prop = 0.8)\n\nNow use the code chunk below to preprocess the training partition into a TF-IDF document term matrix (DTM), as before.\n\ntrain_dtm &lt;- training(partitions) %&gt;%\n  unnest_tokens(output = 'token', \n                input = text_clean) %&gt;%\n  group_by(.id, bclass) %&gt;%\n  count(token) %&gt;%\n  bind_tf_idf(term = token, \n              document = .id, \n              n = n) %&gt;%\n  pivot_wider(id_cols = c(.id, bclass), \n              names_from = token, \n              values_from = tf_idf,\n              values_fill = 0) %&gt;%\n  ungroup()\n\n\n\nLogistic regression as NN\nTo get a feel for keras, first we’ll fit a logistic regression model.\nRecall that in class it was mentioned that standard statistical models can be described by neural networks with no hidden layers; along these lines, standard statistical models can also be fit using optimization routines for neural network training.\nUse the code chunk below to get the TF-IDF values for the (alphabetically) first ten tokens. We’ll use these as predictors.\n\n# extract first ten features\nx_train &lt;- train_dtm %&gt;%\n  ungroup() %&gt;%\n  select(-.id, -bclass) %&gt;%\n  select(1:10) %&gt;%\n  as.matrix()\n\n# extract labels and coerce to binary\ny_train &lt;- train_dtm %&gt;% \n  pull(bclass) %&gt;%\n  factor() %&gt;%\n  as.numeric() - 1\n\nThis is purely for illustration purposes; any model using these variables should not perform well at all because ten tokens won’t contain much information about the classes.\nTo use keras, we’ll go through a few steps that are generally not done separately for fitting statistical models:\n\nModel specification, i.e.,defining an architecture\nModel configuration, i.e., specifying a loss function and fitting method\nModel training, i.e., computing estimates for the parameters\n\nModel specification\nModel architecture is defined layer-by-layer. Keras has some preconfigured model types: for feedforward networks, use keras_model_sequential() .\n\n# specify model type\nmodel &lt;- keras_model_sequential(input_shape = 10)\n\nThe input_shape argument specifies the number of units for the input layer – in other words, the number of predictors.\nAt this stage, the model is just scaffolding:\n\nsummary(model)\n\nModel: \"sequential\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n================================================================================\nTotal params: 0\nTrainable params: 0\nNon-trainable params: 0\n________________________________________________________________________________\n\n\nNow layers can be added one-by-one. For now we’ll just add an output layer – one unit. layer_dense will specify that the previous layer is fully-connected to the added layer.\n\n# add output layer\nmodel &lt;- model %&gt;% layer_dense(1) \n\nThe model summary now shows the output layer.\n\nsummary(model)\n\nModel: \"sequential\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n dense (Dense)                      (None, 1)                       11          \n================================================================================\nTotal params: 11\nTrainable params: 11\nNon-trainable params: 0\n________________________________________________________________________________\n\n\nLastly, we’ll add a sigmoid activation function:\n\nmodel &lt;- model %&gt;% \n  layer_activation(activation = 'sigmoid')\n\nSince there is no hidden layer, our model is\n\\[\n\\begin{aligned}\n\\mathbb{E}Y &= \\frac{1}{1 + \\exp\\{-T\\}} \\\\\nT &= X\\beta\n\\end{aligned}\n\\]\nNotice that this is the logistic regression model (without the distributional assumption).\nModel configuration\nConfiguring a keras model consists in equipping it with a loss and an optimization method. Optionally, metrics that you’d like computed at each training epoch can be included.\n\nmodel %&gt;% compile(\n  loss = 'binary_crossentropy',\n  optimizer = optimizer_sgd(),\n  metrics = 'binary_accuracy'\n)\n\nThis says that to train the model, we’ll minimize binary cross-entropy on the training data using stochastic gradient descent.\nTo train for 10 epochs, pipe the model into fit() and supply the training data. Note that the training data must be numeric, not a data frame.\n\nhistory &lt;- model %&gt;%\n  fit(x = x_train, \n      y = y_train,\n      epochs = 10)\n\nThe following commands will retrieve weights, evaluated loss ans specified metrics, and predictions.\n\n# retrieve weights\nget_weights(model)\n\n[[1]]\n              [,1]\n [1,] -0.309943348\n [2,] -0.607303262\n [3,] -0.684534788\n [4,]  0.080475129\n [5,] -0.047711506\n [6,]  0.232125387\n [7,] -0.203573957\n [8,] -0.295428514\n [9,]  0.001397455\n[10,] -0.489086449\n\n[[2]]\n[1] 0.02782592\n\n# evaluate on specified data\nevaluate(model, x_train, y_train)\n\n           loss binary_accuracy \n      0.6928954       0.5226244 \n\n# compute predictions\nmodel(x_train) %&gt;% head()\n\ntf.Tensor(\n[[0.49740595]\n [0.5064572 ]\n [0.506311  ]\n [0.5053739 ]\n [0.5067238 ]\n [0.5057194 ]], shape=(6, 1), dtype=float32)\n\n\n\n\n\n\n\n\nAction\n\n\n\nCheck your understanding\nDiscuss with your neighbor:\n\nHow many parameters does this model have?\nDo the number of parameters match your expectations?\nWhy will the parameter estimates not match the result of glm() ?\nWould further training epochs improve the performance?\n\n\n\n\n\nSingle-layer network\nNow that you have a sense of the basic keras syntax and model specification/configuration/training procedure, we can train a proper network with one (or more!) hidden layers.\nFirst coerce the DTM into the format needed for training.\n\n# store full DTM as a matrix\nx_train &lt;- train_dtm %&gt;%\n  select(-bclass, -.id) %&gt;%\n  as.matrix()\n\nNow configure a model with one hidden layer having 10 units. Notice that the architecture can be defined by one sequence of pipes rather than stepwise as before.\n\nmodel &lt;- keras_model_sequential(input_shape = ncol(x_train)) %&gt;%\n  layer_dense(10) %&gt;%\n  layer_dense(1) %&gt;%\n  layer_activation(activation = 'sigmoid')\n\nsummary(model)\n\nModel: \"sequential_1\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n dense_2 (Dense)                    (None, 10)                      176540      \n dense_1 (Dense)                    (None, 1)                       11          \n activation_1 (Activation)          (None, 1)                       0           \n================================================================================\nTotal params: 176,551\nTrainable params: 176,551\nNon-trainable params: 0\n________________________________________________________________________________\n\n\nNotice the number of parameters. (Does this match your expectation?) Configure the model:\n\nmodel %&gt;%\n  compile(\n    loss = 'binary_crossentropy',\n    optimizer = optimizer_sgd(),\n    metrics = 'binary_accuracy'\n  )\n\nAnd finally, train:\n\nhistory &lt;- model %&gt;%\n  fit(x = x_train,\n      y = y_train,\n      epochs = 50)\n\nplot(history)\n\n\n\n\nNotice that even after 50 epochs the results are still quite poor. As mentioned in class, the choice of optimization method can have a big impact on the quality of estimates. If we train the model instead using Adam, good accuracy is achieved after just a few epochs:\n\n# change the optimizer\nmodel %&gt;%\n  compile(\n    loss = 'binary_crossentropy',\n    optimizer = 'adam',\n    metrics = 'binary_accuracy'\n  )\n\n# re-train\nhistory &lt;- model %&gt;%\n  fit(x = x_train,\n      y = y_train,\n      epochs = 10)\n\nplot(history)\n\n\n\n\n\n\nValidation data\nOften training data are sub-partitioned into training and ‘validation’ sets. The validation set can be used to provide a soft estimate of accuracy during training.\nThis provides one strategy to avoid overfitting – the practitioner should only train as long as validation accuracy continues to increase.\nKeras makes that easy by supplying an extra argument to fit(). The code chunk below trains for longer and uses 20% of the training data for validation. You should see that the training accuracy gets quite high, but the validation accuracy plateaus around 80%.\n\n# redefine model\nmodel &lt;- keras_model_sequential(input_shape = ncol(x_train)) %&gt;%\n  layer_dense(10) %&gt;%\n  layer_dense(1) %&gt;%\n  layer_activation(activation = 'sigmoid')\n\nmodel %&gt;%\n  compile(\n    loss = 'binary_crossentropy',\n    optimizer = 'adam',\n    metrics = 'binary_accuracy'\n  )\n\n# train with validation split\nhistory &lt;- model %&gt;%\n  fit(x = x_train,\n      y = y_train,\n      epochs = 20,\n      validation_split = 0.2)\n\nplot(history)\n\n\n\n\n\n\n\n\n\n\nAction\n\n\n\nCompute predictions from your trained network on the test partition. Estimate the predictive accuracy. Is it any better than what we managed with principal component regression in class?"
  },
  {
    "objectID": "pstat197a/labs/lab1-setup/lab1-setup.html",
    "href": "pstat197a/labs/lab1-setup/lab1-setup.html",
    "title": "Course technology overview",
    "section": "",
    "text": "Read this and complete all instructions in the ‘action’ boxes during your lab section. Your TA will walk you through the activity and help to troubleshoot issues and answer any questions along the way.\nObjectives:"
  },
  {
    "objectID": "pstat197a/labs/lab1-setup/lab1-setup.html#prerequisites",
    "href": "pstat197a/labs/lab1-setup/lab1-setup.html#prerequisites",
    "title": "Course technology overview",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nTo complete the activity you’ll need to:\n\nhave all of the software listed on the course technology page installed;\nfind (or create) your GitHub account credentials (if you are creating an account for the first time, see advice on choosing a username).\n\n\n\n\n\n\n\nAction\n\n\n\nPreparations:\n\nLog in to your GitHub account.\nOpen your GitHub client.\nOpen a new session in RStudio.\nCreate a class folder for PSTAT197 somewhere on your machine, e.g., ~/documents/pstat197."
  },
  {
    "objectID": "pstat197a/labs/lab1-setup/lab1-setup.html#rstudio-projects",
    "href": "pstat197a/labs/lab1-setup/lab1-setup.html#rstudio-projects",
    "title": "Course technology overview",
    "section": "RStudio projects",
    "text": "RStudio projects\nFirst we’ll get acquainted with the basic functionality of the RStudio IDE and the use of projects as a means of organizing files. If you’ve already used RStudio, great – this will still serve to introduce you to how we’ll use RStudio projects in this class.\n\nRStudio Setup\n\nYour TA will briefly review the (default) layout of the RStudio IDE. You should be able to identify/find the following:\n\nconsole\nterminal\nfile navigator\nenvironment\nhistory\n\nWe’ll use several R packages throughout the quarter. Some of these we will install on the go, but we can install several that we’ll rely on now.\n\n\n\n\n\n\nAction\n\n\n\nInstall packages\nNavigate to the console and copy-paste the following commands. You only need to do this once. This will take a minute or two to complete.\n\n\n\n# package install list \nurl &lt;- 'https://raw.githubusercontent.com/pstat197/pstat197a/main/materials/scripts/package-installs.R'\nsource(url)\n\n# clear environment\nrm(list = ls())\n\n\n\nCreate a local project\nProjects are a means of keeping your work organized. When you create a project in a directory on your local machine, RStudio keeps track of project metadata, history, and the working environment so that every time you open the project you see whatever you had open when you last closed it.\n\n\n\n\n\n\nAction\n\n\n\nCreate a new project:\n\nSelect File &gt; New project\nCreate the project in a new directory as a subdirectory of your class folder\nName it example-project\n\nComment: when naming files it’s good practice to avoid spaces, special characters, and the like. A naming convention we try to follow: choose a descriptive name comprising 1-3 words or common abbreviations separated by hyphens.\n\n\nTake a moment to observe the file navigator. It should consist of a single example-project.Rproj file.\n\n\nAdd content\nWe may as well populate the project with a few files – so let’s add a dataset and write a short script, as if we’re just starting a data analysis.\n\n\n\n\n\n\nAction\n\n\n\nRetrieve data and store a local copy\n\nOpen a new script: File &gt; New File &gt; R Script\nIn the navigator, create a folder called data and a folder called scripts\nCopy and paste the code chunk below into your script.\nExecute once, then save in the scripts folder as data-retrieval.R and close\n\n\n\n\nlibrary(tidyverse)\n\n# retrieve pollution data\nurl &lt;- 'https://raw.githubusercontent.com/pstat197/pstat197a/main/materials/labs/lab1-setup/data/pollution.csv'\npollution &lt;- read_csv(url)\n\n# write as csv to file\nwrite_csv(pollution, file = 'data/pollution.csv')\n\n# clear environment\nrm(list = ls())\n\n\nNext, we’ll do a simple regression analysis.\n\n\n\n\n\n\nAction\n\n\n\nCreate a script\n\nCreate a new script as before\nCopy-paste the code chunk below into your script\nExecute once and examine the results\nSave in the scripts folder as slr-analysis.R\n\n\n\n\nlibrary(tidyverse)\n\n# load data\npollution &lt;- read_csv('data/pollution.csv')\n\n# examine scatterplot with SLR fit\nggplot(pollution,\n       aes(x = log(SO2), y = Mort)) +\n  geom_point() +\n  geom_smooth(method = 'lm')\n\n\n\n# compute SLR fit\nfit &lt;- lm(Mort ~ log(SO2), data = pollution)\nbroom::tidy(fit)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    887.      17.6      50.4  1.37e-49\n2 log(SO2)        16.7      4.99      3.35 1.40e- 3\n\n# interpret\nfit_ci &lt;- confint(fit, parm = 'log(SO2)')*log(1.2)\n\npaste('With 95% confidence, every 20% increase in sulfur dioxide pollution is associated with an increase in two-year mortality rate between', \n      round(fit_ci[1], 2), \n      'and', \n      round(fit_ci[2], 2), \n      'per 100k', sep = ' ') %&gt;% \n  print()\n\n[1] \"With 95% confidence, every 20% increase in sulfur dioxide pollution is associated with an increase in two-year mortality rate between 1.23 and 4.87 per 100k\"\n\n\n\nCongrats on your first project! You can close the RStudio session now.\nWe’ll be using projects structured much like what you just set up, but with one catch: we’ll link up our RStudio projects with shared repositories so that we can all collaborate on the same set of project files."
  },
  {
    "objectID": "pstat197a/labs/lab1-setup/lab1-setup.html#github-repositories",
    "href": "pstat197a/labs/lab1-setup/lab1-setup.html#github-repositories",
    "title": "Course technology overview",
    "section": "GitHub repositories",
    "text": "GitHub repositories\n\nWe will be distributing course assignments as repositories via GitHub Classroom. A repository is simply a storage space.\nHere we’ll walk you through how to access and copy the files in a repository just as you will for course assignments. The first step is to accept an assignment through a link we’ve given to you – this will create a repository for you with the files we intend for you to have.\nFor now, we’ll make a ‘group sandbox’ that you can play in during our next class meeting.\n\n\n\n\n\n\nAction\n\n\n\nAccept an assignment in GitHub Classroom\n\nFollow the link to accept the group-sandbox assignment. Since it’s a group assignment, you will be prompted to join a team.\nJoin the team for the table you sat at during last class meeting. If you don’t remember, check your attendance form receipt.\n\nYou should be directed to a team repository on github.com. You may need to refresh your browser. Keep this window open; you will need the URL.\n\n\n\nGit and GitHub\nAt some point in time – possibly quite recently – you had to install Git on your local machine, as well as create a GitHub account. So, Git and GitHub are two different things.\nGit is version control software that enables you to systematically track and control file changes within a repository – a collection of files possibly with some directory structure. (The definition of ‘repository’ is simply ‘storage place’.)\nGitHub is an online platform for hosting repositories remotely. Anyone with access to a repository can make changes to files in the repository, and this enables multiple people to collaborate on code.\n\n\nlocal &lt;&gt; remote\nUsually remote repositories are not updated directly because contributors need to execute codes to test their changes and the remote server that hosts the repository is not equipped to do this.\nInstead, contributors will prepare changes on their own machine where they can test them, and then update the remote repository once their changes are complete.\nThis process of implementing file changes in a repository involves communicating information between local and remote locations. For this purpose a local copy of the remote repository is needed.\n\n\nCloning a repository\nIn Git lingo, a clone is a local copy of a remote repository. Creating a clone copies files and establishes the link between local and remote repositories so that changes can be sent to and received from the remote repository. You only need to create a clone once.\nTo clone a repository, all one needs is:\n\nthe remote location URL;\nthe local destination where the clone will be created;\npermission from the repository owner, if private.\n\nHere you’ll clone the group sandbox repository you just created/joined. You will need the URL; if you happened to close the page when you accepted the assignment earlier, you should be able to find the repository from your home page on github.com.\n\n\n\n\n\n\n\nAction\n\n\n\nClone the sandbox repository:\n\nOpen your GitHub client (GitKracken or GitHub Desktop or similar) and ensure you are logged in to your GitHub account.\nLook for a ‘Clone Repo’ menu item or similar and simply input the URL and the place you’d like to clone it; proceed through any prompts.\nCheck your file navigator to confirm that the repository files were copied.\n\n\n\n\nAn alternative possibility is to create the clone using a terminal command. In the terminal, navigate to the desired destination, and input:\ngit clone https://github.com/USERNAME/REPONAME\n\n\n\n\n\n\nRemarks\n\n\n\nOn terminal commands:\n\nIt’s recommended to manage Git actions through a visual client, as it’s much easier to see and understand what’s happening.\nHowever, if you know exactly what you’re doing, executing simple actions via Git bash in the terminal can be more efficient at times.\nFor example, you can keep a terminal open in RStudio and manage your repository workflow from there, without having to toggle between environments.\nTry experimenting with terminal commands from RStudio after you have a little experience with basic Git actions."
  },
  {
    "objectID": "pstat197a/labs/lab1-setup/lab1-setup.html#checklist",
    "href": "pstat197a/labs/lab1-setup/lab1-setup.html#checklist",
    "title": "Course technology overview",
    "section": "Checklist",
    "text": "Checklist\nHave you completed all of the activity action items?\n\nInstall software: R, RStudio, Git, and a GitHub client\nCreate a GitHub account\nInstall R packages that will be used frequently\nCreate a local project in RStudio\nAccept the group sandbox assignment on GitHub Classroom\nClone the group sandbox repo"
  },
  {
    "objectID": "pstat197a/labs/lab3-iteration/lab3-iteration.html",
    "href": "pstat197a/labs/lab3-iteration/lab3-iteration.html",
    "title": "Iteration strategies",
    "section": "",
    "text": "In class we discussed multiple testing in the context of an application that involved performing 1,317 \\(t\\)-tests. Implementing these tests involves iteration: repeatedly performing the same computations.\nObjective. Here we’ll look at a few strategies for iteration in R:\n\nloops\nfunctions in the apply family\nfunctional programming using tidyverse\n\nWe’ll illustrate these strategies using the biomarker data and reproduce some of the results shown in class.\n\n\n\n\n\n\nAction\n\n\n\nCreate a new script for lab 3 in your labs project/folder and copy-paste the code chunk below at the top of the script.\n\n\n\nlibrary(tidyverse)\n# install.packages('infer') # execute once then comment out\n\n# data location\nurl &lt;- 'https://raw.githubusercontent.com/pstat197/pstat197a/main/materials/labs/lab3-iteration/data/biomarker-clean.csv'\n\n# function for outlier trimming\ntrim_fn &lt;- function(x){\n  x[x &gt; 3] &lt;- 3\n  x[x &lt; -3] &lt;- -3\n  \n  return(x)\n}\n\n# read in and preprocess data\nasd &lt;- read_csv(url) %&gt;%\n  select(-ados) %&gt;%\n  # log transform\n  mutate(across(.cols = -group, log10)) %&gt;%\n  # center and scale\n  mutate(across(.cols = -group, ~ scale(.x)[, 1])) %&gt;%\n  # trim outliers\n  mutate(across(.cols = -group, trim_fn))"
  },
  {
    "objectID": "pstat197a/labs/lab3-iteration/lab3-iteration.html#background",
    "href": "pstat197a/labs/lab3-iteration/lab3-iteration.html#background",
    "title": "Iteration strategies",
    "section": "",
    "text": "In class we discussed multiple testing in the context of an application that involved performing 1,317 \\(t\\)-tests. Implementing these tests involves iteration: repeatedly performing the same computations.\nObjective. Here we’ll look at a few strategies for iteration in R:\n\nloops\nfunctions in the apply family\nfunctional programming using tidyverse\n\nWe’ll illustrate these strategies using the biomarker data and reproduce some of the results shown in class.\n\n\n\n\n\n\nAction\n\n\n\nCreate a new script for lab 3 in your labs project/folder and copy-paste the code chunk below at the top of the script.\n\n\n\nlibrary(tidyverse)\n# install.packages('infer') # execute once then comment out\n\n# data location\nurl &lt;- 'https://raw.githubusercontent.com/pstat197/pstat197a/main/materials/labs/lab3-iteration/data/biomarker-clean.csv'\n\n# function for outlier trimming\ntrim_fn &lt;- function(x){\n  x[x &gt; 3] &lt;- 3\n  x[x &lt; -3] &lt;- -3\n  \n  return(x)\n}\n\n# read in and preprocess data\nasd &lt;- read_csv(url) %&gt;%\n  select(-ados) %&gt;%\n  # log transform\n  mutate(across(.cols = -group, log10)) %&gt;%\n  # center and scale\n  mutate(across(.cols = -group, ~ scale(.x)[, 1])) %&gt;%\n  # trim outliers\n  mutate(across(.cols = -group, trim_fn))"
  },
  {
    "objectID": "pstat197a/labs/lab3-iteration/lab3-iteration.html#loops",
    "href": "pstat197a/labs/lab3-iteration/lab3-iteration.html#loops",
    "title": "Iteration strategies",
    "section": "Loops",
    "text": "Loops\n\nSimple examples\nA loop is a set of instructions to be repeated a specified number of times while incrementing a flag or index value. For example:\n\nfor(i in 1:4){\n  print(2*i)\n}\n\n[1] 2\n[1] 4\n[1] 6\n[1] 8\n\n\nHere the instructions are:\n\ninitialize index/flag i at i = 1\nexecute code within the braces {...}\nincrement i &lt;- i + 1\nrepeat steps 2-3 until i = 5\n\nWe could make the loop a bit more verbose:\n\nflag_vals &lt;- c(1, 2, 3, 4)\nfor(i in flag_vals){\n  out &lt;- 2*i\n  print(out)\n}\n\n[1] 2\n[1] 4\n[1] 6\n[1] 8\n\n\nNow to retain the results in memory, a storage data structure must be defined and the output of each iteration assigned to some element(s) of the storage object.\n\nrslt &lt;- rep(NA, 4)\nfor(i in 1:4){\n  rslt[i] &lt;- 2*i\n}\nrslt\n\n[1] 2 4 6 8\n\n\nIf we want to perform the same calculation for all values in a vector, we might do something like this:\n\nrslt &lt;- rep(NA, 4)\ninput_vals &lt;- c(15, 27, 3, 12.6)\nfor(i in 1:4){\n  rslt[i] &lt;- 2*input_vals[i]\n}\nrslt\n\n[1] 30.0 54.0  6.0 25.2\n\n\n\n\n\n\n\n\nCheck your understanding\n\n\n\nWhy does the following loop produce an NA ?\n\nrslt &lt;- rep(NA, 4)\ninput_vals &lt;- rnorm(n = 3)\nfor(i in 1:4){\n  rslt[i] &lt;- 2*input_vals[i]\n}\n\nrslt\n\n[1] -3.3664950 -0.5316422  1.2368258         NA\n\n\n\n\nLoops are substantially similar in any programming language but usually not optimized for performance. Additionally, they are somewhat verbose and hard to read due to explicit use of indexing in the syntax.\n\n\nMultiple testing with loops\nIn base R, the \\(t\\)-test is performed using t.test(...) , which takes as arguments two vectors of observations (one for each group). For instance:\n\nx &lt;- asd %&gt;% filter(group == 'ASD') %&gt;% pull(CHIP)\ny &lt;- asd %&gt;% filter(group == 'TD') %&gt;% pull(CHIP)\nt.test(x, y, var.equal = F)\n\n\n    Welch Two Sample t-test\n\ndata:  x and y\nt = -0.18812, df = 151.74, p-value = 0.851\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.2588194  0.2138173\nsample estimates:\n  mean of x   mean of y \n-0.04922954 -0.02672849 \n\n\nThe output is a list:\n\nt.test(x, y) %&gt;% str()\n\nList of 10\n $ statistic  : Named num -0.188\n  ..- attr(*, \"names\")= chr \"t\"\n $ parameter  : Named num 152\n  ..- attr(*, \"names\")= chr \"df\"\n $ p.value    : num 0.851\n $ conf.int   : num [1:2] -0.259 0.214\n  ..- attr(*, \"conf.level\")= num 0.95\n $ estimate   : Named num [1:2] -0.0492 -0.0267\n  ..- attr(*, \"names\")= chr [1:2] \"mean of x\" \"mean of y\"\n $ null.value : Named num 0\n  ..- attr(*, \"names\")= chr \"difference in means\"\n $ stderr     : num 0.12\n $ alternative: chr \"two.sided\"\n $ method     : chr \"Welch Two Sample t-test\"\n $ data.name  : chr \"x and y\"\n - attr(*, \"class\")= chr \"htest\"\n\n\nSo if we want the p-value:\n\nt.test(x, y, var.equal = F)$p.value\n\n[1] 0.8510352\n\n\nTo calculate \\(p\\)-values for all tests using a loop, we wrap the code we used to perform one \\(t\\)-test in a for loop and add appropriate indexing. For speed, we’ll just compute the first 100 tests:\n\nn_tests &lt;- 100\np_vals &lt;- rep(NA, n_tests)\nfor(i in 1:n_tests){\n  x &lt;- asd %&gt;% filter(group == 'ASD') %&gt;% pull(i + 1)\n  y &lt;- asd %&gt;% filter(group == 'TD') %&gt;% pull(i + 1)\n  p_vals[i] &lt;- t.test(x, y, var.equal = F)$p.value\n}\n\nTo line these up with the proteins they correspond to, it’s necessary to keep track of the indexing carefully. In this case, the indexing corresponds to the order of columns. So we could create a data frame like so:\n\ntibble(protein = colnames(asd)[2:(n_tests + 1)],\n       p = p_vals)\n\n# A tibble: 100 × 2\n   protein        p\n   &lt;chr&gt;      &lt;dbl&gt;\n 1 CHIP     0.851  \n 2 CEBPB    0.0322 \n 3 NSE      0.350  \n 4 PIAS4    0.104  \n 5 IL-10 Ra 0.0232 \n 6 STAT3    0.00183\n 7 IRF1     0.592  \n 8 c-Jun    0.0351 \n 9 Mcl-1    0.999  \n10 OAS1     0.942  \n# … with 90 more rows\n\n\nAlternatively, we could have set up the loop to output this result:\n\nn_tests &lt;- 100\nrslt &lt;- tibble(protein = colnames(asd)[2:(n_tests + 1)],\n               p = NA)\nfor(i in 1:n_tests){\n  x &lt;- asd %&gt;% filter(group == 'ASD') %&gt;% pull(i + 1)\n  y &lt;- asd %&gt;% filter(group == 'TD') %&gt;% pull(i + 1)\n  rslt$p[i] &lt;- t.test(x, y, var.equal = F)$p.value\n}\n\n\n\n\n\n\n\nAction\n\n\n\nFollow the example above to write a loop that stores both the \\(p\\)-values and the estimated differences for the first 50 proteins."
  },
  {
    "objectID": "pstat197a/labs/lab3-iteration/lab3-iteration.html#apply-family",
    "href": "pstat197a/labs/lab3-iteration/lab3-iteration.html#apply-family",
    "title": "Iteration strategies",
    "section": "Apply family",
    "text": "Apply family\n\nSimple examples\nIn R, the apply family of functions allows one to efficiently iterate a function over an index set. So, to execute our simple for loop using apply , we could do something like this:\n\nvals &lt;- rnorm(n = 4)\nsimple_fn &lt;- function(x){2*x}\nlapply(vals, simple_fn)\n\n[[1]]\n[1] 1.062206\n\n[[2]]\n[1] 1.619845\n\n[[3]]\n[1] 5.018638\n\n[[4]]\n[1] 0.800267\n\n\nThis applies simple_fn to each element of vals , and returns the result as a list. If we want a neater output format, we could use sapply , which is short for sort-apply:\n\nsapply(vals, simple_fn)\n\n[1] 1.062206 1.619845 5.018638 0.800267\n\n\nIn more complex settings it often makes sense to apply a function across an index set. This is very similar conceptually to a for loop, but faster and easier to read.\n\n# apply a function to an index set\nsimple_fn_ix &lt;- function(i){2*vals[i]}\nrslt_apply &lt;- sapply(1:length(vals), simple_fn_ix)\n\n# equivalent for loop\nrslt_loop &lt;- rep(NA, length(vals))\nfor(i in 1:length(vals)){\n  rslt_loop[i] &lt;- 2*vals[i]\n}\n\n# compare\nrbind(rslt_loop, rslt_apply)\n\n               [,1]     [,2]     [,3]     [,4]\nrslt_loop  1.062206 1.619845 5.018638 0.800267\nrslt_apply 1.062206 1.619845 5.018638 0.800267\n\n\n\n\n\\(t\\)-tests using apply\nWe can use apply functions to compute \\(t\\)-tests for the proteins in the ASD data by coercing the data to a list of data frames that contain the grouping and level for each protein.\n\n# number of tests to perform\nn_tests &lt;- 100\n\n# convert to a list\nasd_list &lt;- asd %&gt;% \n  select(1:(n_tests + 1)) %&gt;%\n  pivot_longer(cols = -group,\n               names_to = 'protein',\n               values_to = 'level') %&gt;%\n  group_by(protein) %&gt;%\n  group_split()\n\n# first entry in list\nasd_list[[1]]\n\n# A tibble: 154 × 3\n   group protein                     level\n   &lt;chr&gt; &lt;chr&gt;                       &lt;dbl&gt;\n 1 ASD   14-3-3 protein beta/alpha -0.124 \n 2 ASD   14-3-3 protein beta/alpha  0.487 \n 3 ASD   14-3-3 protein beta/alpha -0.801 \n 4 ASD   14-3-3 protein beta/alpha  2.73  \n 5 ASD   14-3-3 protein beta/alpha  1.24  \n 6 ASD   14-3-3 protein beta/alpha  0.250 \n 7 ASD   14-3-3 protein beta/alpha  0.932 \n 8 ASD   14-3-3 protein beta/alpha  0.0873\n 9 ASD   14-3-3 protein beta/alpha  0.213 \n10 ASD   14-3-3 protein beta/alpha  0.157 \n# … with 144 more rows\n\n\nThe function t.test(...) can also perform the test using a formula of the form y ~ x and a data frame containing x and y, as below.\n\nt.test(level ~ group, data = asd_list[[1]])\n\n\n    Welch Two Sample t-test\n\ndata:  level by group\nt = -1.5671, df = 150.2, p-value = 0.1192\nalternative hypothesis: true difference in means between group ASD and group TD is not equal to 0\n95 percent confidence interval:\n -0.54341111  0.06269287\nsample estimates:\nmean in group ASD  mean in group TD \n       -0.1341683         0.1061909 \n\n\nIf we just want the \\(p\\)-value again, we can wrap this code in a function whose argument is the index \\(i\\). This function will return the \\(p\\)-value for the \\(i\\)th protein.\n\n# p value for ith protein\ntt_fn &lt;- function(i){\n  t.test(level ~ group, data = asd_list[[i]])$p.value\n}\n\n# check\ntt_fn(1)\n\n[1] 0.1191888\n\n\nNow to perform many tests, we can simply iterate this function over consecutive index values 1:n_tests:\n\nsapply(1:n_tests, tt_fn)\n\n  [1] 1.191888e-01 2.972829e-01 8.297144e-01 3.034583e-02 8.136635e-01\n  [6] 9.517828e-01 3.553359e-01 2.671529e-03 6.458878e-01 3.915314e-04\n [11] 1.759142e-01 3.505666e-02 5.095419e-01 4.788545e-07 2.862286e-01\n [16] 5.714296e-01 7.052780e-03 3.220583e-02 8.510352e-01 1.267133e-03\n [21] 9.482110e-03 1.293157e-04 7.804081e-03 2.208460e-04 1.407044e-01\n [26] 1.023033e-01 8.995855e-02 1.578665e-02 3.113212e-04 2.920587e-02\n [31] 4.663516e-07 2.395764e-01 5.709433e-03 8.962287e-02 2.700053e-02\n [36] 5.357313e-01 7.392658e-01 8.665332e-01 3.538260e-02 1.956257e-04\n [41] 8.658766e-01 2.111378e-04 3.286108e-01 5.374305e-01 1.108687e-01\n [46] 1.711403e-01 4.808293e-01 6.775472e-01 3.556564e-03 2.322968e-02\n [51] 3.746226e-01 7.804488e-01 3.175372e-01 4.085249e-01 4.746117e-03\n [56] 5.917788e-01 3.748021e-02 6.433125e-01 1.121721e-03 3.234610e-03\n [61] 4.154758e-03 1.669733e-03 1.726578e-03 9.990017e-01 7.100178e-04\n [66] 4.811425e-01 8.978465e-01 3.503310e-01 9.423978e-01 3.925728e-01\n [71] 3.025965e-01 4.511875e-02 4.219360e-01 2.117196e-01 1.036412e-01\n [76] 5.590746e-01 8.148983e-01 1.399029e-02 5.096269e-04 9.145121e-02\n [81] 3.331394e-02 4.350959e-01 8.721647e-01 3.266951e-03 2.704495e-01\n [86] 4.929196e-01 1.010954e-03 3.144033e-01 7.933758e-04 1.929813e-03\n [91] 6.104791e-02 1.832399e-03 1.333513e-02 6.412884e-01 2.605232e-02\n [96] 4.130732e-01 2.579393e-01 4.096623e-01 2.925137e-01 5.866341e-01\n\n\nYou might have noticed this was much faster than the loop. We can time it:\n\nstart &lt;- Sys.time()\nrslt &lt;- sapply(1:n_tests, tt_fn)\nend &lt;- Sys.time()\n\nend - start\n\nTime difference of 0.209661 secs\n\n\nAnd compare with the for loop:\n\nstart &lt;- Sys.time()\nn_tests &lt;- 100\nrslt &lt;- tibble(protein = colnames(asd)[2:(n_tests + 1)],\n               p = NA)\nfor(i in 1:n_tests){\n  x &lt;- asd %&gt;% filter(group == 'ASD') %&gt;% pull(i + 1)\n  y &lt;- asd %&gt;% filter(group == 'TD') %&gt;% pull(i + 1)\n  rslt$p[i] &lt;- t.test(x, y, var.equal = F)$p.value\n}\nend &lt;- Sys.time()\n\nend - start\n\nTime difference of 11.63856 secs\n\n\nAnother nice feature of sapply is its ability to sort and arrange multiple outputs. For example, if the function is adjusted to return both the \\(p\\)-value and the test statistic:\n\ntt_fn &lt;- function(i){\n  test_rslt &lt;- t.test(level ~ group, data = asd_list[[i]])\n  out &lt;- c(pval = test_rslt$p.value, \n           tstat = test_rslt$statistic)\n  out\n}\n\ntt_fn(1)\n\n      pval    tstat.t \n 0.1191888 -1.5671297 \n\n\nThen sapply will return a matrix:\n\nsapply(1:5, tt_fn) %&gt;% t() %&gt;% as_tibble()\n\n# A tibble: 5 × 2\n    pval tstat.t\n   &lt;dbl&gt;   &lt;dbl&gt;\n1 0.119   -1.57 \n2 0.297   -1.05 \n3 0.830   -0.215\n4 0.0303  -2.19 \n5 0.814    0.236\n\n\n\n\n\n\n\n\nAction\n\n\n\n\nUse sapply to obtain the estimated differences and standard errors for the groupwise comparisons for the first 50 proteins.\nArrange the result in a data frame with a column indicating the protein, a column indicating the estimated group difference, and a column indicating the standard error."
  },
  {
    "objectID": "pstat197a/labs/lab3-iteration/lab3-iteration.html#tidyverse",
    "href": "pstat197a/labs/lab3-iteration/lab3-iteration.html#tidyverse",
    "title": "Iteration strategies",
    "section": "Tidyverse",
    "text": "Tidyverse\nA final strategy for iteration comes from functional programming tools in tidyverse . The basic idea is:\n\ndefine a grouping structure using relevant variables (in this case, proteins)\ncollapse the data into separate data frames by group\napply a function to each data frame that produces test output given input data\n\n\nNesting\nOne thing that tibbles can do that data frames cannot is store list-columns: columns that are lists of arbitrary objects. This allows for the arrangement of a much more general collection of objects in tabular form.\nAn intuitive example is nested data: a list-column of data frames having the same columns. If we nest the ASD data by protein, we obtain a data frame that looks like this:\n\nasd_nested &lt;- asd %&gt;%\n  pivot_longer(-group, \n               names_to = 'protein', \n               values_to = 'level') %&gt;%\n  nest(data = c(level, group))\n\nasd_nested %&gt;% head(5)\n\n# A tibble: 5 × 2\n  protein  data              \n  &lt;chr&gt;    &lt;list&gt;            \n1 CHIP     &lt;tibble [154 × 2]&gt;\n2 CEBPB    &lt;tibble [154 × 2]&gt;\n3 NSE      &lt;tibble [154 × 2]&gt;\n4 PIAS4    &lt;tibble [154 × 2]&gt;\n5 IL-10 Ra &lt;tibble [154 × 2]&gt;\n\n\nThe data column consists of data frames, one per protein, containing the variables group and level :\n\nasd_nested %&gt;%\n  slice(1L) %&gt;%\n  pull(data)\n\n[[1]]\n# A tibble: 154 × 2\n     level group\n     &lt;dbl&gt; &lt;chr&gt;\n 1  0.335  ASD  \n 2 -0.0715 ASD  \n 3 -0.406  ASD  \n 4 -0.102  ASD  \n 5 -0.395  ASD  \n 6 -0.126  ASD  \n 7  0.486  ASD  \n 8 -0.990  ASD  \n 9 -0.108  ASD  \n10  0.485  ASD  \n# … with 144 more rows\n\n\n\n\nThe map() function\nIn an ordinary data frame one can define a new variable as a function of other variables. The same can be done with list-columns in a tibble: one can define a new variable as a function of the elements of a list stored in another column. To do this, one uses the map() function, which is essentially the tidyverse version of lapply() :\n\nmap(.x, .fn) means roughly “apply the function .fn to each element in .x”\n\nHere we can write a function that takes a data frame with protein level and group as input, and returns a t test as output; then computing each test is as simple as calling mutate :\n\ntt_fn &lt;- function(.df){\n  t.test(level ~ group, data = .df)\n}\n\nrslt &lt;- asd_nested %&gt;%\n  slice(1:10) %&gt;%\n  mutate(ttest.out = map(data, tt_fn))\n\nrslt\n\n# A tibble: 10 × 3\n   protein  data               ttest.out\n   &lt;chr&gt;    &lt;list&gt;             &lt;list&gt;   \n 1 CHIP     &lt;tibble [154 × 2]&gt; &lt;htest&gt;  \n 2 CEBPB    &lt;tibble [154 × 2]&gt; &lt;htest&gt;  \n 3 NSE      &lt;tibble [154 × 2]&gt; &lt;htest&gt;  \n 4 PIAS4    &lt;tibble [154 × 2]&gt; &lt;htest&gt;  \n 5 IL-10 Ra &lt;tibble [154 × 2]&gt; &lt;htest&gt;  \n 6 STAT3    &lt;tibble [154 × 2]&gt; &lt;htest&gt;  \n 7 IRF1     &lt;tibble [154 × 2]&gt; &lt;htest&gt;  \n 8 c-Jun    &lt;tibble [154 × 2]&gt; &lt;htest&gt;  \n 9 Mcl-1    &lt;tibble [154 × 2]&gt; &lt;htest&gt;  \n10 OAS1     &lt;tibble [154 × 2]&gt; &lt;htest&gt;  \n\nrslt %&gt;% slice(1L) %&gt;% pull(ttest.out)\n\n[[1]]\n\n    Welch Two Sample t-test\n\ndata:  level by group\nt = -0.18812, df = 151.74, p-value = 0.851\nalternative hypothesis: true difference in means between group ASD and group TD is not equal to 0\n95 percent confidence interval:\n -0.2588194  0.2138173\nsample estimates:\nmean in group ASD  mean in group TD \n      -0.04922954       -0.02672849 \n\n\nWhile all the data we might want are there, the output is a little unwieldy. Luckily, the infer package contains a pipe-operator-friendly function infer::t_test that returns results in a tidy fashion.\n\nasd_nested %&gt;% \n  slice(1L) %&gt;% \n  unnest(cols = data) %&gt;% \n  infer::t_test(formula = level ~ group,\n         order = c('ASD', 'TD'),\n         alternative = 'two-sided',\n         var.equal = F)\n\n# A tibble: 1 × 7\n  statistic  t_df p_value alternative estimate lower_ci upper_ci\n      &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1    -0.188  152.   0.851 two.sided    -0.0225   -0.259    0.214\n\n\nWe can create a wrapper around this function suitable for our purposes and then apply it to the list-column in asd_nested :\n\n# wrapper around infer::t_test\ntt_fn &lt;- function(.df){\n  infer::t_test(.df, \n         formula = level ~ group,\n         order = c('ASD', 'TD'),\n         alternative = 'two-sided',\n         var.equal = F)\n}\n\n# compute test results\ntt_out &lt;- asd_nested %&gt;%\n  slice(1:n_tests) %&gt;%\n  mutate(ttest = map(data, tt_fn))\n\n# preview\ntt_out %&gt;% head(4)\n\n# A tibble: 4 × 3\n  protein data               ttest           \n  &lt;chr&gt;   &lt;list&gt;             &lt;list&gt;          \n1 CHIP    &lt;tibble [154 × 2]&gt; &lt;tibble [1 × 7]&gt;\n2 CEBPB   &lt;tibble [154 × 2]&gt; &lt;tibble [1 × 7]&gt;\n3 NSE     &lt;tibble [154 × 2]&gt; &lt;tibble [1 × 7]&gt;\n4 PIAS4   &lt;tibble [154 × 2]&gt; &lt;tibble [1 × 7]&gt;\n\n\nNotice that ttest is also a list-column comprised of separate data frames. This column can be un-nested to show the output of infer::t_test explicitly:\n\ntt_out %&gt;% \n  unnest(ttest) %&gt;%\n  head(4)\n\n# A tibble: 4 × 9\n  protein data     statistic  t_df p_value alternative estimate lower_ci\n  &lt;chr&gt;   &lt;list&gt;       &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt;\n1 CHIP    &lt;tibble&gt;    -0.188  152.  0.851  two.sided    -0.0225  -0.259 \n2 CEBPB   &lt;tibble&gt;     2.16   150.  0.0322 two.sided     0.317    0.0273\n3 NSE     &lt;tibble&gt;     0.937  151.  0.350  two.sided     0.148   -0.164 \n4 PIAS4   &lt;tibble&gt;     1.64   152.  0.104  two.sided     0.222   -0.0459\n# … with 1 more variable: upper_ci &lt;dbl&gt;\n\n\nThis approach has a few advantages, namely, it is syntactically more readable than either of the other approaches and it works with the pipe operator, so could in theory be incorporated into a chain that performs additional calculations on, say, the results of the \\(t\\)-test. However, the drawback is that it is slow:\n\n# time it\nstart &lt;- Sys.time()\ntt_out &lt;- asd_nested %&gt;%\n  slice(1:n_tests) %&gt;%\n  mutate(ttest = map(data, tt_fn))\nend &lt;- Sys.time()\n\nend - start\n\nTime difference of 4.896297 secs\n\n\nIt’s not as slow as a for loop, but it’s much slower than apply functions. If speed is a concern or the number of iterations is especially large, apply would be a better choice.\n\n\nAdjusting p-values\nTo adjust the \\(p\\)-values, we simply manipulate the p_value column:\n\n# bonferroni correction\ntt_out %&gt;% \n  unnest(ttest) %&gt;%\n  mutate(p_adj = p_value*n_tests) %&gt;%\n  select(protein, p_value, p_adj) %&gt;%\n  arrange(p_adj) %&gt;%\n  head(4)\n\n# A tibble: 4 × 3\n  protein     p_value     p_adj\n  &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;\n1 FSTL1   0.000000466 0.0000466\n2 C1QR1   0.000000479 0.0000479\n3 DSC2    0.000129    0.0129   \n4 HIF-1a  0.000196    0.0196   \n\n\n\n\n\n\n\n\nAction\n\n\n\nImplement the Benjamini-Hochberg correction\n\nSort the raw \\(p\\)-values using arrange()\nAdd a rank column of consecutive integers (neat trick: try using row_number())\nAdd a column p_adj containing \\(\\frac{m}{i} p_{(i)}\\) where \\(m\\) is the number of tests, \\(i\\) is the rank of the \\(p\\)-value, and \\(p_{(i)}\\) is the \\(i\\)th smallest \\(p\\)-value\nFind the collection of proteins with significantly different serum levels between the ASD and TD groups while controlling the false discovery rate at 1%.\n\nDevelop working code to execute 50 tests. Then use it to compute all 1317 tests."
  },
  {
    "objectID": "pstat197a/labs/lab3-iteration/lab3-iteration.html#checklist",
    "href": "pstat197a/labs/lab3-iteration/lab3-iteration.html#checklist",
    "title": "Iteration strategies",
    "section": "Checklist",
    "text": "Checklist\n\nYou’ve computed \\(p\\)-values iteratively using a loop, sapply , and nest+map .\nYou have commented codes in your script for each action item.\nYou’ve obtained a list of the significant proteins with 1% FDR.\nYou’ve saved your work somewhere where you can access it later."
  },
  {
    "objectID": "pstat197a/labs/lab8-forecasting/lab8-forecasting.html",
    "href": "pstat197a/labs/lab8-forecasting/lab8-forecasting.html",
    "title": "Regression with AR errors",
    "section": "",
    "text": "In this lab you’ll see how to develop and then fit a regression model with AR errors.\nObjectives: select AR order for an error model using exploratory diagnostic techniques; fit a regression model with AR errors and generate forecasts for future series.\n\nSetup\n\n\n\n\n\n\nAction\n\n\n\nOpen RStudio and set up a new script in your lab directory. Copy and paste the code chunk below and execute once.\nYou may need to install the forecast package.\n\n\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(forecast)\nlibrary(fda)\n\nurl &lt;- 'https://raw.githubusercontent.com/pstat197/pstat197a/main/materials/labs/lab8-forecasting/data/soiltemp-200cm.csv'\n\nsoil &lt;- read_csv(url) %&gt;%\n  dplyr::select(-year, -elev) %&gt;%\n  filter(!str_starts(site, 'SHA'))\n\n\n\nSite selection\nAlthough you saw models fit to every site in class, for this lab we’ll just focus on building a model for a single site. You will pick a site and carry out the model-building process; your neighbor(s) will likely pick different sites, so you can compare results with them throughout.\nThe process you go through in this lab is what would be iterated over every site to reproduce the analysis shown in class.\n\n\n\n\n\n\nAction\n\n\n\nTo start, pick a site to work with using the code chunk below. Check with your neighbor and ensure you choose different sites.\n\n\n\n# choose a site at random\nset.seed(111522) # comment out!\nnsites &lt;- soil %&gt;% pull(site) %&gt;% unique() %&gt;% length()\nsite_ix &lt;- sample(1:nsites, size = 1)\n\n# filter rows\nsite_data &lt;- soil %&gt;% \n  filter(site == unique(soil$site)[site_ix])\n\n# preview\nsite_data %&gt;% head()\n\n# A tibble: 6 × 4\n  site     day date        temp\n  &lt;chr&gt;  &lt;dbl&gt; &lt;date&gt;     &lt;dbl&gt;\n1 B21K-1   226 2017-08-14  3.82\n2 B21K-1   227 2017-08-15  3.72\n3 B21K-1   228 2017-08-16  3.31\n4 B21K-1   229 2017-08-17  4.95\n5 B21K-1   230 2017-08-18  5.46\n6 B21K-1   231 2017-08-19  5.80\n\n\n\n\nAR order selection\nOur model (for one site) is\n\\[\n\\begin{aligned}\nY_t &= f(t) + \\epsilon_t \\\\\n\\epsilon_t &= \\sum_{d = 1}^D \\alpha_d \\epsilon_{t - d} + \\xi_t\n\\end{aligned}\n\\]\nThere are a few ways to pick the order \\(D\\), but the most common is to inspect the correlation structure of the residuals.\nSo first we need to get the residuals.\n\n\n\n\n\n\nAction\n\n\n\nCopy the code chunk below verbatim and execute. This will fit the regression part of our model and store the residuals along with other model quantities.\n\n\n\n# predictor matrix\nxreg &lt;- site_data %&gt;%\n  pull(day) %&gt;%\n  fda::fourier(nbasis = 4, period = 365)\n\n# response\ny &lt;- pull(site_data, temp)\n\n# create a data frame\nreg_df &lt;- bind_cols(temp = y, \n                    xreg)\n\n# fit the model\nfit &lt;- lm(temp ~ . - 1, data = reg_df)\n\n# obtain fitted values, residuals, etc.\nfit_df &lt;- broom::augment(fit) %&gt;%\n  bind_cols(date = site_data$date)\n\nTo start, let’s just plot the residual series:\n\n# plot residual series\nfit_df %&gt;%\n  ggplot(aes(x = date, y = .resid)) +\n  geom_path()\n\n\n\n\nIdeally, this should be (approximately) “stationary”, which means the series has:\n\nconstant mean over time\nconstant variance over time\n\n\n\n\n\n\n\nAction\n\n\n\nPlot the residual series for your site and compare with your neighbor.\nDiscuss: do your residual series look stationary, or close enough? What do you look to in the graphic to make that assessment?\n\n\nIf we plot the residuals against themselves at several time lags, we can observe the correlation decaying gradually.\n\n# plot residuals at various lags\nfit_df %&gt;%\n  dplyr::select(.resid) %&gt;%\n  mutate(lag1 = lag(.resid, n = 1),\n         lag2 = lag(.resid, n = 2),\n         lag3 = lag(.resid, n = 3),\n         lag4 = lag(.resid, n = 4),\n         lag5 = lag(.resid, n = 5),\n         lag6 = lag(.resid, n = 6)) %&gt;%\n  pivot_longer(-.resid) %&gt;%\n  ggplot(aes(x = .resid, y = value)) +\n  geom_point() + \n  facet_wrap(~ name)\n\n\n\n\nWe can capture this pattern succinctly by computing the correlation coefficient at each lag. This is called the (sample) autocorrelation function.\n\nresid_acf &lt;- acf(fit_df$.resid, plot = F)\n\nplot(resid_acf, main = '')\n\n\n\n\nIf you’re curious, try computing a few correlations ‘by hand’ and checking that they closely match resid_acf (they won’t match exactly).\n\n\n\n\n\n\nCheck your understanding\n\n\n\nDiscuss with your neighbor:\n\nWhat do the heights of the vertical lines in the ACF plot show?\nWhat does the ACF plot capture about the residuals?\n\n\n\nNow, any autoregressive process will have some autocorrelation even at distant lags, regardless of the order.\nTo select the order, the partial autocorrelation is used – the correlation at a given lag after adjusting for previous lags.\n\nresid_pacf &lt;- pacf(fit_df$.resid, plot = F)\n\nplot(resid_pacf, main = '')\n\n\n\n\n\n\n\n\n\n\nAside\n\n\n\nThe partial autocorrelations are computed iteratively by:\n\nComputing the correlation between \\(e_t\\) and \\(e_{t - 1}\\)\nRemoving the part of \\(e_t\\) explainable based on \\(e_{t - 1}\\) and obtaining an adjusted residual \\(\\tilde{e}_t = e_t - \\beta_1 e_{t - 1}\\)\nRepeat 1-2 for lags 2, 3, …, replacing \\(e_{t - h}\\) with \\(\\tilde{e}_{t - h}\\)\n\n\n\nThe simplest heuristic for AR order selection is to choose as \\(D\\) the number of significant partial autocorrelations.\n\n\n\n\n\n\nAction\n\n\n\n\nCompute and plot the PACF for your residual series.\nDetermine the order of autoregression: how many partial autocorrelations are outside the confidence bands?\nCompare with your neighbor.\n\n\n\n\n\nFitting an AR model\nWe could at this point fit our full model in one go. However, let’s instead start by fitting the error model directly to the residuals.\n\n# fit error model\nfit_resid &lt;- Arima(fit_df$.resid,\n      order = c(2, 0, 0),\n      include.mean = F,\n      method = 'ML')\n\nFrom the fitted model, we can obtain the theoretical autocorrelation and compare it with the sample:\n\nresid_acf_fitted &lt;- ARMAacf(ar = coef(fit_resid), \n                            lag.max = 25)\n\nplot(resid_acf, main = '')\nlines(resid_acf_fitted, col = 'red')\n\n\n\n\nForecasting based on this model is straightforward. There is a predict method:\n\npredict(fit_resid, n.ahead = 5)\n\n$pred\nTime Series:\nStart = 644 \nEnd = 648 \nFrequency = 1 \n[1] 1.598966 1.500999 1.380972 1.253629 1.127627\n\n$se\nTime Series:\nStart = 644 \nEnd = 648 \nFrequency = 1 \n[1] 0.1988213 0.3622029 0.5026578 0.6179443 0.7102430\n\n\nBut the forecast package returns predictions a bit more neatly arranged:\n\nforecast(fit_resid, h = 5)\n\n    Point Forecast     Lo 80    Hi 80       Lo 95    Hi 95\n644       1.598966 1.3441662 1.853766  1.20928339 1.988649\n645       1.500999 1.0368175 1.965181  0.79109459 2.210904\n646       1.380972 0.7367903 2.025154  0.39578099 2.366163\n647       1.253629 0.4617016 2.045557  0.04248054 2.464778\n648       1.127627 0.2174142 2.037840 -0.26442347 2.519678\n\n\nNow we could mimic what was done in our first class on this data and add the residual forecasts to predictions from the regression model, but it is much more efficient to fit a single model (rather than two) that captures both the regression and the error part.\n\n\n\n\n\n\nTakeaway\n\n\n\nAn autoregressive model (and time series models generally) may look like it’s a model describing the conditional mean of a series in the present given the past. However, it’s really about the correlation structure.\n\n\n\n\nRegression with AR errors\nNow that we’ve identified the error model we can fit the full model simultaneously using maximum likelihood.\nWe would typically write this model not as a two-level model but as:\n\\[\nY_{t} = f(t) + \\epsilon_t\n\\;,\\qquad\n\\epsilon_t \\sim AR(2)\n\\] Writing the model more explicitly in terms of the basis expansion for \\(f\\):\n\\[\nY_{t} = \\beta_0 + \\sum_{j = 1}^4 \\beta_j f_j(t) + \\epsilon_t\n\\;,\\qquad\n\\epsilon_t \\sim AR(2)\n\\]\nThis can also be fitted using Arima(). Because we want to do forecasting, let’s hold out the last month or so of data.\n\n# determine a point at which to cut the series\ncutpt &lt;- nrow(xreg) - 30\n\n# training series\ny_train &lt;- y[1:cutpt]\nx_train &lt;- xreg[1:cutpt, ]\n\n# fit the model\nfit_full &lt;- Arima(y_train,\n                  order = c(2, 0, 0),\n                  xreg = x_train,\n                  include.mean = F,\n                  method = 'ML')\n\nbroom::tidy(fit_full) %&gt;% knitr::kable()\n\n\n\n\nterm\nestimate\nstd.error\n\n\n\n\nar1\n1.5192225\n0.0332166\n\n\nar2\n-0.5663175\n0.0332869\n\n\nconst\n60.3315298\n3.4003353\n\n\nsin1\n-59.3071214\n3.5694270\n\n\ncos1\n-61.6934230\n3.1327989\n\n\nsin2\n23.9546701\n3.2289715\n\n\ncos2\n16.1671305\n3.2584591\n\n\n\n\n\n\n\n\n\n\n\nWhy no mean?\n\n\n\nThe argument include.mean = F tells Arima() not to ‘de-mean’ the residual series, i.e., fit an intercept.\nThe reason for this choice is that the regressors include a constant term, so an intercept is already being fit at the level of the regression model. Including another intercept produces an unidentifiable model.\nIf you’re curious, try changing to include.mean = T. R should throw an error.\n\n\n\n\n\n\n\n\nAction\n\n\n\nCompare the (non-AR parameter) estimates from this model with the coefficient estimates assuming \\(iid\\) errors that we fit in selecting an error model (i.e., fit ). Are they close? Why are they different at all?\n\n\n\n\nForecasts\nNow, to forecast, we need to supply future regressor values. Luckily for us, the regressors are deterministic functions of day of the year, so that is easy. (If we had covariates, however, this might be a potential problem.)\n\n# testing series\ny_test &lt;- y[(cutpt + 1):nrow(xreg)]\nx_test &lt;- xreg[(cutpt + 1):nrow(xreg), ]\n\npreds &lt;- forecast(fit_full, \n                  h = nrow(x_test), \n                  xreg = x_test)\n\npreds %&gt;% as_tibble() %&gt;% head()\n\n# A tibble: 6 × 5\n  `Point Forecast` `Lo 80` `Hi 80` `Lo 95` `Hi 95`\n             &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1           -0.189  -0.450  0.0722  -0.588   0.210\n2           -0.266  -0.741  0.209   -0.992   0.460\n3           -0.356  -1.01   0.301   -1.36    0.649\n4           -0.452  -1.26   0.354   -1.68    0.781\n5           -0.547  -1.47   0.376   -1.96    0.865\n6           -0.641  -1.66   0.374   -2.19    0.912\n\n\nTo plot the forecasts, bind the fitted values and predictions with the original data and visualize:\n\nfig_forecast &lt;- site_data %&gt;%\n  dplyr::select(date, temp) %&gt;%\n  bind_cols(pred = c(fit_full$fitted, preds$mean),\n            status = c(rep('obs', nrow(x_train)),\n                       rep('pred', nrow(x_test)))) %&gt;%\n  filter(date &gt;= ymd('2018-01-01')) %&gt;% # adjust if needed\n  ggplot(aes(x = date)) +\n  geom_path(aes(y = temp, linetype = status)) +\n  geom_path(aes(y = pred), color = 'blue', alpha = 0.5)\n\nfig_forecast\n\n\n\n\n\nci_df &lt;- site_data %&gt;% \n  slice_tail(n = nrow(x_test)) %&gt;% \n  dplyr::select(date) %&gt;%\n  bind_cols(lwr = preds$lower[, 2],\n            upr = preds$upper[, 2])\n\nfig_forecast +\n  geom_ribbon(aes(x = date, ymin = lwr, ymax = upr),\n              alpha = 0.3, fill = 'blue',\n              data = ci_df)\n\n\n\n\n\n\n\n\n\n\nAction\n\n\n\nRemake the plot of forecasts with uncertainty quantification and zoom in on the forecasted region (by filtering out more of the dates when creating the base layer fig_forecast).\nCompare and contrast with your neighbor.\n\nHow good are the forecasts and for how long?\nWhat happens to the uncertainty for longer-term forecasts?"
  },
  {
    "objectID": "pstat197a/labs/lab2-tidyverse/lab2-tidyverse.html",
    "href": "pstat197a/labs/lab2-tidyverse/lab2-tidyverse.html",
    "title": "Tidyverse basics",
    "section": "",
    "text": "Read through the R Basics section and then complete all actions in the Tidyverse basics section. This lab is for your own benefit and no submission is expected.\nObjectives:"
  },
  {
    "objectID": "pstat197a/labs/lab2-tidyverse/lab2-tidyverse.html#data-types",
    "href": "pstat197a/labs/lab2-tidyverse/lab2-tidyverse.html#data-types",
    "title": "Tidyverse basics",
    "section": "Data types",
    "text": "Data types\nThere are five main data types in R.\nNumeric (double- or single-precision floating point) data represent real numbers. Numeric data are abbreviated num and by default are stored as double-precision floating point.\n\n# a number\n4.5\n\n[1] 4.5\n\n# check structure\nstr(4.5)\n\n num 4.5\n\n# stored as double\nis.double(4.5)\n\n[1] TRUE\n\n\nInteger data are integers. For the most part they behave like numeric data, except occupy less memory, which can in some cases be convenient. To distinguish integers from doubles, R uses a trailing L after values; the data type is abbreviated int.\n\n# an integer\n4L\n\n[1] 4\n\n# check structure\nstr(4L)\n\n int 4\n\n\nLogical data are binary and represented in R as having values TRUE and FALSE. They are abbreviated logi in R. Often they are automatically coerced to integer data with values 0 (false) and 1 (true) to perform arithmetic and other operations.\n\n# logical value\nTRUE\n\n[1] TRUE\n\n# check structure\nstr(TRUE)\n\n logi TRUE\n\n# arithmetic\nTRUE + FALSE\n\n[1] 1\n\n# check structure\nstr(FALSE + FALSE)\n\n int 0\n\n\nCharacter data represent strings of text and are sometimes called ‘strings’. They are abbreviated chr in R and values are surrounded by quotation marks; this distinguishes, for example, the character 4 from the number 4. Single quotations can be used to input strings as well as double quotations. Arithmetic is not possible with strings for obvious reasons.\n\n# a character string\n'yay'\n\n[1] \"yay\"\n\n# check structure\nstr('yay')\n\n chr \"yay\"\n\n# string arithmetic won't work\n'4' + '1'\n\nError in \"4\" + \"1\": non-numeric argument to binary operator\n\n# but can be performed after coercing character to string\nas.numeric('4') + as.numeric('1')\n\n[1] 5\n\n\nFactor data represent categorical variables. In R these are encoded numerically according to the number of ‘levels’ of the factor, which represent the unique values of the categorical variable, and each level is labeled. R will print the labels, not the levels, of factors; the data type is abbreviated fct.\n\n# a factor\nfactor(1, levels = c(1, 2), labels = c('blue', 'red'))\n\n[1] blue\nLevels: blue red\n\n# less verbose definition\nfactor('blue', levels = c('blue', 'red'))\n\n[1] blue\nLevels: blue red\n\n# check structure\nstr(factor('blue', levels = c('blue', 'red')))\n\n Factor w/ 2 levels \"blue\",\"red\": 1\n\n\nUsually factors won’t be defined explicitly, but instead interpreted from character data. The levels and labels of factors can be manipulated using a variety of helper functions."
  },
  {
    "objectID": "pstat197a/labs/lab2-tidyverse/lab2-tidyverse.html#object-classes",
    "href": "pstat197a/labs/lab2-tidyverse/lab2-tidyverse.html#object-classes",
    "title": "Tidyverse basics",
    "section": "Object classes",
    "text": "Object classes\nThe most basic type of object in R is a vector. Vectors are concatenations of data values of the same type. They are defined using the concatenation operator c() and are indexed by consecutive integers; subvectors can be retrieved by specifying the indices between square brackets.\n\n# numeric vector\nc(1, 4, 7)\n\n[1] 1 4 7\n\n# character vector\nc('blue', 'red')\n\n[1] \"blue\" \"red\" \n\n# indexing\nc(1, 4, 7)[1]\n\n[1] 1\n\nc(1, 4, 7)[2]\n\n[1] 4\n\nc(1, 4, 7)[3]\n\n[1] 7\n\nc(1, 4, 7)[2:3]\n\n[1] 4 7\n\nc(1, 4, 7)[c(1, 3)]\n\n[1] 1 7\n\n\nUsually objects are assigned names for easy retrieval. Vectors will not show any special object class if the structure is examined; str() will simply return the data type, index range, and the values.\n\n# assign a name\nmy_vec &lt;- c(1, 4, 7)\n\n# check structure\nstr(my_vec)\n\n num [1:3] 1 4 7\n\n\nNext up in complexity are arrays. These are blocks of data values of the same type indexed along two or more dimensions. For arrays, str() will return the data type, index structure, and data values; when printed directly, data values are arranged according to the indexing.\n\n# an array\nmy_ary &lt;- array(data = c(1, 2, 3, 4, 5, 6, 7, 8), \n           dim = c(2, 4))\n\nmy_ary\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    3    5    7\n[2,]    2    4    6    8\n\nstr(my_ary)\n\n num [1:2, 1:4] 1 2 3 4 5 6 7 8\n\n# another array\nmy_oth_ary &lt;- array(data = c(1, 2, 3, 4, 5, 6, 7, 8), \n           dim = c(2, 2, 2))\n\nmy_oth_ary\n\n, , 1\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\n, , 2\n\n     [,1] [,2]\n[1,]    5    7\n[2,]    6    8\n\nstr(my_oth_ary)\n\n num [1:2, 1:2, 1:2] 1 2 3 4 5 6 7 8\n\n\nFor arrays, elements can be retrieved by index coordinates, and slices can be retrieved by leaving index positions blank, which will return all elements along the corresponding indices.\n\n# one element\nmy_ary[1, 2]\n\n[1] 3\n\n# one element\nmy_oth_ary[1, 2, 1]\n\n[1] 3\n\n# a slice (second row)\nmy_ary[2, ]\n\n[1] 2 4 6 8\n\n# a slice (first layer)\nmy_oth_ary[ , , 1]\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\n\nNext there are lists, which are perhaps the most flexible data structure. A list is an indexed collection of any objects.\n\n# a list\nlist('cat', c(1, 4, 7), TRUE)\n\n[[1]]\n[1] \"cat\"\n\n[[2]]\n[1] 1 4 7\n\n[[3]]\n[1] TRUE\n\n# a named list\nlist(animal = 'cat',\n     numbers = c(1, 4, 7),\n     short = TRUE)\n\n$animal\n[1] \"cat\"\n\n$numbers\n[1] 1 4 7\n\n$short\n[1] TRUE\n\n\nList elements can be retrieved by index in double square brackets, or by name.\n\n# assign a name\nmy_lst &lt;- list(animal = 'cat',\n               numbers = c(1, 4, 7),\n               short = TRUE)\n\n# check structure\nstr(my_lst)\n\nList of 3\n $ animal : chr \"cat\"\n $ numbers: num [1:3] 1 4 7\n $ short  : logi TRUE\n\n# retrieve an element\nmy_lst[[1]]\n\n[1] \"cat\"\n\n# equivalent\nmy_lst$animal\n\n[1] \"cat\"\n\n\nFinally, data frames are type-heterogeneous lists of vectors of equal length. More informally, they are 2D arrays with columns of differing data types. str() will essentially show the list structure; but when printed, data frames will appear arranged in a table.\n\n# a data frame\nmy_df &lt;- data.frame(animal = c('cat', 'hare', 'tortoise'),\n                    has.fur = c(TRUE, TRUE, FALSE),\n                    weight.lbs = c(9.1, 8.2, 22.7))\n\nstr(my_df)\n\n'data.frame':   3 obs. of  3 variables:\n $ animal    : chr  \"cat\" \"hare\" \"tortoise\"\n $ has.fur   : logi  TRUE TRUE FALSE\n $ weight.lbs: num  9.1 8.2 22.7\n\nmy_df\n\n    animal has.fur weight.lbs\n1      cat    TRUE        9.1\n2     hare    TRUE        8.2\n3 tortoise   FALSE       22.7\n\n\nThe data frame is the standard object type for representing datasets in R. For the most part, modern computing in R is designed around the data frame."
  },
  {
    "objectID": "pstat197a/labs/lab2-tidyverse/lab2-tidyverse.html#packages",
    "href": "pstat197a/labs/lab2-tidyverse/lab2-tidyverse.html#packages",
    "title": "Tidyverse basics",
    "section": "Packages",
    "text": "Packages\nR packages are add-ons that can include special functions, datasets, object classes, and the like. They are published software and can be installed using install.packages('PACKAGE NAME') and, once installed, loaded via library('PACKAGE NAME') or require('PACKAGE NAME')."
  },
  {
    "objectID": "pstat197a/labs/lab2-tidyverse/lab2-tidyverse.html#concepts",
    "href": "pstat197a/labs/lab2-tidyverse/lab2-tidyverse.html#concepts",
    "title": "Tidyverse basics",
    "section": "Concepts",
    "text": "Concepts\nThe tidyverse is a collection of packages for data manipulation, visualization, and statistical modeling. Some are specialized, such as forcats or lubridate, which contain functions for manipulating factors and dates and times, respectively. The packages share some common underyling principles.\n\nPackages are built around the data frame\nFunctions are designed to work with the pipe operator %&gt;%\nPackages facilitate readable code\n\nThe tidyverse facilitates programming in readable sequences of steps that are performed on dataframe. For example:\n\nmy_df %&gt;% STEP1() %&gt;% STEP2() %&gt;% STEP3()\n\nIf it helps, imagine that step 1 is defining a new variable, step 2 is selecting a subset of columns, and step 3 is fitting a model of some kind."
  },
  {
    "objectID": "pstat197a/labs/lab2-tidyverse/lab2-tidyverse.html#tibbles",
    "href": "pstat197a/labs/lab2-tidyverse/lab2-tidyverse.html#tibbles",
    "title": "Tidyverse basics",
    "section": "Tibbles",
    "text": "Tibbles\ntidyverse packages leverage a slight generalization of the data frame called a tibble. For the most part, tibbles behave as data frames do, but they are slightly more flexible in ways you’ll encounter later.\nFor now, think of a tibble as just another name for a data frame."
  },
  {
    "objectID": "pstat197a/labs/lab2-tidyverse/lab2-tidyverse.html#the-pipe-operator",
    "href": "pstat197a/labs/lab2-tidyverse/lab2-tidyverse.html#the-pipe-operator",
    "title": "Tidyverse basics",
    "section": "The pipe operator %>%",
    "text": "The pipe operator %&gt;%\nIn short, x %&gt;% f(y) is equivalent to f(x, y) .\nIn other words, the pipe operator ‘pipes’ the result of the left-hand operation into the first argument of the right-hand function.\n\n# a familiar example\nmy_vec &lt;- c(1, 2, 5) \nstr(my_vec)\n\n num [1:3] 1 2 5\n\n# use the pipe operator instead\nmy_vec %&gt;% str()\n\n num [1:3] 1 2 5"
  },
  {
    "objectID": "pstat197a/labs/lab2-tidyverse/lab2-tidyverse.html#dplyr-verbs",
    "href": "pstat197a/labs/lab2-tidyverse/lab2-tidyverse.html#dplyr-verbs",
    "title": "Tidyverse basics",
    "section": "dplyr verbs",
    "text": "dplyr verbs\nThe dplyr package contains functions for manipulating data frames (tibbles). The functions are named with verbs that describe common operations.\n\nCore verbs\n\n\n\n\n\n\nAction\n\n\n\nFor each verb listed below, copy the code chunk into your script and execute.\nGo through the list with your neighbor and check your understanding by describing what the code example accomplishes.\n\n\nfilter – filter the rows of a data frame according to a condition and return a subset of rows meeting that condition\n\n# filter rows\nbackground %&gt;%\n  filter(math.comf &gt; 3)\n\n# A tibble: 37 × 30\n   response.id prog.prof prog.comf math.prof math.comf stat.prof stat.comf\n         &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;\n 1           5 Int               3 Adv               4 Int               3\n 2           7 Int               3 Adv               4 Int               3\n 3           8 Adv               4 Adv               5 Adv               4\n 4          12 Int               5 Int               4 Int               4\n 5          13 Int               4 Adv               4 Adv               4\n 6          15 Adv               3 Adv               4 Adv               4\n 7          17 Int               3 Int               4 Int               4\n 8          18 Int               4 Adv               5 Adv               5\n 9          19 Adv               5 Adv               5 Adv               5\n10          20 Adv               4 Int               4 Adv               5\n# … with 27 more rows, and 23 more variables: updv.num &lt;chr&gt;, dom &lt;chr&gt;,\n#   consent &lt;chr&gt;, PSTAT100 &lt;chr&gt;, `PSTAT120A-B` &lt;chr&gt;, PSTAT126 &lt;chr&gt;,\n#   PSTAT127 &lt;chr&gt;, PSTAT134 &lt;chr&gt;, CS9 &lt;chr&gt;, PSTAT131 &lt;chr&gt;, PSTAT174 &lt;chr&gt;,\n#   LING104 &lt;chr&gt;, LING105 &lt;chr&gt;, PSTAT115 &lt;chr&gt;, PSTAT122 &lt;chr&gt;,\n#   `PSTAT160A-B` &lt;chr&gt;, CS16 &lt;chr&gt;, `CS5A-B` &lt;chr&gt;, POLS15 &lt;chr&gt;,\n#   LING110 &lt;chr&gt;, `CS130A-B` &lt;chr&gt;, `CS165A-B` &lt;chr&gt;, rsrch &lt;lgl&gt;\n\n\nselect – select a subset of columns from a data frame\n\n# select a column\nbackground %&gt;%\n  select(math.comf)\n\n# A tibble: 59 × 1\n   math.comf\n       &lt;dbl&gt;\n 1         3\n 2         3\n 3         3\n 4         3\n 5         4\n 6         4\n 7         5\n 8         3\n 9         3\n10         4\n# … with 49 more rows\n\n\npull – extract a single column from a data frame\n\n# pull a column\nbackground %&gt;%\n  pull(rsrch)\n\n [1] FALSE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE FALSE  TRUE FALSE FALSE\n[13] FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE FALSE FALSE  TRUE FALSE  TRUE\n[25] FALSE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE\n[37] FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE\n[49]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE  TRUE FALSE  TRUE FALSE\n\n\nmutate – define a new column as a function of existing columns\n\n# define a new variable\nbackground %&gt;%\n  mutate(avg.comf = (math.comf + prog.comf + stat.comf)/3)\n\n# A tibble: 59 × 31\n   response.id prog.prof prog.comf math.prof math.comf stat.prof stat.comf\n         &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;\n 1           1 Adv               5 Int               3 Adv               4\n 2           2 Int               5 Int               3 Int               4\n 3           3 Int               3 Int               3 Int               4\n 4           4 Int               3 Int               3 Int               3\n 5           5 Int               3 Adv               4 Int               3\n 6           7 Int               3 Adv               4 Int               3\n 7           8 Adv               4 Adv               5 Adv               4\n 8           9 Int               3 Int               3 Int               3\n 9          10 Int               4 Int               3 Adv               4\n10          12 Int               5 Int               4 Int               4\n# … with 49 more rows, and 24 more variables: updv.num &lt;chr&gt;, dom &lt;chr&gt;,\n#   consent &lt;chr&gt;, PSTAT100 &lt;chr&gt;, `PSTAT120A-B` &lt;chr&gt;, PSTAT126 &lt;chr&gt;,\n#   PSTAT127 &lt;chr&gt;, PSTAT134 &lt;chr&gt;, CS9 &lt;chr&gt;, PSTAT131 &lt;chr&gt;, PSTAT174 &lt;chr&gt;,\n#   LING104 &lt;chr&gt;, LING105 &lt;chr&gt;, PSTAT115 &lt;chr&gt;, PSTAT122 &lt;chr&gt;,\n#   `PSTAT160A-B` &lt;chr&gt;, CS16 &lt;chr&gt;, `CS5A-B` &lt;chr&gt;, POLS15 &lt;chr&gt;,\n#   LING110 &lt;chr&gt;, `CS130A-B` &lt;chr&gt;, `CS165A-B` &lt;chr&gt;, rsrch &lt;lgl&gt;,\n#   avg.comf &lt;dbl&gt;\n\n\nThese operations can be chained together, for example:\n\n# sequence of verbs\nbackground %&gt;%\n  filter(stat.prof == 'Adv') %&gt;%\n  mutate(avg.comf = (math.comf + prog.comf + stat.comf)/3) %&gt;%\n  select(avg.comf, rsrch) \n\n# A tibble: 36 × 2\n   avg.comf rsrch\n      &lt;dbl&gt; &lt;lgl&gt;\n 1     4    FALSE\n 2     4.33 TRUE \n 3     3.67 FALSE\n 4     4    FALSE\n 5     3.67 FALSE\n 6     4.67 TRUE \n 7     5    TRUE \n 8     4.33 TRUE \n 9     4.33 TRUE \n10     4    TRUE \n# … with 26 more rows\n\n\n\n\n\n\n\n\n\nAction\n\n\n\n\nWrite a chain of verbs in order to find the proficiency ratings of all respondents with research experience and 6-8 upper division courses.\nWrite a chain of verbs in order to find the proficiency ratings of all respondents without research experience and the same number of upper division courses\nCompare results and discuss with your neighbor: do these suggest any patterns?\n\n\n\n\n\nSummaries\nSummaries are easily computed across rows using summarize() . So if for example we want to use the filtering and selection from before to find the proportion of advanced students in statistics with research experience, use:\n\n# a summary\nbackground %&gt;%\n  filter(stat.prof == 'Adv') %&gt;%\n  mutate(avg.comf = (math.comf + prog.comf + stat.comf)/3) %&gt;%\n  select(avg.comf, rsrch) %&gt;%\n  summarize(prop.rsrch = mean(rsrch))\n\n# A tibble: 1 × 1\n  prop.rsrch\n       &lt;dbl&gt;\n1      0.611\n\n# equivalent\nbackground %&gt;%\n  filter(stat.prof == 'Adv') %&gt;%\n  mutate(avg.comf = (math.comf + prog.comf + stat.comf)/3) %&gt;%\n  select(avg.comf, rsrch) %&gt;%\n  pull(rsrch) %&gt;%\n  mean()\n\n[1] 0.6111111\n\n\nThe advantage of summarize , however, is that multiple summaries can be computed at once:\n\nbackground %&gt;%\n  filter(stat.prof == 'Adv') %&gt;%\n  mutate(avg.comf = (math.comf + prog.comf + stat.comf)/3) %&gt;%\n  select(avg.comf, rsrch) %&gt;%\n  summarize(prop.rsrch = mean(rsrch),\n            med.comf = median(avg.comf))\n\n# A tibble: 1 × 2\n  prop.rsrch med.comf\n       &lt;dbl&gt;    &lt;dbl&gt;\n1      0.611        4\n\n\nThe variant summarize_all computes the same summary across all columns. (Notice the use of the helper verb contains() to select all columns containing a particular string.)\n\n# average comfort levels across all students\nbackground %&gt;%\n  select(contains('comf')) %&gt;%\n  summarise_all(.funs = mean)\n\n# A tibble: 1 × 3\n  prog.comf math.comf stat.comf\n      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1      3.97      3.85      4.08\n\n\nGrouped summaries are summaries computed separately among subsets of observations. To define a grouping structure using an existing column, use group_by() . Notice the ‘groups’ attribute printed with the output.\n\n# create a grouping\nbackground %&gt;%\n  group_by(stat.prof)\n\n# A tibble: 59 × 30\n# Groups:   stat.prof [3]\n   response.id prog.prof prog.comf math.prof math.comf stat.prof stat.comf\n         &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;\n 1           1 Adv               5 Int               3 Adv               4\n 2           2 Int               5 Int               3 Int               4\n 3           3 Int               3 Int               3 Int               4\n 4           4 Int               3 Int               3 Int               3\n 5           5 Int               3 Adv               4 Int               3\n 6           7 Int               3 Adv               4 Int               3\n 7           8 Adv               4 Adv               5 Adv               4\n 8           9 Int               3 Int               3 Int               3\n 9          10 Int               4 Int               3 Adv               4\n10          12 Int               5 Int               4 Int               4\n# … with 49 more rows, and 23 more variables: updv.num &lt;chr&gt;, dom &lt;chr&gt;,\n#   consent &lt;chr&gt;, PSTAT100 &lt;chr&gt;, `PSTAT120A-B` &lt;chr&gt;, PSTAT126 &lt;chr&gt;,\n#   PSTAT127 &lt;chr&gt;, PSTAT134 &lt;chr&gt;, CS9 &lt;chr&gt;, PSTAT131 &lt;chr&gt;, PSTAT174 &lt;chr&gt;,\n#   LING104 &lt;chr&gt;, LING105 &lt;chr&gt;, PSTAT115 &lt;chr&gt;, PSTAT122 &lt;chr&gt;,\n#   `PSTAT160A-B` &lt;chr&gt;, CS16 &lt;chr&gt;, `CS5A-B` &lt;chr&gt;, POLS15 &lt;chr&gt;,\n#   LING110 &lt;chr&gt;, `CS130A-B` &lt;chr&gt;, `CS165A-B` &lt;chr&gt;, rsrch &lt;lgl&gt;\n\n\nSometimes it can be helpful to simply count the observations in each group:\n\n# count observations\nbackground %&gt;%\n  group_by(stat.prof) %&gt;%\n  count()\n\n# A tibble: 3 × 2\n# Groups:   stat.prof [3]\n  stat.prof     n\n  &lt;chr&gt;     &lt;int&gt;\n1 Adv          36\n2 Beg           2\n3 Int          21\n\n\nTo compute a grouped summary, first group the data frame and then specify the summary of interest:\n\n# a grouped summary\nbackground %&gt;%\n  group_by(stat.prof) %&gt;%\n  select(contains('.comf')) %&gt;%\n  summarize_all(.funs = mean)\n\n# A tibble: 3 × 4\n  stat.prof prog.comf math.comf stat.comf\n  &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 Adv            4.06      4.14      4.39\n2 Beg            3.5       2.5       2.5 \n3 Int            3.86      3.48      3.71\n\n\n\n\n\n\n\n\nAction\n\n\n\nGrouped summaries\n\nCompute the median comfort level of all students in each subject area.\nCompute the median comfort level of all students in each subject area after grouping by number of upper division classes taken.\nCompare and discuss with your neighbor: do you notice any interesting patterns?"
  },
  {
    "objectID": "pstat197a/labs/lab2-tidyverse/lab2-tidyverse.html#tidyr-verbs",
    "href": "pstat197a/labs/lab2-tidyverse/lab2-tidyverse.html#tidyr-verbs",
    "title": "Tidyverse basics",
    "section": "tidyr verbs",
    "text": "tidyr verbs\nIn general, tidyr verbs reshape data frames in various ways. For now, we’ll just cover two tidyr verbs.\nSuppose we want to calculate multiple summaries of multiple variables using the techniques above. By default, the output is one row with one column for each summary/variable combination:\n\n# many variables, many summaries\ncomf_sum &lt;- background %&gt;%\n  select(contains('comf')) %&gt;%\n  summarise_all(.funs = list(mean = mean, \n                             median = median,\n                             min = min, \n                             max = max))\n\ncomf_sum\n\n# A tibble: 1 × 12\n  prog.comf_mean math.comf_mean stat.comf_mean prog.comf_median math.comf_median\n           &lt;dbl&gt;          &lt;dbl&gt;          &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;\n1           3.97           3.85           4.08                4                4\n# … with 7 more variables: stat.comf_median &lt;dbl&gt;, prog.comf_min &lt;dbl&gt;,\n#   math.comf_min &lt;dbl&gt;, stat.comf_min &lt;dbl&gt;, prog.comf_max &lt;dbl&gt;,\n#   math.comf_max &lt;dbl&gt;, stat.comf_max &lt;dbl&gt;\n\n\nIt would be much better to reshape this into a table. gather will reshape the data frame from wide format to long format by ‘gathering’ the columns together.\n\n# gather columns into long format\ncomf_sum %&gt;% gather(stat, val) \n\n# A tibble: 12 × 2\n   stat               val\n   &lt;chr&gt;            &lt;dbl&gt;\n 1 prog.comf_mean    3.97\n 2 math.comf_mean    3.85\n 3 stat.comf_mean    4.08\n 4 prog.comf_median  4   \n 5 math.comf_median  4   \n 6 stat.comf_median  4   \n 7 prog.comf_min     3   \n 8 math.comf_min     2   \n 9 stat.comf_min     2   \n10 prog.comf_max     5   \n11 math.comf_max     5   \n12 stat.comf_max     5   \n\n\nThis is a little better, but it would be more legible in a 2x2 table. We can separate the ‘stat’ variable that has the column names into two columns:\n\n# separate into rows and columns\ncomf_sum %&gt;%\n  gather(stat, val) %&gt;%\n  separate(stat, into = c('variable', 'stat'), sep = '_') \n\n# A tibble: 12 × 3\n   variable  stat     val\n   &lt;chr&gt;     &lt;chr&gt;  &lt;dbl&gt;\n 1 prog.comf mean    3.97\n 2 math.comf mean    3.85\n 3 stat.comf mean    4.08\n 4 prog.comf median  4   \n 5 math.comf median  4   \n 6 stat.comf median  4   \n 7 prog.comf min     3   \n 8 math.comf min     2   \n 9 stat.comf min     2   \n10 prog.comf max     5   \n11 math.comf max     5   \n12 stat.comf max     5   \n\n\nAnd then spread the stat column over a few rows, resulting in a table where the rows are the variables and the columns are the summaries:\n\n# spread into table\ncomf_sum %&gt;%\n  gather(stat, val) %&gt;%\n  separate(stat, into = c('variable', 'stat'), sep = '_') %&gt;%\n  spread(stat, val)\n\n# A tibble: 3 × 5\n  variable    max  mean median   min\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 math.comf     5  3.85      4     2\n2 prog.comf     5  3.97      4     3\n3 stat.comf     5  4.08      4     2"
  },
  {
    "objectID": "pstat197a/labs/lab2-tidyverse/lab2-tidyverse.html#ggplot",
    "href": "pstat197a/labs/lab2-tidyverse/lab2-tidyverse.html#ggplot",
    "title": "Tidyverse basics",
    "section": "ggplot",
    "text": "ggplot\nThe ggplot package is for data visualization. The syntax takes some getting used to if you haven’t seen it before. We’ll just look at one example.\nSuppose we want to summarize the prior coursework in the class.\n\n# summary of classes taken\nclasses &lt;- background %&gt;%\n  select(11:29) %&gt;%\n  mutate_all(~factor(.x, levels = c('no', 'yes'))) %&gt;%\n  mutate_all(~as.numeric(.x) - 1) %&gt;%\n  summarize_all(mean) %&gt;%\n  gather(class, proportion)\n\nclasses\n\n# A tibble: 19 × 2\n   class       proportion\n   &lt;chr&gt;            &lt;dbl&gt;\n 1 PSTAT100        0.271 \n 2 PSTAT120A-B     0.983 \n 3 PSTAT126        0.932 \n 4 PSTAT127        0.169 \n 5 PSTAT134        0.305 \n 6 CS9             0.576 \n 7 PSTAT131        0.424 \n 8 PSTAT174        0.169 \n 9 LING104         0.0339\n10 LING105         0.0339\n11 PSTAT115        0.102 \n12 PSTAT122        0.356 \n13 PSTAT160A-B     0.458 \n14 CS16            0.492 \n15 CS5A-B          0.0508\n16 POLS15          0.0169\n17 LING110         0.0169\n18 CS130A-B        0.0508\n19 CS165A-B        0.0339\n\n\nWe could report the results in a table, in which case perhaps arranging in descending order may be helpful:\n\nclasses %&gt;% arrange(desc(proportion))\n\n# A tibble: 19 × 2\n   class       proportion\n   &lt;chr&gt;            &lt;dbl&gt;\n 1 PSTAT120A-B     0.983 \n 2 PSTAT126        0.932 \n 3 CS9             0.576 \n 4 CS16            0.492 \n 5 PSTAT160A-B     0.458 \n 6 PSTAT131        0.424 \n 7 PSTAT122        0.356 \n 8 PSTAT134        0.305 \n 9 PSTAT100        0.271 \n10 PSTAT127        0.169 \n11 PSTAT174        0.169 \n12 PSTAT115        0.102 \n13 CS5A-B          0.0508\n14 CS130A-B        0.0508\n15 LING104         0.0339\n16 LING105         0.0339\n17 CS165A-B        0.0339\n18 POLS15          0.0169\n19 LING110         0.0169\n\n\nLet’s say we’d rather plot this data. We’ll put the course number on one axis and the proportion of students who took it on the other.\n\n# plot it\nclasses %&gt;%\n  ggplot(aes(x = proportion, y = class)) +\n  geom_point()\n\n\n\n\nThese commands work by defining plot layers. In the chunk above, the first argument to ggplot() is the data. Then, aes() defines an ‘aesthetic mapping’ of the columns of the input data frame to graphical elements. This defines a set of axes. Then, a layer of points is added to the plot with geom_point() ; no arguments are needed because the geometric object (‘geom’) inherits attributes (x and y coordinates) from the aesthetic mapping.\nAgain we might prefer to arrange the classes by descending order in proportion.\n\nfig &lt;- classes %&gt;%\n  ggplot(aes(x = proportion, y = reorder(class, proportion))) +\n  geom_point()\n\nfig\n\n\n\n\nAnd perhaps fix the plot labels:\n\n# adjust labels\nfig + labs(x = 'proportion of class', y = '')\n\n\n\n\nNotice that ggplot allows for a plot to be stored by name and then further modified with additional layers."
  },
  {
    "objectID": "pstat197a/labs/lab2-tidyverse/lab2-tidyverse.html#checklist",
    "href": "pstat197a/labs/lab2-tidyverse/lab2-tidyverse.html#checklist",
    "title": "Tidyverse basics",
    "section": "Checklist",
    "text": "Checklist\n\nAll actions were completed.\nAll code chunks were copied into your script.\nYour script is saved in a lab subfolder of your class directory with an associated project."
  },
  {
    "objectID": "pstat197a/about-syllabus.html",
    "href": "pstat197a/about-syllabus.html",
    "title": "Course syllabus",
    "section": "",
    "text": "Concurrent course listing: PSTAT197A and CMPSC190DD are held concurrently; enrollment is by instructor consent and admitted students may enroll under either listing. The course content, expectations, assessments, and course policies are identical for students enrolled in either course.\nCatalog description: Introduction to research skills. Discussion of current research trends, writing literature reviews, etc. Students will be required to present materials reflecting their interests, which will be critically appraised for both content and presentation. Emphasis will be placed on aiding students to acquire a high-level of professionalism. Prerequisite: PSTAT126."
  },
  {
    "objectID": "pstat197a/about-syllabus.html#learning-outcomes",
    "href": "pstat197a/about-syllabus.html#learning-outcomes",
    "title": "Course syllabus",
    "section": "Learning outcomes",
    "text": "Learning outcomes\nThis course emphasizes collaborative, interactive, and hands-on learning. Instruction in PSTAT197A will support all students in:\n\nusing modern technology and version control to collaborate efficiently on programming for data science projects;\nrecognizing and articulating problem patterns based on data semantics and one or more research questions;\nidentifying and accessing resources to aid in learning independently about methodology and/or application domains pertinent to a problem of interest;\ncommunicating data analysis and/or research findings in a project team setting and to a small audience of peers.\n\nCourse staff are committed to creating an inclusive learning environment. Data science involves a combination of computing, statistics and probability, and domain expertise, as well as use of technology and narrative communication and storytelling, and no one person should expect to be an expert in all of these areas. Course staff recognize this fact that core competencies vary considerably, acknowledge that each student has particular strengths and weaknesses and interests, and make their best effort to avoid promoting one skill set over others in the practice of data science."
  },
  {
    "objectID": "pstat197a/about-syllabus.html#expectations-and-assessments",
    "href": "pstat197a/about-syllabus.html#expectations-and-assessments",
    "title": "Course syllabus",
    "section": "Expectations and assessments",
    "text": "Expectations and assessments\nMuch of the course is designed around group activity and discussion. Students are therefore expected to:\n\nprepare for class meetings in advance by completing any assigned reading or activity;\nattend and actively participate in class meetings and section meetings;\nprovide meaningful, timely, and concrete contributions to group activities.\n\nStudents having any difficulty in meeting these expectations should raise the issue(s) promptly with the instructor.\nQualitative feedback is emphasized over numerical scores. Students are assessed on:\n\nattendance, preparation, and participation;\nquality of submitted work;\nindividual contributions to group assignments;\noral interview."
  },
  {
    "objectID": "pstat197a/about-syllabus.html#sec-software",
    "href": "pstat197a/about-syllabus.html#sec-software",
    "title": "Course syllabus",
    "section": "Software",
    "text": "Software\nComputing in PSTAT197A will be shown in R, and codes and other materials will be shared via GitHub. The following software will be required to access course materials:\n\nR version 4.2.0+\nRStudio version 2022.07.1+\nGit version 2.36.1+\nGitHub Desktop (or another visual GitHub client)\n\nInstallations and basic functionality will be covered in the first section meeting.\nWhile PSTAT197A is not language-agnostic and some instruction in R is provided, it is also not a course especially emphasizing programming technique in R. Students are free to use or experiment with other software at their discretion provided it does not interfere with their participation in the class, but are expected to submit work and collaborate using RStudio-supported files."
  },
  {
    "objectID": "pstat197a/about-syllabus.html#sec-github",
    "href": "pstat197a/about-syllabus.html#sec-github",
    "title": "Course syllabus",
    "section": "GitHub",
    "text": "GitHub\nStudents will learn and practice basic functionality of Git and GitHub for version control and collaboration by accessing course materials via GitHub repositories and submitting work via repository contributions.\nWe have a GitHub classroom for the data science capstone. Materials will be deployed via direct links. Students will be asked to submit work by contributing to team repositories; any such contributions will remain visible to course staff and team contributors, and so are not strictly private.\nTo access GitHub Classroom materials students will need to create a GitHub account if they do not already have one. Here is some advice on choosing a username."
  },
  {
    "objectID": "pstat197a/about-syllabus.html#policies",
    "href": "pstat197a/about-syllabus.html#policies",
    "title": "Course syllabus",
    "section": "Policies",
    "text": "Policies\nAttendance. Regular attendance is expected. Each student can miss two sessions without notice; further absences may impact course grades. Students are responsible for material discussed in their absence and should review posted session notes and consult a classmate.\nDeadlines. Students are expected to meet assignment deadlines in a timely manner. All deadlines have a 24-hour grace period. Late or amended work may not be accepted.\nEmail. Course staff will make their best effort to reply to email within 48 weekday hours. However, due to high volume, staff cannot guarantee that all messages will receive replies.\nIllness. Students who are ill are required to stay home. Students ill with COVID-19 must comply with university policy regarding reporting and quarantine. Accommodations will be made to ensure that students absent due to illness do not fall behind.\nAccommodations. Reasonable accommodations will be made for any student with a qualifying disability. Such requests should be made through the Disabled Students Program (DSP). More information, instructions on how to access accommodations, and information on related resources can be found on the DSP website. Note: in this class there are no timed assessments.\nLetter grades. Letter grades are assigned based only on the assessments identified above and according to university guidelines, with the relative weighting of assessments determined at the discretion of the instructor. While grade calculations will not be disclosed, students are entitled to an explanation of the criteria used to determine their grades if desired. Grades will not be changed except in the case of clerical errors. If students feel their grade has been unfairly assigned, they are entitled to contest it following UCSB procedure for contesting grades.\nConduct. All course participants are expected to maintain respectful and honorable conduct consistent with UCSB ethical standards. Students uncomfortable with the behavior of another course participant for any reason should notify the instructor, course staff, or, if the complaint relates to course staff conduct, an administrative or departmental officer. Evidence of academic dishonesty will be reported to the Office of Student Conduct (OSC); evidence of problematic behavior will be addressed on a case-by-case basis in accord with university policies."
  },
  {
    "objectID": "pstat197a/slides/week4-lasso.html#announcementsreminders",
    "href": "pstat197a/slides/week4-lasso.html#announcementsreminders",
    "title": "Variable selection with the LASSO",
    "section": "Announcements/reminders",
    "text": "Announcements/reminders\n\nOffice hours follow all class and section meetings in building 434 room 122.\n\nEnter through courtyard, not on Storke side"
  },
  {
    "objectID": "pstat197a/slides/week4-lasso.html#minute-activity",
    "href": "pstat197a/slides/week4-lasso.html#minute-activity",
    "title": "Variable selection with the LASSO",
    "section": "10 Minute Activity",
    "text": "10 Minute Activity\nWith your table, share your diagrams.\n\nidentify common elements/aspects\npick one or a blend that you like best and sketch it on the whiteboard"
  },
  {
    "objectID": "pstat197a/slides/week4-lasso.html#design-assessment-1",
    "href": "pstat197a/slides/week4-lasso.html#design-assessment-1",
    "title": "Variable selection with the LASSO",
    "section": "Design assessment",
    "text": "Design assessment\nHow would you score the proteomics analysis along the following dimensions?\n\nreproducibility: how easy is it to reproduce the results exactly?\ntransparency: how widely distributed is potential variability in results across the elements of the data analysis?\nreplicability: how likely is it that if the study were conducted with new subjects, the same panel would be obtained?"
  },
  {
    "objectID": "pstat197a/slides/week4-lasso.html#two-steps",
    "href": "pstat197a/slides/week4-lasso.html#two-steps",
    "title": "Variable selection with the LASSO",
    "section": "Two steps",
    "text": "Two steps\nAt a very high level, the analysis we’ve covered is a ‘two-step’ procedure:\n\n(selection step) select variables\n(estimation step) train a classifier using selected variables\n\n\nA likely reason for this approach is that one cannot fit a regression model with 1317 predictors based on only 154 data points (\\(p &gt; n\\))."
  },
  {
    "objectID": "pstat197a/slides/week4-lasso.html#a-schematic",
    "href": "pstat197a/slides/week4-lasso.html#a-schematic",
    "title": "Variable selection with the LASSO",
    "section": "A schematic",
    "text": "A schematic\nHere’s a representation of the design of the analysis procedure.\n\n\n\n\n\n\n\nG\n\n \n\ncluster1\n\n statistical modeling  \n\ncluster2\n\n outputs   \n\ndata\n\n data   \n\nfit\n\n logistic   regression   \n\ndata-&gt;fit\n\n    \n\ns1\n\n multiple   testing   \n\ndata-&gt;s1\n\n    \n\ns2\n\n correlation   analysis   \n\ndata-&gt;s2\n\n    \n\ns3\n\n random   forest   \n\ndata-&gt;s3\n\n    \n\njoin\n\n   \n\ndata-&gt;join\n\n multiple partitioning   \n\nselec\n\n ensemble   selection   \n\nselec-&gt;fit\n\n    \n\nsel\n\n selected   variables   \n\nfit-&gt;sel\n\n    \n\nfit-&gt;join\n\n   \n\ns1-&gt;selec\n\n    \n\ns2-&gt;selec\n\n    \n\ns3-&gt;selec\n\n    \n\npred\n\n accuracy   quantification   \n\njoin-&gt;pred\n\n   \n\n\n\n\n\n\nlots of failure modes\nrepeated use of the same data for different sub-analyses"
  },
  {
    "objectID": "pstat197a/slides/week4-lasso.html#what-about-one-step",
    "href": "pstat197a/slides/week4-lasso.html#what-about-one-step",
    "title": "Variable selection with the LASSO",
    "section": "What about one step?",
    "text": "What about one step?\nIf we did simultaneous selection and estimation, our schematic might look like this:\n\n\n\n\n\n\n\nG\n\n \n\ncluster1\n\n data partitioning  \n\ncluster2\n\n statistical modeling  \n\ncluster3\n\n outputs   \n\ndata\n\n data   \n\ntrain\n\n training   set   \n\ndata-&gt;train\n\n    \n\ntest\n\n test   set   \n\ndata-&gt;test\n\n    \n\nfit\n\n selection and   estimation   \n\ntrain-&gt;fit\n\n    \n\njoin\n\n   \n\ntest-&gt;join\n\n   \n\nsel\n\n selected   variables   \n\nfit-&gt;sel\n\n    \n\nfit-&gt;join\n\n   \n\npred\n\n accuracy   quantification   \n\njoin-&gt;pred"
  },
  {
    "objectID": "pstat197a/slides/week4-lasso.html#sparse-estimation",
    "href": "pstat197a/slides/week4-lasso.html#sparse-estimation",
    "title": "Variable selection with the LASSO",
    "section": "Sparse estimation",
    "text": "Sparse estimation\nImagine a logistic regression model with all 1317 predictors:\n\\[\n\\log\\left(\\frac{p_i}{1 - p_i}\\right) = \\beta_0 + \\sum_{j = 1}^{1317} \\beta_j x_{ij}\n\\]\n\nSuppose we also assume sparsity constraint: a condition that most of them must be zero.\n\n\nAs long as the constraint is strong enough so that the number of nonzero coefficients is \\(n\\) or fewer (i.e., \\(\\|\\beta\\|_0 &lt; n\\)), one can compute estimates."
  },
  {
    "objectID": "pstat197a/slides/week4-lasso.html#the-lasso",
    "href": "pstat197a/slides/week4-lasso.html#the-lasso",
    "title": "Variable selection with the LASSO",
    "section": "The LASSO",
    "text": "The LASSO\nThe Least Absolute Shrinkage and Selection Operator (LASSO) (Tibshirani (1996) ) is the most widely-used sparse regression estimator.\n\nMathematically, the LASSO is the \\(L_1\\)-constrained MLE, i.e., the solution to the optimization problem:\n\\[\n\\begin{aligned}\n\\text{maximize}\\quad &\\mathcal{L}(\\beta; x, y) \\\\\n\\text{subject to}\\quad &\\|\\beta\\|_1 &lt; t\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "pstat197a/slides/week4-lasso.html#lagrangian-form",
    "href": "pstat197a/slides/week4-lasso.html#lagrangian-form",
    "title": "Variable selection with the LASSO",
    "section": "Lagrangian form",
    "text": "Lagrangian form\nThe solution to the constrained problem\n\\[\n\\begin{aligned}\n\\text{maximize}\\quad &\\mathcal{L}(\\beta; x, y) \\\\\n\\text{subject to}\\quad &\\|\\beta\\|_1 &lt; t\n\\end{aligned}\n\\]\ncan be expressed as the unconstrained optimization\n\\[\n\\hat{\\beta} = \\arg\\min_\\beta \\left\\{-\\mathcal{L}(\\beta; x, y) + \\lambda\\|\\beta\\|_1\\right\\}\n\\]\nwhere \\(\\lambda\\) is a Lagrange multiplier."
  },
  {
    "objectID": "pstat197a/slides/week4-lasso.html#toy-example",
    "href": "pstat197a/slides/week4-lasso.html#toy-example",
    "title": "Variable selection with the LASSO",
    "section": "Toy example",
    "text": "Toy example\n\n\nI simulated 250 observations according to a logistic regression model with 13 predictors, all but 3 of which had zero coefficients:\n\\[\n\\beta = \\left[ 0\\; 3\\; 0\\; 0\\; 0\\; 0\\; 0\\; 0\\; 0\\; 0\\; 0\\; 2\\; 2\\right]^T\n\\]\nAnd computed LASSO estimates for a few values of \\(\\lambda\\).\n\n\n\n# A tibble: 14 × 4\n   term  truth lambda1 lambda2\n   &lt;chr&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1 Int    -0.5  -0.171  -0.206\n 2 V1      0     0       0    \n 3 V2      3     1.49    1.96 \n 4 V3      0     0       0    \n 5 V4      0     0       0    \n 6 V5      0     0       0    \n 7 V6      0     0       0    \n 8 V7      0     0       0    \n 9 V8      0     0       0.084\n10 V9      0     0       0    \n11 V10     0     0       0    \n12 V11     0     0      -0.069\n13 V12     2     0.892   1.29 \n14 V13     2     1.01    1.40"
  },
  {
    "objectID": "pstat197a/slides/week4-lasso.html#constraint-strength",
    "href": "pstat197a/slides/week4-lasso.html#constraint-strength",
    "title": "Variable selection with the LASSO",
    "section": "Constraint strength",
    "text": "Constraint strength\nIn practice, \\(\\lambda\\) controls the strength of the constraint and thus the sparsity:\n\nLarger \\(\\lambda\\) ➜ stronger constraint ➜ more sparse\nSmaller \\(\\lambda\\) ➜ weaker constraint ➜ less sparse"
  },
  {
    "objectID": "pstat197a/slides/week4-lasso.html#selecting-lambda",
    "href": "pstat197a/slides/week4-lasso.html#selecting-lambda",
    "title": "Variable selection with the LASSO",
    "section": "Selecting \\(\\lambda\\)",
    "text": "Selecting \\(\\lambda\\)\nHow to choose the strength of constraint?\n\nCompute estimates for a “path” \\(\\lambda_1 &gt; \\lambda_2 &gt; \\cdots &gt; \\lambda_K\\)\nChoose \\(\\lambda^*\\) that minimizes an error metric.\n\n\nTypically prediction error is used as the metric, and estimated by:\n\npartitioning the data many times\naveraging test set predictive accuracy across partitions"
  },
  {
    "objectID": "pstat197a/slides/week4-lasso.html#regularization-profile-estimates",
    "href": "pstat197a/slides/week4-lasso.html#regularization-profile-estimates",
    "title": "Variable selection with the LASSO",
    "section": "Regularization profile: estimates",
    "text": "Regularization profile: estimates\n\nValue of coefficient estimates as a function of regularization strength for the toy example. Each path corresponds to one model coefficient. Vertical line indicates optimal strength."
  },
  {
    "objectID": "pstat197a/slides/week4-lasso.html#regularization-profile-error",
    "href": "pstat197a/slides/week4-lasso.html#regularization-profile-error",
    "title": "Variable selection with the LASSO",
    "section": "Regularization profile: error",
    "text": "Regularization profile: error\n\nPrediction error metric (‘deviance’, similar to RMSE) as a function of regularization strength. Two plausible choices shown – one more conservative, one less conservative."
  },
  {
    "objectID": "pstat197a/slides/week4-lasso.html#lambda-selection",
    "href": "pstat197a/slides/week4-lasso.html#lambda-selection",
    "title": "Variable selection with the LASSO",
    "section": "\\(\\lambda\\) selection",
    "text": "\\(\\lambda\\) selection\n\nEstimate profile with optimal choice indicated by vertical line."
  },
  {
    "objectID": "pstat197a/slides/week4-lasso.html#coefficient-estimates",
    "href": "pstat197a/slides/week4-lasso.html#coefficient-estimates",
    "title": "Variable selection with the LASSO",
    "section": "Coefficient estimates",
    "text": "Coefficient estimates\n\n\n\n\n\nterm\nestimate\ntruth\n\n\n\n\n(Intercept)\n-0.1712466\n-0.5\n\n\nV2\n1.4867846\n3.0\n\n\nV12\n0.8922530\n2.0\n\n\nV13\n1.0142979\n2.0"
  },
  {
    "objectID": "pstat197a/slides/week4-lasso.html#recomputing-estimates",
    "href": "pstat197a/slides/week4-lasso.html#recomputing-estimates",
    "title": "Variable selection with the LASSO",
    "section": "Recomputing estimates",
    "text": "Recomputing estimates\nNotice that the estimate magnitudes are off by a considerable amount. If we recompute estimates without the constraint:\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-0.2620012\n0.1894367\n-1.383054\n0.1666482\n\n\nV2\n2.5653900\n0.3313415\n7.742434\n0.0000000\n\n\nV12\n1.7554670\n0.2786778\n6.299271\n0.0000000\n\n\nV13\n1.8323539\n0.2632716\n6.959937\n0.0000000"
  },
  {
    "objectID": "pstat197a/slides/week4-lasso.html#errors",
    "href": "pstat197a/slides/week4-lasso.html#errors",
    "title": "Variable selection with the LASSO",
    "section": "Errors",
    "text": "Errors\nNow if I simulate some new observations:\n\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nsensitivity\nbinary\n0.8500000\n\n\nspecificity\nbinary\n0.9000000\n\n\nroc_auc\nbinary\n0.9508333"
  },
  {
    "objectID": "pstat197a/slides/week4-lasso.html#alternative-analysis",
    "href": "pstat197a/slides/week4-lasso.html#alternative-analysis",
    "title": "Variable selection with the LASSO",
    "section": "Alternative analysis",
    "text": "Alternative analysis\n\nPreprocessing\\(\\lambda\\) selectionRefittingAccuracy\n\n\nLog-transform, center and scale, but don’t trim outliers.\n\nlibrary(tidymodels)\n\n# read in data\nbiomarker &lt;- read_csv('data/biomarker-clean.csv') %&gt;%\n  select(-ados) %&gt;%\n  mutate(across(-group, ~scale(log(.x))[,1]),\n         class = as.numeric(group == 'ASD'))\n\n# partition\nset.seed(101622)\npartitions &lt;- biomarker %&gt;%\n  initial_split(prop = 0.8)\n\nx_train &lt;- training(partitions) %&gt;%\n  select(-group, -class) %&gt;%\n  as.matrix()\ny_train &lt;- training(partitions) %&gt;%\n  pull(class)\n\n\n\nEstimated error\n\n# reproducibility\nset.seed(102022)\n\n# multiple partitioning for lambda selection\ncv_out &lt;- cv.glmnet(x_train, \n                    y_train, \n                    family = 'binomial', \n                    nfolds = 5, \n                    type.measure = 'deviance')\n\ncvout_df &lt;- tidy(cv_out) \n\n\n\n\n\n\n\n\n\n\nCoefficient paths\n\n# LASSO estimates\nfit &lt;- glmnet(x_train, y_train, family = 'binomial')\nfit_df &lt;- tidy(fit)\n\n\n\n\n\n\n\n\n\n\n\n\nAfter dropping the penalty and recomputing estimates, the final model is:\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-0.0404465\n0.2312305\n-0.1749184\n0.8611438\n\n\nMAPK2\n-0.9896079\n0.2831955\n-3.4944340\n0.0004751\n\n\nIgD\n-0.8264133\n0.2471363\n-3.3439569\n0.0008259\n\n\nDERM\n-0.9971069\n0.2758803\n-3.6142740\n0.0003012\n\n\n\n\n\n\n\n\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nsensitivity\nbinary\n0.8125000\n\n\nspecificity\nbinary\n0.7333333\n\n\nroc_auc\nbinary\n0.8375000"
  },
  {
    "objectID": "pstat197a/slides/week4-lasso.html#some-final-thoughts",
    "href": "pstat197a/slides/week4-lasso.html#some-final-thoughts",
    "title": "Variable selection with the LASSO",
    "section": "Some final thoughts",
    "text": "Some final thoughts\n\nRegardless of method, classification based on serum levels seems to achieve 70-80% accuracy out of sample.\nNot specific enough to be used as a diagnostic tool, but may be helpful in early detection.\nPublished analysis seems a little over-complicated; computationally intensive methods are less transparent and more difficult to reproduce."
  },
  {
    "objectID": "pstat197a/slides/week4-lasso.html#concepts-discussed",
    "href": "pstat197a/slides/week4-lasso.html#concepts-discussed",
    "title": "Variable selection with the LASSO",
    "section": "Concepts discussed",
    "text": "Concepts discussed\n\nmultiple hypothesis testing, FWER and FDR control\nclassification using logistic regression, random forests\nvariable selection using LASSO regularization\ndata partitioning and accuracy quantification"
  },
  {
    "objectID": "pstat197a/slides/week4-lasso.html#transferrable-skills",
    "href": "pstat197a/slides/week4-lasso.html#transferrable-skills",
    "title": "Variable selection with the LASSO",
    "section": "Transferrable skills",
    "text": "Transferrable skills\n\niterative computations in R, three ways\nfitting a logistic regression model with glm()\nuse of yardstick and rsample for quantifying classification accuracy\n\n\n\n\n\n\n\nTibshirani, Robert. 1996. “Regression Shrinkage and Selection via the Lasso.” Journal of the Royal Statistical Society: Series B (Methodological) 58 (1): 267–88."
  },
  {
    "objectID": "pstat197a/slides/week1-github.html#announcementsreminders",
    "href": "pstat197a/slides/week1-github.html#announcementsreminders",
    "title": "Basic GitHub actions",
    "section": "Announcements/reminders",
    "text": "Announcements/reminders\n\ntoday, sit at your table from last time\nassignment for next time:\n\nread MDSR 9.1 – 9.2\nprepare a reading response"
  },
  {
    "objectID": "pstat197a/slides/week1-github.html#objective-for-today",
    "href": "pstat197a/slides/week1-github.html#objective-for-today",
    "title": "Basic GitHub actions",
    "section": "Objective for today",
    "text": "Objective for today\nLearn how to interact with GitHub repositories:\n\nretrieve and submit file changes;\nexamine repository updates;\nuse branches for parallel workflow;\nresolve conflicts."
  },
  {
    "objectID": "pstat197a/slides/week1-github.html#basic-git-actions",
    "href": "pstat197a/slides/week1-github.html#basic-git-actions",
    "title": "Basic GitHub actions",
    "section": "Basic Git actions",
    "text": "Basic Git actions\n\nCommunication actions for moving file changes between locations"
  },
  {
    "objectID": "pstat197a/slides/week1-github.html#branching-workflow",
    "href": "pstat197a/slides/week1-github.html#branching-workflow",
    "title": "Basic GitHub actions",
    "section": "Branching workflow",
    "text": "Branching workflow\n\nTypical use of repository branches for development of new features"
  },
  {
    "objectID": "pstat197a/slides/week1-github.html#activity-overview",
    "href": "pstat197a/slides/week1-github.html#activity-overview",
    "title": "Basic GitHub actions",
    "section": "Activity overview",
    "text": "Activity overview\n\nMake individual changes to files and create ‘commits’\nCreate repository branches to enable you to work more efficiently in parallel.\nMerge branches with the main branch via pull request.\nCreate and resolve a merge conflict."
  },
  {
    "objectID": "pstat197a/slides/week1-github.html#setup",
    "href": "pstat197a/slides/week1-github.html#setup",
    "title": "Basic GitHub actions",
    "section": "Setup",
    "text": "Setup\n\nhave everyone open their GitHub client, the sandbox project in RStudio, and the group sandbox repository in the browser on github.com\nchoose one person to operate the table workstation; have them do the same and log in to github using their credentials\non the workstation, create a directory for the class documents/22f-pstat197a and clone the group sandbox repo into this directory"
  },
  {
    "objectID": "pstat197a/slides/week1-perspectives.html#announcementsreminders",
    "href": "pstat197a/slides/week1-perspectives.html#announcementsreminders",
    "title": "On data science",
    "section": "Announcements/reminders",
    "text": "Announcements/reminders\n\nJoin Slack workspace, monitor channel #22f-pstat197a for announcements.\nOffice hours by demand immediately following section and class meetings\n\nYan will hold a drop in hour in building 434 room 126 after Josh’s section\n\nInstall course software and bring your laptop to section meetings. Remember your table number from today."
  },
  {
    "objectID": "pstat197a/slides/week1-perspectives.html#google-trends-data-science",
    "href": "pstat197a/slides/week1-perspectives.html#google-trends-data-science",
    "title": "On data science",
    "section": "Google trends: data science",
    "text": "Google trends: data science\n\n\nData science emerged as a term of art in the last decade\nInterest exploded in the last five years"
  },
  {
    "objectID": "pstat197a/slides/week1-perspectives.html#origins-data-analysis",
    "href": "pstat197a/slides/week1-perspectives.html#origins-data-analysis",
    "title": "On data science",
    "section": "Origins: ‘data analysis’",
    "text": "Origins: ‘data analysis’\nTukey advocated for ‘data analysis’ as a broader field than statistics (Tukey 1962), including:\n\nstatistical theory and methodology;\nvisualization and data display techniques;\ncomputation and scalability;\nbreadth of application.\n\n\nLook famililar? Tukey’s ‘data analysis’ is proto-modern data science."
  },
  {
    "objectID": "pstat197a/slides/week1-perspectives.html#early-data-analysis-concepts",
    "href": "pstat197a/slides/week1-perspectives.html#early-data-analysis-concepts",
    "title": "On data science",
    "section": "Early data analysis concepts",
    "text": "Early data analysis concepts\nIn the 1960’s and 1970’s, these concepts meant very different things.\n\nvisualization meant drawing\ncomputation meant data re-expression by hand\n\n\nBut the ideas were still somewhat radical. At the time most relied on highly reductive numerical results to interpret data:\n\nANOVA tables\nregression tables\np-values"
  },
  {
    "objectID": "pstat197a/slides/week1-perspectives.html#example-boxplots",
    "href": "pstat197a/slides/week1-perspectives.html#example-boxplots",
    "title": "On data science",
    "section": "Example: boxplots",
    "text": "Example: boxplots\n\nFigure from (Tukey et al. 1977)"
  },
  {
    "objectID": "pstat197a/slides/week1-perspectives.html#early-data-analysis-concepts-1",
    "href": "pstat197a/slides/week1-perspectives.html#early-data-analysis-concepts-1",
    "title": "On data science",
    "section": "Early data analysis concepts",
    "text": "Early data analysis concepts\nThe new techniques allowed for iterative investigation:\n\nformulate a question\nexamine data graphics and summaries\nadjust computations and graphics to hone in on content of interest\nrefine the question"
  },
  {
    "objectID": "pstat197a/slides/week1-perspectives.html#birth-to-death-ratio-by-state",
    "href": "pstat197a/slides/week1-perspectives.html#birth-to-death-ratio-by-state",
    "title": "On data science",
    "section": "Birth-to-death ratio by state",
    "text": "Birth-to-death ratio by state\nSuppose we want to explain variation in birth-to-death ratios in the U.S. 1\n\nInitial question: is population density an associated factor?\n\nThis example follows (Tukey et al. 1977)"
  },
  {
    "objectID": "pstat197a/slides/week1-perspectives.html#first-iteration",
    "href": "pstat197a/slides/week1-perspectives.html#first-iteration",
    "title": "On data science",
    "section": "First iteration",
    "text": "First iteration\n\nA first attempt"
  },
  {
    "objectID": "pstat197a/slides/week1-perspectives.html#first-iteration-1",
    "href": "pstat197a/slides/week1-perspectives.html#first-iteration-1",
    "title": "On data science",
    "section": "First iteration",
    "text": "First iteration\n\nWhat if we adjust the computation?"
  },
  {
    "objectID": "pstat197a/slides/week1-perspectives.html#second-iteration",
    "href": "pstat197a/slides/week1-perspectives.html#second-iteration",
    "title": "On data science",
    "section": "Second iteration",
    "text": "Second iteration\n\nWhat about median age instead?"
  },
  {
    "objectID": "pstat197a/slides/week1-perspectives.html#second-iteration-1",
    "href": "pstat197a/slides/week1-perspectives.html#second-iteration-1",
    "title": "On data science",
    "section": "Second iteration",
    "text": "Second iteration\n\nAdjust computations for easy linear approximation"
  },
  {
    "objectID": "pstat197a/slides/week1-perspectives.html#third-iteration",
    "href": "pstat197a/slides/week1-perspectives.html#third-iteration",
    "title": "On data science",
    "section": "Third iteration",
    "text": "Third iteration\n\nAre there outliers?"
  },
  {
    "objectID": "pstat197a/slides/week1-perspectives.html#fourth-iteration",
    "href": "pstat197a/slides/week1-perspectives.html#fourth-iteration",
    "title": "On data science",
    "section": "Fourth iteration",
    "text": "Fourth iteration\n\nAre outliers spatially correlated?"
  },
  {
    "objectID": "pstat197a/slides/week1-perspectives.html#a-bit-of-history",
    "href": "pstat197a/slides/week1-perspectives.html#a-bit-of-history",
    "title": "On data science",
    "section": "A bit of history",
    "text": "A bit of history\nIt’s worth noting that in the first half of the 20th century, much of statistics focused on methodology and theory for the analysis of small iid samples, and in particular:\n\ninference on means and inference on tables;\nanalysis of variance;\ntests of distribution.\n\n\nThe inferential framework brought to bear on these ‘simpler’ problems largely carried over when the field began to specialize."
  },
  {
    "objectID": "pstat197a/slides/week1-perspectives.html#contrasting-approaches",
    "href": "pstat197a/slides/week1-perspectives.html#contrasting-approaches",
    "title": "On data science",
    "section": "Contrasting approaches",
    "text": "Contrasting approaches\nFrom 1960-2010, adopters of the ‘data analysis as a field’ view were largely industry practitioners and applied statisticians who advocated for training and practice that included empirical methods and computation in addition to statistical inference (Donoho 2017).\n\nTheir ideas evolved into an alternative approach to working with data:\n\ndata-driven rather than theory-driven;\niterative rather than conclusive."
  },
  {
    "objectID": "pstat197a/slides/week1-perspectives.html#confirmatory-approach",
    "href": "pstat197a/slides/week1-perspectives.html#confirmatory-approach",
    "title": "On data science",
    "section": "Confirmatory approach",
    "text": "Confirmatory approach\nThe “confirmatory” approach of the classical inferential framework.\n\n\n\n\n\n\n\nconfirm\n\n \n\ncluster_1\n\n data generation  \n\ncluster_2\n\n data analysis  \n\ncluster_3\n\n decision   \n\nsci\n\n domain  knowledge   \n\nhyp\n\n hypotheses   \n\nsci-&gt;hyp\n\n    \n\nexp\n\n designed  experiment   \n\nhyp-&gt;exp\n\n    \n\nmdl\n\n statistical  model   \n\nexp-&gt;mdl\n\n    \n\ndat\n\n data   \n\nexp-&gt;dat\n\n    \n\nyay\n\n supporting  evidence   \n\nmdl-&gt;yay\n\n    \n\nnay\n\n opposing  evidence   \n\nmdl-&gt;nay\n\n    \n\ndat-&gt;yay\n\n    \n\ndat-&gt;nay\n\n   \n\n\n\n\n\n\noutput is a decision\nstatistical model determined by experimental design\nanalysis based on statistical theory"
  },
  {
    "objectID": "pstat197a/slides/week1-perspectives.html#exploratory-approach",
    "href": "pstat197a/slides/week1-perspectives.html#exploratory-approach",
    "title": "On data science",
    "section": "Exploratory approach",
    "text": "Exploratory approach\nThe “exploratory” approach of iterative modern data analysis.\n\n\n\n\n\n\n\nexplore\n\n \n\ncluster_1\n\n data analysis  \n\ncluster_2\n\n findings   \n\nsci\n\n domain  knowledge   \n\nq\n\n question  formulation   \n\nsci-&gt;q\n\n    \n\ndat\n\n data   \n\nq-&gt;dat\n\n    \n\nmdl\n\n statistical  model   \n\ndat-&gt;mdl\n\n    \n\nmdl-&gt;q\n\n    \n\nf1\n\n finding 1   \n\nmdl-&gt;f1\n\n    \n\nf2\n\n finding 2   \n\nmdl-&gt;f2\n\n    \n\ndots\n\n ⋮  \n\n\n\n\n\n\noutputs are findings\nstatistical model determined by data\nanalysis techniques include empirical methods"
  },
  {
    "objectID": "pstat197a/slides/week1-perspectives.html#drivers-of-change",
    "href": "pstat197a/slides/week1-perspectives.html#drivers-of-change",
    "title": "On data science",
    "section": "Drivers of change",
    "text": "Drivers of change\nIn the 2000s and especially after 2010, the iterative approach enjoys broader applicability than it used to:\n\ndue to automated and/or scalable data collection\n\nobservational data is widely available across domains\nand includes large numbers of variables\n\nhighly specialized data problems evade methodology with theoretical support\nmore accessible to analysts without advanced statistical training"
  },
  {
    "objectID": "pstat197a/slides/week1-perspectives.html#machine-learning",
    "href": "pstat197a/slides/week1-perspectives.html#machine-learning",
    "title": "On data science",
    "section": "Machine learning",
    "text": "Machine learning\nMachine learning was largely advanced by computer scientists through 2010 and later (Emmert-Streib et al. 2020), most notably:\n\nneural networks and deep learning\noptimization\nalgorithmic analysis\n\n\nThis was a major driver in advancing modern predictive modeling, and engaging with these tools required going beyond statistics."
  },
  {
    "objectID": "pstat197a/slides/week1-perspectives.html#a-theory-about-data-science",
    "href": "pstat197a/slides/week1-perspectives.html#a-theory-about-data-science",
    "title": "On data science",
    "section": "A theory about data science",
    "text": "A theory about data science\n\nAround mid-century, it was proposed that specialists should be trained in computational as well as statistical methods\nOver time practitioners developed iterative processes for data-driven problem solving that was more flexible than the classical inferential framework\nComputer scientists advanced the field of machine learning substantially\nIterative problem solving together with applied machine learning was well-suited to meet the demands of modern data, but the area was not codified in an academic discipline"
  },
  {
    "objectID": "pstat197a/slides/week1-perspectives.html#what-is-research",
    "href": "pstat197a/slides/week1-perspectives.html#what-is-research",
    "title": "On data science",
    "section": "What is research?",
    "text": "What is research?\nResearch is systematic investigation undertaken in order to establish or discover facts.\n\nWhat are facts in data science?\n\nmethod M outperforms method M’ at task T\nwe analyzed data D and reached the conclusion that…"
  },
  {
    "objectID": "pstat197a/slides/week1-perspectives.html#the-research-landscape",
    "href": "pstat197a/slides/week1-perspectives.html#the-research-landscape",
    "title": "On data science",
    "section": "The research landscape",
    "text": "The research landscape\nFormal communities – i.e., journals, departments, conferences – have not coalesced around data science research to date.\n\nRelevant research largely occurs in statistics, computer science, and application domains, and can be divided broadly into:\n\nmethodology – creating new techniques to analyze data\napplications – applying existing methods to generate new findings"
  },
  {
    "objectID": "pstat197a/slides/week1-perspectives.html#methodological-research",
    "href": "pstat197a/slides/week1-perspectives.html#methodological-research",
    "title": "On data science",
    "section": "Methodological research",
    "text": "Methodological research\nMethodological research might involve:\n\ndesigning a faster algorithm for solving a particular problem\nproposing a new technique for analyzing a particular type of data\ngeneralizing a technique to a broader range of problems"
  },
  {
    "objectID": "pstat197a/slides/week1-perspectives.html#applied-research",
    "href": "pstat197a/slides/week1-perspectives.html#applied-research",
    "title": "On data science",
    "section": "Applied research",
    "text": "Applied research\nApplied research might involve:\n\nanalyzing a specific dataset or producing a novel analysis of existing data\ncreating ad-hoc methods for a domain-specific problem\nimporting methodology from another area to bear on a domain-specific problem"
  },
  {
    "objectID": "pstat197a/slides/week1-perspectives.html#data-science-capstones",
    "href": "pstat197a/slides/week1-perspectives.html#data-science-capstones",
    "title": "On data science",
    "section": "Data science capstones",
    "text": "Data science capstones\nMost of the time, our data science capstones fall pretty squarely in the applied domain:\n\nsponsor provides data and high-level goals\nstudent team works on producing an analysis or analyses\nmentor advises on methodology"
  },
  {
    "objectID": "pstat197a/slides/week1-perspectives.html#relevant-skills",
    "href": "pstat197a/slides/week1-perspectives.html#relevant-skills",
    "title": "On data science",
    "section": "Relevant skills",
    "text": "Relevant skills\nThere are a few avenues to prepare for this sort of work.\n\nWe’ll focus on:\n\nrecognizing problem patterns\ndeveloping a functional view of methodology\ncollaborating efficiently\nindependent learning strategies\nengaging with literature constructively\n\n\n\nIt won’t provide you with exhaustive methodological preparation, but should support you in learning ‘on the job’."
  },
  {
    "objectID": "pstat197a/slides/week1-perspectives.html#reading-responses",
    "href": "pstat197a/slides/week1-perspectives.html#reading-responses",
    "title": "On data science",
    "section": "Reading responses",
    "text": "Reading responses\nQuestions on the perspectives paper (Peng and Parker 2022) to review:\n\nWhat is meant by a ‘systems approach’ to data science?\nWhat is meant by ‘design thinking’ in data science?\n(Why) Are these useful concepts?"
  },
  {
    "objectID": "pstat197a/slides/week1-perspectives.html#systems-approach",
    "href": "pstat197a/slides/week1-perspectives.html#systems-approach",
    "title": "On data science",
    "section": "Systems approach",
    "text": "Systems approach\n\n\n\n\nSeveral systems affect the relationship between expected and actual results. Where would you locate them on the figure?\n\nData analytic\nSoftware\nScientific"
  },
  {
    "objectID": "pstat197a/slides/week1-perspectives.html#example-systems-for-data-cleaning",
    "href": "pstat197a/slides/week1-perspectives.html#example-systems-for-data-cleaning",
    "title": "On data science",
    "section": "Example systems for data cleaning",
    "text": "Example systems for data cleaning\n\n\nHow might this diagram help an analyst?"
  },
  {
    "objectID": "pstat197a/slides/week1-perspectives.html#design-thinking",
    "href": "pstat197a/slides/week1-perspectives.html#design-thinking",
    "title": "On data science",
    "section": "Design thinking",
    "text": "Design thinking\nThe design thinking framework might be summed up:\n\ndata scientists trade in data analyses\na data analysis is a designed product\nthinking about design principles can help make a better product\n\n\nMany of you focused on how design principles are a response to project constraints. Are there other ways a design perspective might be useful?"
  },
  {
    "objectID": "pstat197a/slides/week1-perspectives.html#scenario-1",
    "href": "pstat197a/slides/week1-perspectives.html#scenario-1",
    "title": "On data science",
    "section": "Scenario 1",
    "text": "Scenario 1\nYou’re working at a news organization and developing a recommender system for targeted article previews to deploy on the organization’s website. It will show users article previews based on their behavior. Assume you don’t have any significant resource constraints, and can access users’ profiles in full and log interactions in near-real-time.\n\nGoal: show previews most likely to attract interest.\n\n\nConsiderations:\n\nwhat material should be shown in the preview? headlines? images? text?\nwhat behavior can/should be leveraged for the recommender system?\nwhat are a few relevant design aspects of how the system should behave?\nare there ethical concerns?"
  },
  {
    "objectID": "pstat197a/slides/week1-perspectives.html#scenario-2",
    "href": "pstat197a/slides/week1-perspectives.html#scenario-2",
    "title": "On data science",
    "section": "Scenario 2",
    "text": "Scenario 2\nYou’re working on a research team studying ecological impacts of land use. The team has access to longitudinal species surveys at locations of interest across the U.S., quarterly county-level land allocation statistics, satellite images, and state budget information for sustainability, restoration, and conservation initiatives.\n\nGoal: identify intervention opportunities that are most likely to positively impact ecological diversity.\n\n\nConsiderations:\n\nwhat data would you use and how would you combine data sources?\nare there external data that might be useful?\nwhat analysis outputs would be most important for identifying intervention opportunities?\ncan you think of other design features that might be useful for the data analysis?"
  },
  {
    "objectID": "pstat197a/slides/week1-perspectives.html#a-few-design-principles",
    "href": "pstat197a/slides/week1-perspectives.html#a-few-design-principles",
    "title": "On data science",
    "section": "A few design principles",
    "text": "A few design principles\nLet’s look at some design principles from (McGowan, Peng, and Hicks 2021)."
  },
  {
    "objectID": "pstat197a/slides/week1-perspectives.html#design-principles-matchedness",
    "href": "pstat197a/slides/week1-perspectives.html#design-principles-matchedness",
    "title": "On data science",
    "section": "Design principles: matchedness",
    "text": "Design principles: matchedness"
  },
  {
    "objectID": "pstat197a/slides/week1-perspectives.html#design-principles-exhuastiveness",
    "href": "pstat197a/slides/week1-perspectives.html#design-principles-exhuastiveness",
    "title": "On data science",
    "section": "Design principles: exhuastiveness",
    "text": "Design principles: exhuastiveness"
  },
  {
    "objectID": "pstat197a/slides/week1-perspectives.html#design-principles-transparency",
    "href": "pstat197a/slides/week1-perspectives.html#design-principles-transparency",
    "title": "On data science",
    "section": "Design principles: transparency",
    "text": "Design principles: transparency"
  },
  {
    "objectID": "pstat197a/slides/week1-perspectives.html#design-principles-reproducibility",
    "href": "pstat197a/slides/week1-perspectives.html#design-principles-reproducibility",
    "title": "On data science",
    "section": "Design principles: reproducibility",
    "text": "Design principles: reproducibility"
  },
  {
    "objectID": "pstat197a/slides/week1-perspectives.html#next-time",
    "href": "pstat197a/slides/week1-perspectives.html#next-time",
    "title": "On data science",
    "section": "Next time",
    "text": "Next time\nWe’ll do a github icebreaker activity.\n\nComplete lab activity from Wednesday section meeting\nBring laptops"
  },
  {
    "objectID": "pstat197a/slides/week1-perspectives.html#references",
    "href": "pstat197a/slides/week1-perspectives.html#references",
    "title": "On data science",
    "section": "References",
    "text": "References\n\n\n\n\n\n\nDonoho, David. 2017. “50 Years of Data Science.” Journal of Computational and Graphical Statistics 26 (4): 745–66.\n\n\nEmmert-Streib, Frank, Zhen Yang, Han Feng, Shailesh Tripathi, and Matthias Dehmer. 2020. “An Introductory Review of Deep Learning for Prediction Models with Big Data.” Frontiers in Artificial Intelligence 3: 4.\n\n\nMcGowan, Lucy D’Agostino, Roger D Peng, and Stephanie C Hicks. 2021. “Design Principles for Data Analysis.” arXiv Preprint arXiv:2103.05689.\n\n\nPeng, Roger D, and Hilary S Parker. 2022. “Perspective on Data Science.” Annual Review of Statistics and Its Application 9: 1–20.\n\n\nTukey, John W. 1962. “The Future of Data Analysis.” The Annals of Mathematical Statistics 33 (1): 1–67.\n\n\nTukey, John W et al. 1977. Exploratory Data Analysis. Vol. 2. Reading, MA."
  },
  {
    "objectID": "pstat197a/slides/week6-planning.html#todays-agenda",
    "href": "pstat197a/slides/week6-planning.html#todays-agenda",
    "title": "",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\nDiscuss Winter/Spring enrollment\nDemo assignment repository\nGroup meetings"
  },
  {
    "objectID": "pstat197a/slides/week6-planning.html#capstone-projects",
    "href": "pstat197a/slides/week6-planning.html#capstone-projects",
    "title": "",
    "section": "Capstone projects",
    "text": "Capstone projects\n\n10 projects, half industry, half lab\nproject abstracts will be distributed following Thanksgiving\nyou get to rank preferred projects\nassignments made over the break"
  },
  {
    "objectID": "pstat197a/slides/week6-planning.html#expectations",
    "href": "pstat197a/slides/week6-planning.html#expectations",
    "title": "",
    "section": "Expectations",
    "text": "Expectations\n\ncommit to two terms of work\n\ncompose and sign a team contract defining project roles and teamwork expectations\n\n4 credits per term \\(\\approx\\) 12hr/week commitment\n\none 1.25hr class meeting per week\none 1hr project meeting per week with sponsor and mentor and one 1hr+ student team meeting per week\n~2hr of meeting prep and ‘soft’ tasks per week\n5hr of project work per week\n~2hr flex time"
  },
  {
    "objectID": "pstat197a/slides/week6-planning.html#timeline",
    "href": "pstat197a/slides/week6-planning.html#timeline",
    "title": "",
    "section": "Timeline",
    "text": "Timeline\n\nnow: fill out enrollment request form (even if you’re not sure about commitment right now)\nend of fall: review project abstracts and specify preferences\nwinter break: receive assignments and develop motivational statement\nwinter quarter: develop team contract and begin work\n\npeer review and reflection at midquarter, start of spring, and mid-spring\n\nend of spring: poster showcase on campus"
  },
  {
    "objectID": "pstat197a/slides/week6-planning.html#group-assignment",
    "href": "pstat197a/slides/week6-planning.html#group-assignment",
    "title": "",
    "section": "Group assignment",
    "text": "Group assignment\nSit with your team.\nGo to the table number matching your team number mod 10."
  },
  {
    "objectID": "pstat197a/slides/week3-biomarkers.html#announcementsreminders",
    "href": "pstat197a/slides/week3-biomarkers.html#announcementsreminders",
    "title": "Multiple testing corrections",
    "section": "Announcements/reminders",
    "text": "Announcements/reminders\n\ndon’t forget to fill out attendance form for each class meeting\n\nbut don’t fill it out if you don’t come to class\n\nfirst group assignment due Friday 10/14 11:59pm PST\n\nplease add your lab scripts from labs 1, 2\nlabs/labN-TITLE-USERNAME.R\n\nsection attendance is expected"
  },
  {
    "objectID": "pstat197a/slides/week3-biomarkers.html#background",
    "href": "pstat197a/slides/week3-biomarkers.html#background",
    "title": "Multiple testing corrections",
    "section": "Background",
    "text": "Background\nLevels of proteins in plasma/serum are altered in autism spectrum disorder (ASD).\n\nGoal: identify a panel of proteins useful as a blood biomarker for early detection of ASD.\n\na ‘panel’ is a handful of tests that help distinguish between conditions\nso in other words, find proteins whose serum levels are predictive of ASD"
  },
  {
    "objectID": "pstat197a/slides/week3-biomarkers.html#dataset",
    "href": "pstat197a/slides/week3-biomarkers.html#dataset",
    "title": "Multiple testing corrections",
    "section": "Dataset",
    "text": "Dataset\nData from Hewitson et al. (2021)\n\nSerum samples from 76 boys with ASD and 78 typically developing (TD) boys, 18 months-8 years of age\nA total of 1,125 proteins were analyzed from each sample\n\n1,317 measured, 192 failed quality control\n(we don’t know which ones failed QC so will use all)"
  },
  {
    "objectID": "pstat197a/slides/week3-biomarkers.html#sample-characteristics",
    "href": "pstat197a/slides/week3-biomarkers.html#sample-characteristics",
    "title": "Multiple testing corrections",
    "section": "Sample characteristics",
    "text": "Sample characteristics\n\nAgeDemographicsComorbiditiesMedications\n\n\n\n\n\n\n\nAttribute\nASD (n = 76)\nTD (n = 78)\n\n\n\n\nAge: mean (SD) years\n5.6 (1.7)\n5.7 (2.0)\n\n\n\n\n\n\n\n\n\n\n\n\nAttribute\nASD (n = 76)\nTD (n = 78)\n\n\n\n\nWhite/Caucasian\n33 (45.2%)\n40 (51.9%)\n\n\nHispanic/Latino\n26 (35.6%)\n6 (7.8%)\n\n\nAfrican American/Black\n3 (4.1%)\n14 (18.2%)\n\n\nAsian or Pacific Islander\n2 (2.6%)\n3 (3.9%)\n\n\nMultiple ethnicities or Other\n9 (12.3%)\n14 (18.2%)\n\n\nNot reported\n3 (4.1%)\n1 (1.2%)\n\n\n\n\n\n\n\n\n\n\n\n\nAttribute\nASD (n = 76)\nTD (n = 78)\n\n\n\n\nNone\n38 (52.8%)\n58 (75.3%)\n\n\nADHD\n2 (2.8%)\n1 (1.3%)\n\n\nSeasonal Allergies\n30 (41.7%)\n17 (22.4%)\n\n\nAsthma\n2 (2.8%)\n0 (0%)\n\n\nCeliac Disease\n1 (1.4%)\n0 (0%)\n\n\nGERD\n1 (1.4%)\n0 (0%)\n\n\nPTSD\n0 (0%)\n1 (1.3%)\n\n\nSleep Apnea\n2 (2.8%)\n0 (0%)\n\n\nNot reported\n4 (5.6%)\n1 (1.3%)\n\n\n\n\n\n\n\n\n\n\n\n\nAttribute\nASD (n = 76)\nTD (n = 78)\n\n\n\n\nNone\n69 (92%)\n75 (97.4%)\n\n\nAnti-depressant\n2 (2.7%)\n0 (0%)\n\n\nAnti-psychotic\n0 (0%)\n1 (1.3%)\n\n\nSedative\n1 (1.3%)\n0 (0%)\n\n\nSSRI\n2 (2.27%)\n0 (0%)\n\n\nStimulant\n1 (1.3%)\n1 (1.3%)\n\n\nNot reported\n1 (1.3%)\n1 (1.3%)"
  },
  {
    "objectID": "pstat197a/slides/week3-biomarkers.html#data-glimpse",
    "href": "pstat197a/slides/week3-biomarkers.html#data-glimpse",
    "title": "Multiple testing corrections",
    "section": "Data glimpse",
    "text": "Data glimpse\n\nExample rowsGroup sizes\n\n\n\nasd_clean %&gt;% head(5)\n\n# A tibble: 5 × 1,318\n  group    CHIP  CEBPB     NSE   PIAS4 `IL-10 Ra`  STAT3   IRF1 `c-Jun` `Mcl-1`\n  &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 ASD    0.335   0.520 -0.554   0.650      -0.358  0.305 -0.484   0.309  1.57  \n2 ASD   -0.0715  1.01   3       1.28       -0.133  1.13   0.253   0.408  0.0643\n3 ASD   -0.406  -0.531 -0.0592  1.13        0.554 -0.334  0.287  -0.845  1.42  \n4 ASD   -0.102  -0.251  1.47    0.0773     -0.705  0.893  2.61   -0.372 -0.467 \n5 ASD   -0.395  -0.536  0.0410 -0.299      -0.830  0.899  1.01   -0.843 -1.15  \n# … with 1,308 more variables: OAS1 &lt;dbl&gt;, `c-Myc` &lt;dbl&gt;, SMAD3 &lt;dbl&gt;,\n#   SMAD2 &lt;dbl&gt;, `IL-23` &lt;dbl&gt;, PDGFRA &lt;dbl&gt;, `IL-12` &lt;dbl&gt;, STAT1 &lt;dbl&gt;,\n#   STAT6 &lt;dbl&gt;, LRRK2 &lt;dbl&gt;, Osteocalcin &lt;dbl&gt;, `IL-5` &lt;dbl&gt;, GPDA &lt;dbl&gt;,\n#   IgA &lt;dbl&gt;, LPPL &lt;dbl&gt;, HEMK2 &lt;dbl&gt;, PDXK &lt;dbl&gt;, TLR4 &lt;dbl&gt;, REG4 &lt;dbl&gt;,\n#   `HSP 27` &lt;dbl&gt;, `YKL-40` &lt;dbl&gt;, `Alpha enolase` &lt;dbl&gt;, `Apo L1` &lt;dbl&gt;,\n#   CD38 &lt;dbl&gt;, CD59 &lt;dbl&gt;, FABPL &lt;dbl&gt;, `GDF-11` &lt;dbl&gt;, BTC &lt;dbl&gt;,\n#   `HIF-1a` &lt;dbl&gt;, S100A6 &lt;dbl&gt;, SECTM1 &lt;dbl&gt;, RSPO3 &lt;dbl&gt;, PSP &lt;dbl&gt;, …\n\n\n\n\n\n\n# A tibble: 2 × 2\n  group     n\n  &lt;chr&gt; &lt;int&gt;\n1 ASD      76\n2 TD       78"
  },
  {
    "objectID": "pstat197a/slides/week3-biomarkers.html#module-objectives",
    "href": "pstat197a/slides/week3-biomarkers.html#module-objectives",
    "title": "Multiple testing corrections",
    "section": "Module objectives",
    "text": "Module objectives\nMethodology\n\nmultiple testing\nclassification: logistic regression; random forests\nvariable selection: LASSO regularization\nclassification accuracy measures\n\n\nConcepts\n\ndata partitioning for predictive modeling\nmodel interpretability\nhigh dimensional data \\(n &lt; p\\)"
  },
  {
    "objectID": "pstat197a/slides/week3-biomarkers.html#marginal-differences",
    "href": "pstat197a/slides/week3-biomarkers.html#marginal-differences",
    "title": "Multiple testing corrections",
    "section": "Marginal differences",
    "text": "Marginal differences\nIdea: test for a significant difference in serum levels between groups for a given protein, say protein \\(i\\).\n\nNotation:\n\n\\(\\mu^i_{ASD}\\): mean serum level of protein \\(i\\) in the ASD group\n\\(\\mu^i_{TD}\\): mean serum level of protein \\(i\\) in the TD group\n\\(\\delta_i\\): difference in means \\(\\mu^i_{ASD} - \\mu^i_{TD}\\)\nhats indicate sample estimates (e.g. \\(\\hat{\\delta}_i\\))"
  },
  {
    "objectID": "pstat197a/slides/week3-biomarkers.html#review-t-test",
    "href": "pstat197a/slides/week3-biomarkers.html#review-t-test",
    "title": "Multiple testing corrections",
    "section": "Review: \\(t\\)-test",
    "text": "Review: \\(t\\)-test\nThe \\(t\\)-test tests \\(H_{0i}: \\delta_i = 0\\) against its negation \\(\\neg H_{0i}: \\delta_i \\neq 0\\) using the rule\n\\[\n\\text{reject $H_{0i}$ if}\\qquad \\left|\\frac{\\hat{\\delta}_i}{SE(\\hat{\\delta}_i)}\\right| &gt; t_\\alpha\n\\]\n\n\\(SE(\\hat{\\delta}_i)\\) is a standard error for the difference estimate; quantifies variability of the estimate\nprocedure controls type I error at \\(\\alpha\\), ensuring \\(P\\left(\\text{reject}_i|H_i\\right) \\leq 0.05\\)"
  },
  {
    "objectID": "pstat197a/slides/week3-biomarkers.html#review-p-values",
    "href": "pstat197a/slides/week3-biomarkers.html#review-p-values",
    "title": "Multiple testing corrections",
    "section": "Review: \\(p\\)-values",
    "text": "Review: \\(p\\)-values\nThe \\(p\\)-value for a test is the probability of obtaining a sample at least as contrary to \\(H_{0i}\\) as the sample in hand, assuming \\(H_{0i}\\) is true.\n\nBy construction, \\(p &lt; \\alpha\\) just in case the test rejects with type I error controlled at \\(\\alpha\\).\n\n\nSo a common heuristic is:\n\\[\n\\text{reject $H_{0i}$ if} \\qquad p_i \\leq \\alpha\n\\]"
  },
  {
    "objectID": "pstat197a/slides/week3-biomarkers.html#one-test",
    "href": "pstat197a/slides/week3-biomarkers.html#one-test",
    "title": "Multiple testing corrections",
    "section": "One test",
    "text": "One test\nHere is R output for one test.\n\nasd %&gt;%\n  t_test(formula = CHIP ~ group,\n         order = c('ASD', 'TD'),\n         alternative = 'two-sided',\n         var.equal = F)\n\n# A tibble: 1 × 7\n  statistic  t_df p_value alternative estimate lower_ci upper_ci\n      &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1     0.927  75.7   0.357 two.sided       384.    -441.    1210.\n\n\n\nQuestions:\n\nWhat are the hypotheses in words?\nWhat are the test assumptions?\nWhat is the conclusion of the test?"
  },
  {
    "objectID": "pstat197a/slides/week3-biomarkers.html#many-tests",
    "href": "pstat197a/slides/week3-biomarkers.html#many-tests",
    "title": "Multiple testing corrections",
    "section": "Many tests",
    "text": "Many tests\nA plausible approach for identifying a protein panel, then, is to select all those proteins for which the \\(t\\)-test indicates a significant difference.\n\n1,317 tests\neasy to compute\nconceptually straightforward\n\n\nHow likely are mistakes?"
  },
  {
    "objectID": "pstat197a/slides/week3-biomarkers.html#test-outcomes",
    "href": "pstat197a/slides/week3-biomarkers.html#test-outcomes",
    "title": "Multiple testing corrections",
    "section": "Test outcomes",
    "text": "Test outcomes\nLet \\(H_i\\) denote the \\(i\\)th null hypothesis and \\(R_i\\) denote the event that \\(H_i\\) is rejected.\n\n\n\n\n\n\\(H_i\\)\n\\(\\neg H_i\\)\n\n\n\n\n\\(R_i\\)\n\\(V\\) false rejections\n\\(S\\) correct\n\n\n\\(\\neg R_i\\)\n\\(T\\) correct\n\\(W\\) false non-rejections\n\n\n\n\n\nThe multiple testing problem is that individual error rates compound over multiple tests."
  },
  {
    "objectID": "pstat197a/slides/week3-biomarkers.html#familywise-error",
    "href": "pstat197a/slides/week3-biomarkers.html#familywise-error",
    "title": "Multiple testing corrections",
    "section": "Familywise error",
    "text": "Familywise error\nFamilywise error rate (FWER) is the probability of one or more type I errors: \\(P(V \\geq 1)\\).\n\nSuppose there are \\(m\\) true hypotheses \\(\\mathcal{H}: \\{H_i: i \\in C\\}\\).\n\n\nIf the tests are independent and exact then:\n\\[\n\\begin{aligned}\nP(V \\geq 1)\n&= P\\left[ \\bigcup_{i \\in C} R_i | \\mathcal{H} \\right] \\\\\n&= 1 - \\prod_{i \\in C} \\left( 1- P(R_i|H_i) \\right) \\\\\n&= 1 - (1 - \\alpha)^m\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "pstat197a/slides/week3-biomarkers.html#fwer-example",
    "href": "pstat197a/slides/week3-biomarkers.html#fwer-example",
    "title": "Multiple testing corrections",
    "section": "FWER Example",
    "text": "FWER Example\nIf individual tests are exactly controlled at \\(\\alpha = 0.05\\) and independent, at least one error is nearly certain by 100 tests.\n\nFamilywise error rate as a function of the number of tests, assuming tests are independent with exact type I error 0.05."
  },
  {
    "objectID": "pstat197a/slides/week3-biomarkers.html#bonferroni-correction",
    "href": "pstat197a/slides/week3-biomarkers.html#bonferroni-correction",
    "title": "Multiple testing corrections",
    "section": "Bonferroni correction",
    "text": "Bonferroni correction\nThe simplest multiple testing correction is based on the Bonferroni inequality:\n\\[\nP\\left[ \\bigcup_{i \\in C} R_i | \\mathcal{H} \\right] \\leq \\sum_{i \\in C} P(R_i|\\mathcal{H})\n\\]\n\nIf the individual tests are controlled at level \\(\\alpha\\), then \\(FWER \\leq m\\alpha\\).\n\n\nSo a simple solution is to test at level \\(\\alpha^* = \\frac{\\alpha}{m}\\).\n\n\nIn other words, reject if \\(p_i &lt; \\frac{\\alpha}{m}\\)."
  },
  {
    "objectID": "pstat197a/slides/week3-biomarkers.html#false-discovery-rate",
    "href": "pstat197a/slides/week3-biomarkers.html#false-discovery-rate",
    "title": "Multiple testing corrections",
    "section": "False discovery rate",
    "text": "False discovery rate\nFWER control will limit false rejections, but at the cost of power; controlling the probability of one type I error is a conservative approach.\n\nMore common in modern applications are procedures to control false discovery rate: the expected proportion of rejections that are false.\n\\[\n\\text{FDR} = \\mathbb{E}\\left[\\frac{\\text{false rejections}}{\\text{total rejections}}\\right]\n\\]\n\n\nConceptually, if say FDR is controlled at \\(0.05\\), then one would expect 5% of rejections to be false."
  },
  {
    "objectID": "pstat197a/slides/week3-biomarkers.html#benjamini-hochberg-correction",
    "href": "pstat197a/slides/week3-biomarkers.html#benjamini-hochberg-correction",
    "title": "Multiple testing corrections",
    "section": "Benjamini-Hochberg correction",
    "text": "Benjamini-Hochberg correction\nBenjamini and Hochberg (1995) conceived a procedure based on sorting \\(p\\)-values.\n\nSupposing \\(m\\) independent tests are performed:\n\nSort the \\(p\\)-values in increasing order \\(p_{(1)}, p_{(2)}, \\dots, p_{(m)}\\)\nReject whenever \\(p_{(i)} &lt; \\frac{i\\alpha}{m}\\)\n\n\n\nThey proved that this controls FDR at \\(\\alpha\\)."
  },
  {
    "objectID": "pstat197a/slides/week3-biomarkers.html#benjamini-yekutieli-correction",
    "href": "pstat197a/slides/week3-biomarkers.html#benjamini-yekutieli-correction",
    "title": "Multiple testing corrections",
    "section": "Benjamini-Yekutieli correction",
    "text": "Benjamini-Yekutieli correction\nThe Benjamini-Hochberg assumes tests are independent, which is obviously not true in most situations. (Why?)\n\nBenjamini and Yekutieli (2001) modified the correction to hold without the independence assumption:\n\nSort the \\(p\\)-values in increasing order \\(p_{(1)}, p_{(2)}, \\dots, p_{(m)}\\)\nReject whenever \\(p_{(i)} &lt; \\frac{i\\alpha}{m H_m}\\)\n\n\n\nAbove, \\(H_m = \\sum_{i = 1}^m \\frac{1}{i}\\) ."
  },
  {
    "objectID": "pstat197a/slides/week3-biomarkers.html#implementing-corrections",
    "href": "pstat197a/slides/week3-biomarkers.html#implementing-corrections",
    "title": "Multiple testing corrections",
    "section": "Implementing corrections",
    "text": "Implementing corrections\nThe easiest way to implement these corrections is to adjust the \\(p\\)-values with a multiplier:\n\n(Bonferroni) \\(p^b_i = m\\times p_i\\)\n(Benjamini-Hochberg) \\(p^{bh}_{(i)} = \\frac{m}{i} p_{(i)}\\)\n(Benjamini-Yekuteili) \\(p^{bh}_{(i)} = \\frac{m H_m}{i} p_{(i)}\\)"
  },
  {
    "objectID": "pstat197a/slides/week3-biomarkers.html#computations",
    "href": "pstat197a/slides/week3-biomarkers.html#computations",
    "title": "Multiple testing corrections",
    "section": "Computations",
    "text": "Computations\n\nPreprocessingAssumptionsTestsCorrections\n\n\n\ntrim_fn &lt;- function(x){\n  x[x &gt; 3] &lt;- 3\n  x[x &lt; -3] &lt;- -3\n  \n  return(x)\n}\n\nasd_clean &lt;- asd %&gt;% \n  select(-ados) %&gt;%\n  # log transform\n  mutate(across(.cols = -group, log10)) %&gt;%\n  # center and scale\n  mutate(across(.cols = -group, ~ scale(.x)[, 1])) %&gt;%\n  # trim outliers (affects results??)\n  mutate(across(.cols = -group, trim_fn))\n\nasd_nested &lt;- asd_clean %&gt;%\n  pivot_longer(-group, \n               names_to = 'protein', \n               values_to = 'level') %&gt;%\n  nest(data = c(level, group))\n\nasd_nested %&gt;% head(4)\n\n# A tibble: 4 × 2\n  protein data              \n  &lt;chr&gt;   &lt;list&gt;            \n1 CHIP    &lt;tibble [154 × 2]&gt;\n2 CEBPB   &lt;tibble [154 × 2]&gt;\n3 NSE     &lt;tibble [154 × 2]&gt;\n4 PIAS4   &lt;tibble [154 × 2]&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# compute for several groups\ntest_fn &lt;- function(.df){\n  t_test(.df, \n         formula = level ~ group,\n         order = c('ASD', 'TD'),\n         alternative = 'two-sided',\n         var.equal = F)\n}\n\ntt_out &lt;- asd_nested %&gt;%\n  mutate(ttest = map(data, test_fn)) %&gt;%\n  unnest(ttest) %&gt;%\n  arrange(p_value)\n\ntt_out %&gt;% head(5)\n\n# A tibble: 5 × 9\n  protein     data     statistic  t_df     p_value alternative estimate lower_ci\n  &lt;chr&gt;       &lt;list&gt;       &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt;\n1 DERM        &lt;tibble&gt;     -6.10  151.     8.27e-9 two.sided     -0.885    -1.17\n2 RELT        &lt;tibble&gt;     -5.65  152.     7.82e-8 two.sided     -0.775    -1.05\n3 FSTL1       &lt;tibble&gt;     -5.27  152.     4.66e-7 two.sided     -0.783    -1.08\n4 C1QR1       &lt;tibble&gt;     -5.26  152.     4.79e-7 two.sided     -0.782    -1.08\n5 Calcineurin &lt;tibble&gt;     -5.24  151.     5.37e-7 two.sided     -0.734    -1.01\n# … with 1 more variable: upper_ci &lt;dbl&gt;\n\n\n\n\n\n# multiple testing corrections\nm &lt;- nrow(tt_out)\nhm &lt;- log(m) + 1/(2*m) - digamma(1)\n  \ntt_corrected &lt;- tt_out %&gt;%\n  select(data, protein, p_value) %&gt;%\n  mutate(rank = row_number()) %&gt;%\n  mutate(p_bh = p_value*m/rank,\n         p_by = p_value*m*hm/rank,\n         p_bonf = p_value*m)\n\ntt_corrected %&gt;% head(5)\n\n# A tibble: 5 × 7\n  data               protein           p_value  rank      p_bh      p_by  p_bonf\n  &lt;list&gt;             &lt;chr&gt;               &lt;dbl&gt; &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 &lt;tibble [154 × 2]&gt; DERM        0.00000000827     1 0.0000109 0.0000845 1.09e-5\n2 &lt;tibble [154 × 2]&gt; RELT        0.0000000782      2 0.0000515 0.000400  1.03e-4\n3 &lt;tibble [154 × 2]&gt; FSTL1       0.000000466       3 0.000205  0.00159   6.14e-4\n4 &lt;tibble [154 × 2]&gt; C1QR1       0.000000479       4 0.000158  0.00122   6.31e-4\n5 &lt;tibble [154 × 2]&gt; Calcineurin 0.000000537       5 0.000141  0.00110   7.07e-4"
  },
  {
    "objectID": "pstat197a/slides/week3-biomarkers.html#results",
    "href": "pstat197a/slides/week3-biomarkers.html#results",
    "title": "Multiple testing corrections",
    "section": "Results",
    "text": "Results\n\nComparing methodsTop 10 proteins\n\n\n\n\n\n\n\nAdjusted vs. raw p-values for each multiple correction method.\n\n\n\n\n\n\n\n# top 10\ntt_corrected %&gt;%\n  select(protein, p_by) %&gt;%\n  slice_min(order_by = p_by, n = 10)\n\n# A tibble: 10 × 2\n   protein              p_by\n   &lt;chr&gt;               &lt;dbl&gt;\n 1 DERM            0.0000845\n 2 RELT            0.000400 \n 3 Calcineurin     0.00110  \n 4 C1QR1           0.00122  \n 5 MRC2            0.00132  \n 6 IgD             0.00136  \n 7 CXCL16, soluble 0.00149  \n 8 PTN             0.00154  \n 9 FSTL1           0.00159  \n10 Cadherin-5      0.00179"
  },
  {
    "objectID": "pstat197a/slides/week3-biomarkers.html#neat-graphic-volcano-plot",
    "href": "pstat197a/slides/week3-biomarkers.html#neat-graphic-volcano-plot",
    "title": "Multiple testing corrections",
    "section": "Neat graphic: volcano plot",
    "text": "Neat graphic: volcano plot\n\nUpregulation and downregulation of serum levels of proteins analyzed – p-values against number of doublings (positive) or halvings (negative) of serum level in ASD group relative to TD group."
  },
  {
    "objectID": "pstat197a/slides/week3-biomarkers.html#next-time",
    "href": "pstat197a/slides/week3-biomarkers.html#next-time",
    "title": "Multiple testing corrections",
    "section": "Next time",
    "text": "Next time\nOther approaches to the same problem:\n\ncorrelation with ADOS (severity diagnostic score)\nvariable importance in random forest classifier"
  },
  {
    "objectID": "pstat197a/slides/week3-biomarkers.html#references",
    "href": "pstat197a/slides/week3-biomarkers.html#references",
    "title": "Multiple testing corrections",
    "section": "References",
    "text": "References\n\n\n\n\n\n\nBenjamini, Yoav, and Yosef Hochberg. 1995. “Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing.” Journal of the Royal Statistical Society: Series B (Methodological) 57 (1): 289–300.\n\n\nBenjamini, Yoav, and Daniel Yekutieli. 2001. “The Control of the False Discovery Rate in Multiple Testing Under Dependency.” Annals of Statistics, 1165–88.\n\n\nHewitson, Laura, Jeremy A Mathews, Morgan Devlin, Claire Schutte, Jeon Lee, and Dwight C German. 2021. “Blood Biomarker Discovery for Autism Spectrum Disorder: A Proteomic Analysis.” PLoS One 16 (2): e0246581."
  },
  {
    "objectID": "pstat197a/slides/week9-claimsresults.html#todays-agenda",
    "href": "pstat197a/slides/week9-claimsresults.html#todays-agenda",
    "title": "Results from claims assignment",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\nReview results of claims assignment\nDiscuss vignette guidelines\nRemaining time to organize in groups"
  },
  {
    "objectID": "pstat197a/slides/week9-claimsresults.html#preface",
    "href": "pstat197a/slides/week9-claimsresults.html#preface",
    "title": "Results from claims assignment",
    "section": "Preface",
    "text": "Preface\nMy goal here is to use the results of your work on the last assignment to learn as a group what worked well and what didn’t.\n\nAs a disclaimer, many groups did not successfully generate predictions according to instructions. This is okay.\n\n\nMy goal is not to judge anyone based on results or having completed the assignment."
  },
  {
    "objectID": "pstat197a/slides/week9-claimsresults.html#accessing-claims-predictions",
    "href": "pstat197a/slides/week9-claimsresults.html#accessing-claims-predictions",
    "title": "Results from claims assignment",
    "section": "Accessing claims predictions",
    "text": "Accessing claims predictions\nResults from the last assignment are now available online.\n\nlibrary(tidyverse)\n\ngithub_url &lt;- \"https://github.com/pstat197/pstat197a/raw/main/materials/slides/data/f22-claims-evals.RData\"\n\nload(url(github_url))\n\nevals %&gt;% head()\n\n# A tibble: 6 × 2\n  group eval            \n  &lt;int&gt; &lt;list&gt;          \n1     1 &lt;smplErrr&gt;      \n2     2 &lt;tibble [6 × 5]&gt;\n3     3 &lt;tibble [6 × 5]&gt;\n4     4 &lt;rlng_rrr&gt;      \n5     5 &lt;tibble [6 × 5]&gt;\n6     6 &lt;rlng_rrr&gt;"
  },
  {
    "objectID": "pstat197a/slides/week9-claimsresults.html#checking-errors-example",
    "href": "pstat197a/slides/week9-claimsresults.html#checking-errors-example",
    "title": "Results from claims assignment",
    "section": "Checking errors: example",
    "text": "Checking errors: example\n\nevals %&gt;%\n  filter(group == 4) %&gt;%\n  pull(eval)\n\n[[1]]\n&lt;error/rlang_error&gt;\nError in `metric_set()`:\n! Failed to compute `sensitivity()`.\nCaused by error in `dplyr::summarise()`:\n! Problem while computing `.estimate = metric_fn(...)`.\n---\nBacktrace:\n  1. tibble(group = 1:15) %&gt;% ...\n 15. yardstick (local) panel(., truth = mclass, estimate = mclass.pred, estimator = \"macro\")\n 16. base::mapply(...)\n 17. yardstick (local) `&lt;fn&gt;`(dots[[1L]][[1L]], dots[[2L]][[1L]])\nCaused by error in `multiclass_checks()`:\n! `truth` and `estimate` levels must be equivalent.\n`truth`: N/A: No relevant content., Physical Activity, Possible Fatality, Potentially unlawful activity, Other claim content\n`estimate`: N/A: No relevant content., Other claim content, Physical Activity, Possible Fatality, Potentially unlawful activity\n---\nBacktrace:\n  1. tibble(group = 1:15) %&gt;% ...\n 34. yardstick (local) metric_fn(...)\n 35. yardstick::metric_vec_template(...)\n 36. yardstick:::validate_truth_estimate_checks(...)\n 38. yardstick:::validate_truth_estimate_types.factor(...)\n 40. yardstick:::multiclass_checks.factor(truth, estimate)"
  },
  {
    "objectID": "pstat197a/slides/week9-claimsresults.html#accuracies",
    "href": "pstat197a/slides/week9-claimsresults.html#accuracies",
    "title": "Results from claims assignment",
    "section": "Accuracies",
    "text": "Accuracies\n\naccuracies &lt;- evals %&gt;%\n  rowwise() %&gt;%\n  filter(is_tibble(eval)) %&gt;%\n  ungroup() %&gt;%\n  unnest(eval)\n\naccuracies %&gt;% head(8) %&gt;% knitr::kable()\n\n\n\n\ngroup\n.metric\n.estimator\n.estimate\nclass\nn\n\n\n\n\n2\nsensitivity\nmacro\n0.8081203\nmulticlass\n99\n\n\n2\nspecificity\nmacro\n0.9370310\nmulticlass\n99\n\n\n2\naccuracy\nmulticlass\n0.8181818\nmulticlass\n99\n\n\n2\nsensitivity\nbinary\n0.8431373\nbinary\n99\n\n\n2\nspecificity\nbinary\n0.8750000\nbinary\n99\n\n\n2\naccuracy\nbinary\n0.8585859\nbinary\n99\n\n\n3\nsensitivity\nmacro\n0.2058975\nmulticlass\n828\n\n\n3\nspecificity\nmacro\n0.8035735\nmulticlass\n828"
  },
  {
    "objectID": "pstat197a/slides/week9-claimsresults.html#comparisons",
    "href": "pstat197a/slides/week9-claimsresults.html#comparisons",
    "title": "Results from claims assignment",
    "section": "Comparisons",
    "text": "Comparisons"
  },
  {
    "objectID": "pstat197a/slides/week9-claimsresults.html#vignette-guidelines",
    "href": "pstat197a/slides/week9-claimsresults.html#vignette-guidelines",
    "title": "Results from claims assignment",
    "section": "Vignette guidelines",
    "text": "Vignette guidelines\nLet’s review as a group."
  },
  {
    "objectID": "pstat197a/slides/week0-intro.html#before-we-begin",
    "href": "pstat197a/slides/week0-intro.html#before-we-begin",
    "title": "Course orientation",
    "section": "Before we begin…",
    "text": "Before we begin…\n\nConfer with your table and choose a word of the day. Agree on spelling.\nPlease sign in using the attendance reporting form found here:\nhttps://pstat197.github.io/pstat197a/about/links.html"
  },
  {
    "objectID": "pstat197a/slides/week0-intro.html#welcome",
    "href": "pstat197a/slides/week0-intro.html#welcome",
    "title": "Course orientation",
    "section": "Welcome",
    "text": "Welcome\nPSTAT197A/CMPSC190DD is the first course in UCSB’s year-long data science capstone sequence.\n\nAudience: undergraduate students of any discipline with a basic background in data science and an interest in research\nAim: prepare for an independent research or project experience"
  },
  {
    "objectID": "pstat197a/slides/week0-intro.html#capstone-projects",
    "href": "pstat197a/slides/week0-intro.html#capstone-projects",
    "title": "Course orientation",
    "section": "Capstone projects",
    "text": "Capstone projects\nMost students are preparing for capstone projects in winter and spring. Course foci were chosen with this in mind.\n\nProjects are varied ➜ emphasize problem patterns over methodology\nProjects are collaborative ➜ emphasize teamwork and discussion\nProjects are specialized ➜ practice independent learning based on use cases\n\nRead about past projects at https://centralcoastdatascience.org/projects"
  },
  {
    "objectID": "pstat197a/slides/week0-intro.html#continuing-in-capstones",
    "href": "pstat197a/slides/week0-intro.html#continuing-in-capstones",
    "title": "Course orientation",
    "section": "Continuing in capstones",
    "text": "Continuing in capstones\nContinuation in PSTAT197B-C/CMPSC190DE-DF during winter and spring:\n\nstudents admitted to this course in spring have a seat;\nstudents admitted from the waitlist are on the waitlist."
  },
  {
    "objectID": "pstat197a/slides/week0-intro.html#outcomes",
    "href": "pstat197a/slides/week0-intro.html#outcomes",
    "title": "Course orientation",
    "section": "Outcomes",
    "text": "Outcomes\nI hope to support all of you in:\n\nusing modern software with version control for collaboration;\nrecognizing problem patterns based on data semantics and research questions;\nidentifying and accessing resources for independent learning given a problem of interest;\ncommunicating data analysis and/or research findings."
  },
  {
    "objectID": "pstat197a/slides/week0-intro.html#classroom-environment",
    "href": "pstat197a/slides/week0-intro.html#classroom-environment",
    "title": "Course orientation",
    "section": "Classroom environment",
    "text": "Classroom environment\nWe are in an interactive classroom for a reason: to interact!\nLet’s acknowledge:\n\nPreparations and areas of expertise vary widely among the class\nIt’s okay not to know things\nIf you have a question, probably someone else does too"
  },
  {
    "objectID": "pstat197a/slides/week0-intro.html#resources",
    "href": "pstat197a/slides/week0-intro.html#resources",
    "title": "Course orientation",
    "section": "Resources",
    "text": "Resources\nAll course content is hosted on our website\nhttps://pstat197.github.io/pstat197a/"
  },
  {
    "objectID": "pstat197a/slides/week0-intro.html#modules",
    "href": "pstat197a/slides/week0-intro.html#modules",
    "title": "Course orientation",
    "section": "Modules",
    "text": "Modules\nThe course is configured in modules defined by a dataset and questions (much like a project).\nA module typically comprises:\n\nOne session on data introduction (lecture/discussion)\nTwo sessions on problem patterns and related methodology (lecture)\nTwo labs with related examples (section meeting)\nOne session on sharing data analysis results (discussion)"
  },
  {
    "objectID": "pstat197a/slides/week0-intro.html#module-content",
    "href": "pstat197a/slides/week0-intro.html#module-content",
    "title": "Course orientation",
    "section": "Module content",
    "text": "Module content\nThe module datasets are currently as follows:\n\nClass intake survey data (exploratory/descriptive analysis)\nBiomarkers of autism (predictive modeling and variable selection)\nWeb fraud (text processing and deep learning)\nSoil temperatures (correlated data)"
  },
  {
    "objectID": "pstat197a/slides/week0-intro.html#group-assignments",
    "href": "pstat197a/slides/week0-intro.html#group-assignments",
    "title": "Course orientation",
    "section": "Group assignments",
    "text": "Group assignments\nEach module you will be assigned a working group.\nYour group’s objective is to produce an analysis of the dataset:\n\nReproduce analysis presented/discussed in class meeting\nExtend the analysis by\n\napplying an alternative method that addresses the same question(s)\nor addressing a corollary question"
  },
  {
    "objectID": "pstat197a/slides/week0-intro.html#vignettes",
    "href": "pstat197a/slides/week0-intro.html#vignettes",
    "title": "Course orientation",
    "section": "Vignettes",
    "text": "Vignettes\nAt the end of the class in place of a fifth module you will create a vignette (short demonstration) on a topic of interest.\n\npresent a use case\nexplain methodology\ndemonstrate implementation with example code"
  },
  {
    "objectID": "pstat197a/slides/week0-intro.html#expectations-and-assessments",
    "href": "pstat197a/slides/week0-intro.html#expectations-and-assessments",
    "title": "Course orientation",
    "section": "Expectations and assessments",
    "text": "Expectations and assessments\n\n\nStudents are expected to:\n\nprepare for class meetings as directed;\nattend and actively participate in class and section meetings;\ncontribute meaningfully to group activities and assignments.\n\n\nStudents are assessed on:\n\nattendance, preparation, and participation;\nquality of submitted work;\nindividual contributions to group assignments;\noral interview/presentation."
  },
  {
    "objectID": "pstat197a/slides/week0-intro.html#next-time",
    "href": "pstat197a/slides/week0-intro.html#next-time",
    "title": "Course orientation",
    "section": "Next time",
    "text": "Next time\nWe’ll discuss:\n\ndata science as a discipline;\nthe research landscape;\nsystems and design thinking for data science."
  },
  {
    "objectID": "pstat197a/slides/week0-intro.html#checklist",
    "href": "pstat197a/slides/week0-intro.html#checklist",
    "title": "Course orientation",
    "section": "Checklist",
    "text": "Checklist\nComplete all of the following before our next meeting.\n\nReview all content in the about section of the course webpage.\nInstall course software and create a GitHub account.\nFill out capstone project intake form.\nRead Peng, R. D., & Parker, H. S. (2022). Perspective on data science. Annual Review of Statistics and Its Application, 9, 1-20. (access online via UCSB library).\nPrepare a reading response."
  },
  {
    "objectID": "pstat197a/slides/week6-nn.html#announcementsreminders",
    "href": "pstat197a/slides/week6-nn.html#announcementsreminders",
    "title": "Artificial Neural Networks",
    "section": "Announcements/reminders",
    "text": "Announcements/reminders\nBefore your section meetings this week:\n\ninstall python\ncomplete pre-lab activity (to be posted with lab)\n\n\nPart of your assignment this week: fill out midquarter self-evaluation.\n\n\nAdd code request form for capstones later this week. Think now about whether you’re interested and check the schedule."
  },
  {
    "objectID": "pstat197a/slides/week6-nn.html#discuss-with-your-table-3min",
    "href": "pstat197a/slides/week6-nn.html#discuss-with-your-table-3min",
    "title": "Artificial Neural Networks",
    "section": "Discuss with your table (3min)",
    "text": "Discuss with your table (3min)\nWhat do you know (or have you heard) about neural networks?\n\nwhat are they\nwhat are they used for\nany other info?"
  },
  {
    "objectID": "pstat197a/slides/week6-nn.html#graphical-model-diagram",
    "href": "pstat197a/slides/week6-nn.html#graphical-model-diagram",
    "title": "Artificial Neural Networks",
    "section": "Graphical model diagram",
    "text": "Graphical model diagram\nConsider an arbitrary statistical model with one response \\(Y\\) and three predictors \\(x_1, x_2, x_3\\).\n\nA simple diagram of the model would look like this:\n\n\n\n\n\n\n\nG\n\n \n\ncluster_0\n\n predictors  \n\ncluster_2\n\n response   \n\nin1\n\n x1   \n\no1\n\n Y   \n\nin1-&gt;o1\n\n    \n\nin2\n\n x2   \n\nin2-&gt;o1\n\n    \n\nin3\n\n x3   \n\nin3-&gt;o1"
  },
  {
    "objectID": "pstat197a/slides/week6-nn.html#graph-layers",
    "href": "pstat197a/slides/week6-nn.html#graph-layers",
    "title": "Artificial Neural Networks",
    "section": "Graph layers",
    "text": "Graph layers\nA model that maps predictors directly to the response has just two “layers”:\n\nan input layer \\(X\\)\nan output layer \\(Y\\) (or more accurately \\(\\mathbb{E}Y\\))\n\n\nNeural networks add layers between the input and output."
  },
  {
    "objectID": "pstat197a/slides/week6-nn.html#vanilla-neural-network",
    "href": "pstat197a/slides/week6-nn.html#vanilla-neural-network",
    "title": "Artificial Neural Networks",
    "section": "‘Vanilla’ neural network",
    "text": "‘Vanilla’ neural network\n\n\n\n\n\n\n\n\n\nG\n\n \n\ncluster_0\n\n    \n\ncluster_2\n\n    \n\ncluster_1\n\n     \n\nin1\n\n     \n\nh1\n\n     \n\nin1-&gt;h1\n\n    \n\nh2\n\n     \n\nin1-&gt;h2\n\n    \n\nh3\n\n     \n\nin1-&gt;h3\n\n    \n\nh4\n\n     \n\nin1-&gt;h4\n\n    \n\nin2\n\n     \n\nin2-&gt;h1\n\n    \n\nin2-&gt;h2\n\n    \n\nin2-&gt;h3\n\n    \n\nin2-&gt;h4\n\n    \n\nin3\n\n     \n\nin3-&gt;h1\n\n    \n\nin3-&gt;h2\n\n    \n\nin3-&gt;h3\n\n    \n\nin3-&gt;h4\n\n    \n\no1\n\n     \n\nh1-&gt;o1\n\n    \n\nh2-&gt;o1\n\n    \n\nh3-&gt;o1\n\n    \n\nh4-&gt;o1\n\n   \n\n\n\n\n\n\n\none input layer\none hidden layer\none output layer\none parameter per edge"
  },
  {
    "objectID": "pstat197a/slides/week6-nn.html#more-formally",
    "href": "pstat197a/slides/week6-nn.html#more-formally",
    "title": "Artificial Neural Networks",
    "section": "More formally",
    "text": "More formally\nLet \\(Y\\in\\mathbb{R}^n\\) and \\(X \\in \\mathbb{R}^{n\\times p}\\) represent some data. The vanilla neural network is:\n\\[\\begin{aligned}\n\\color{#eed5b7}{\\mathbb{E}Y} &= \\sigma_z(\\color{#7ac5cd}{Z}\\color{#8b3e2f}{\\beta})\n  \\qquad &\\text{output layer}\\\\\n\n\\color{#7ac5cd}{Z} &=\n  \\left[\\sigma_x(\\color{#66cdaa}{X}\\color{#8b3e2f}{\\alpha_1})\n    \\;\\cdots\\;\n    \\sigma_x(\\color{#66cdaa}{X}\\color{#8b3e2f}{\\alpha_M})\\right]\n  \\qquad &\\text{hidden layer} \\\\\n  \n\\color{#66cdaa}{X} &= \\left[x_1 \\;\\cdots\\; x_p\\right]\n  \\qquad&\\text{input layer}\n\\end{aligned}\\]\n\n\\(\\sigma_x, \\sigma_z\\) are (known) activation functions\n\\(\\color{#8b3e2f}{\\beta}, \\color{#8b3e2f}{\\alpha}\\) are weights (model parameters)\n\n\\(p(M + 1)\\) of them as written"
  },
  {
    "objectID": "pstat197a/slides/week6-nn.html#training-a-network",
    "href": "pstat197a/slides/week6-nn.html#training-a-network",
    "title": "Artificial Neural Networks",
    "section": "Training a network",
    "text": "Training a network\nNotice that the output is simply a long composition:\n\\[\nY = f(X) \\quad\\text{where}\\quad f \\equiv \\sigma_z \\circ h_\\beta\\circ \\sigma_x \\circ h_\\alpha\n\\]\n\neach function is either known or linear\ncompute parameters by minimizing a loss function\nminimization by gradient descent"
  },
  {
    "objectID": "pstat197a/slides/week6-nn.html#gradient-descent",
    "href": "pstat197a/slides/week6-nn.html#gradient-descent",
    "title": "Artificial Neural Networks",
    "section": "Gradient descent",
    "text": "Gradient descent\nDenoting the parameter vector by \\(\\theta = \\left(\\alpha^T \\; \\beta^T\\right)\\), initialize \\(\\theta^{(0)}\\) and repeat:\n\\[\n\\theta^{(r + 1)} \\longleftarrow \\theta^{(r)} + c_r \\nabla L^{(r)}\n\\]\n\n\\(L^{(r)}\\) is a loss function evaluated at the \\(r\\)th iteration\n\nof the form \\(L^{(r)} = \\frac{1}{n}\\sum_i L_i (\\theta^{(r)}, Y)\\)\n\n\\(c_r\\) is the ‘learning rate’; can be fixed or chosen adaptively\neach cycle through all the parameters is one ‘epoch’"
  },
  {
    "objectID": "pstat197a/slides/week6-nn.html#updates-for-the-vnn",
    "href": "pstat197a/slides/week6-nn.html#updates-for-the-vnn",
    "title": "Artificial Neural Networks",
    "section": "Updates for the VNN",
    "text": "Updates for the VNN\nIndividual parameter updates at the \\(r\\)th iteration are given by:\n\\[\n\\beta_{m}^{(r + 1)} \\longleftarrow\n    \\beta_{m}^{(r)} + c_r \\underbrace{\\frac{1}{n}\\sum_{i = 1}^n \\frac{\\partial L_i}{\\partial \\beta_{m}}\\Big\\rvert_{\\beta_{m} = \\beta_{m}^{(r)}}}_{\\text{gradient at current iteration}} \\\\\n    \\alpha_{mp}^{(r + 1)} \\longleftarrow\n    \\alpha_{mp}^{(r)} + c_r \\underbrace{\\frac{1}{n}\\sum_{i = 1}^n \\frac{\\partial L_i}{\\partial \\alpha_{mp}}\\Big\\rvert_{\\alpha_{mp} = \\alpha_{mp}^{(r)}}}_{\\text{gradient at current iteration}}\n\\]"
  },
  {
    "objectID": "pstat197a/slides/week6-nn.html#chain-rule",
    "href": "pstat197a/slides/week6-nn.html#chain-rule",
    "title": "Artificial Neural Networks",
    "section": "Chain rule",
    "text": "Chain rule\nThe gradient is easy to compute. Denoting \\(t_{i} = z_{i}^T\\beta\\):\n\\[\n\\begin{aligned}\n\\frac{\\partial L_i}{\\partial \\alpha_{mp}}\n    &= \\underbrace{\\frac{\\partial L_i}{\\partial f}\n        \\frac{\\partial f}{\\partial t_i}}_{\\delta_i}\n        \\underbrace{\\frac{\\partial t_i}{\\partial z_{im}}\n        \\frac{\\partial z_{im}}{\\partial \\alpha_{mp}}}_{s_{im}x_{ip}} \\\\\n\\frac{\\partial L_i}{\\partial \\beta_{m}}\n    &= \\underbrace{\\frac{\\partial L_i}{\\partial f}\n        \\frac{\\partial f}{\\partial t_i}}_{\\delta_i}\n        \\underbrace{\\frac{\\partial t_i}{\\partial \\beta_{m}}}_{z_{im}}\n\\end{aligned}\n\\]\n\nExplicitly computing gradients for each update gives the backpropagation algorithm of Rumelhart, Hinton, and Williams (1986) ."
  },
  {
    "objectID": "pstat197a/slides/week6-nn.html#backpropagation",
    "href": "pstat197a/slides/week6-nn.html#backpropagation",
    "title": "Artificial Neural Networks",
    "section": "Backpropagation",
    "text": "Backpropagation\nInitialize parameters and repeat:\n\nForward pass: compute \\(f(X), Z\\)\nBackward pass: compute \\(\\delta_i, s_{mi}\\) by ‘back-propagating’ current estimates\nUpdate the weights\n\\[\n\\hat{\\beta}_{km} \\longleftarrow\n  \\hat{\\beta}_{km} + c_r \\frac{1}{n}\\sum_i \\delta_{ki}z_{mi} \\\\\n  \\hat{\\alpha}_{mp} \\longleftarrow\n  \\hat{\\alpha}_{mp} + c_r \\frac{1}{n}\\sum_i s_{mi}x_{ip}\n\\]"
  },
  {
    "objectID": "pstat197a/slides/week6-nn.html#gradient-estimation",
    "href": "pstat197a/slides/week6-nn.html#gradient-estimation",
    "title": "Artificial Neural Networks",
    "section": "Gradient estimation",
    "text": "Gradient estimation\nExplicitly computing the gradient sums over all observations \\(i = 1, \\dots, n\\):\n\\[\ng = \\nabla \\frac{1}{n} \\sum_i L_i\n\\]\n\nIt’s much faster to estimate the gradient based on a “batch” of \\(m\\) observations (subsample) \\(J \\subset \\{1, \\dots, n\\}\\):\n\\[\n\\hat{g} = \\nabla \\frac{1}{m}\\sum_{i \\in J} L_i\n\\]"
  },
  {
    "objectID": "pstat197a/slides/week6-nn.html#modern-optimization-methods",
    "href": "pstat197a/slides/week6-nn.html#modern-optimization-methods",
    "title": "Artificial Neural Networks",
    "section": "Modern optimization methods",
    "text": "Modern optimization methods\nModern methods for training neural networks update parameters using gradient estimates and adaptive learning rates.1\n\nstochastic gradient descent (SGD): Bottou et al. (1998) replace \\(g\\) by \\(\\hat{g}\\)\nAdaGrad: Duchi, Hazan, and Singer (2011) use SGD with adaptive learning rates\nAdam: Kingma and Ba (2014) apply bias corrections to \\(\\hat{g}\\) based on moment estimates\n\nA nice overview is given in Goodfellow, Bengio, and Courville (2016) ."
  },
  {
    "objectID": "pstat197a/slides/week6-nn.html#increasing-width",
    "href": "pstat197a/slides/week6-nn.html#increasing-width",
    "title": "Artificial Neural Networks",
    "section": "Increasing width",
    "text": "Increasing width\n\n\n\n\n\n\n\nG\n\n \n\ncluster_0\n\n    \n\ncluster_1\n\n    \n\ncluster_2\n\n     \n\nin1\n\n     \n\nh1\n\n     \n\nin1-&gt;h1\n\n    \n\nh2\n\n     \n\nin1-&gt;h2\n\n    \n\nh3\n\n     \n\nin1-&gt;h3\n\n    \n\nh4\n\n     \n\nin1-&gt;h4\n\n    \n\nh5\n\n     \n\nin1-&gt;h5\n\n    \n\nin2\n\n     \n\nin2-&gt;h1\n\n    \n\nin2-&gt;h2\n\n    \n\nin2-&gt;h3\n\n    \n\nin2-&gt;h4\n\n    \n\nin2-&gt;h5\n\n    \n\nin3\n\n     \n\nin3-&gt;h1\n\n    \n\nin3-&gt;h2\n\n    \n\nin3-&gt;h3\n\n    \n\nin3-&gt;h4\n\n    \n\nin3-&gt;h5\n\n    \n\no1\n\n     \n\nh1-&gt;o1\n\n    \n\nh2-&gt;o1\n\n    \n\nh3-&gt;o1\n\n    \n\nh4-&gt;o1\n\n    \n\nh5-&gt;o1\n\n    \n\na\n\n     \n\no1-&gt;a\n\n    \n\nb\n\n width   \n\no1-&gt;b\n\n    \n\nc\n\n     \n\no1-&gt;c\n\n    \n\na-&gt;b\n\n    \n\nb-&gt;c\n\n   \n\n\nOne more hidden unit."
  },
  {
    "objectID": "pstat197a/slides/week6-nn.html#increasing-depth",
    "href": "pstat197a/slides/week6-nn.html#increasing-depth",
    "title": "Artificial Neural Networks",
    "section": "Increasing depth",
    "text": "Increasing depth\n\n\n\n\n\n\n\nG\n\n \n\ncluster_3\n\n    \n\ncluster_1\n\n    \n\ncluster_2\n\n    \n\ncluster_0\n\n     \n\nin1\n\n     \n\nh1\n\n     \n\nin1-&gt;h1\n\n    \n\nh2\n\n     \n\nin1-&gt;h2\n\n    \n\nh3\n\n     \n\nin1-&gt;h3\n\n    \n\nh4\n\n     \n\nin1-&gt;h4\n\n    \n\nh5\n\n     \n\nin1-&gt;h5\n\n    \n\nin2\n\n     \n\nin2-&gt;h1\n\n    \n\nin2-&gt;h2\n\n    \n\nin2-&gt;h3\n\n    \n\nin2-&gt;h4\n\n    \n\nin2-&gt;h5\n\n    \n\nin3\n\n     \n\nin3-&gt;h1\n\n    \n\nin3-&gt;h2\n\n    \n\nin3-&gt;h3\n\n    \n\nin3-&gt;h4\n\n    \n\nin3-&gt;h5\n\n    \n\nz1\n\n     \n\nh1-&gt;z1\n\n    \n\nz2\n\n     \n\nh1-&gt;z2\n\n    \n\nz3\n\n     \n\nh1-&gt;z3\n\n    \n\nz4\n\n     \n\nh1-&gt;z4\n\n    \n\nh2-&gt;z1\n\n    \n\nh2-&gt;z2\n\n    \n\nh2-&gt;z3\n\n    \n\nh2-&gt;z4\n\n    \n\nh3-&gt;z1\n\n    \n\nh3-&gt;z2\n\n    \n\nh3-&gt;z3\n\n    \n\nh3-&gt;z4\n\n    \n\nh4-&gt;z1\n\n    \n\nh4-&gt;z2\n\n    \n\nh4-&gt;z3\n\n    \n\nh4-&gt;z4\n\n    \n\nh5-&gt;z1\n\n    \n\nh5-&gt;z2\n\n    \n\nh5-&gt;z3\n\n    \n\nh5-&gt;z4\n\n    \n\no1\n\n     \n\nz1-&gt;o1\n\n    \n\nz2-&gt;o1\n\n    \n\nz3-&gt;o1\n\n    \n\nz4-&gt;o1\n\n    \n\na\n\n     \n\nb\n\n depth   \n\na-&gt;b\n\n    \n\nc\n\n     \n\nb-&gt;c\n\n   \n\n\nOne more hidden layer."
  },
  {
    "objectID": "pstat197a/slides/week6-nn.html#sequential-networks",
    "href": "pstat197a/slides/week6-nn.html#sequential-networks",
    "title": "Artificial Neural Networks",
    "section": "Sequential networks",
    "text": "Sequential networks\nNetworks of arbitrary width and depth in which the connectivity is uni-directional are known as “sequential” or “feedforward” networks/models.\n\\[\n\\begin{aligned}\n\\mathbb{E}Y &= \\sigma_1(Z_1\\beta_1) &\\text{output layer}\\\\\nZ_k &= \\sigma_k(Z_{k - 1} \\beta_k) &\\text{hidden layers } k = 2, \\dots, D - 1\\\\\nZ_D &\\equiv X &\\text{input layer}\n\\end{aligned}\n\\]\n\nchain rule calculations get longer but are otherwise the same\n“universal approximation” properties"
  },
  {
    "objectID": "pstat197a/slides/week6-nn.html#approximation-properties",
    "href": "pstat197a/slides/week6-nn.html#approximation-properties",
    "title": "Artificial Neural Networks",
    "section": "Approximation properties",
    "text": "Approximation properties\nSuppose:\n\n\\(\\mathbb{E}Y = f(X)\\) gives the ‘true’ relationship\n\\(\\tilde{f}(X)\\) represents the output layer of a feedforward neural network with one hidden layer of width \\(w\\)\n\n\nHornik, Stinchcombe, and White (1989) showed that, under some regularity conditions, for any \\(\\epsilon &gt; 0\\) there exists a width \\(w\\) and parameters such that:\n\\[\n\\sup_x \\|f(x) - \\tilde{f}(x)\\| &lt; \\epsilon\n\\]"
  },
  {
    "objectID": "pstat197a/slides/week6-nn.html#approximation-properties-1",
    "href": "pstat197a/slides/week6-nn.html#approximation-properties-1",
    "title": "Artificial Neural Networks",
    "section": "Approximation properties",
    "text": "Approximation properties\nSimilar results exist for deep networks with bounded width1.\n\nThese results do tell us that in most problems there exist both deep and shallow networks that approximate the true input-output relationship arbitrarily well\nThey don’t tell us how to find them.\n\nLu et al. (2017)"
  },
  {
    "objectID": "pstat197a/slides/week6-nn.html#performance-considerations",
    "href": "pstat197a/slides/week6-nn.html#performance-considerations",
    "title": "Artificial Neural Networks",
    "section": "Performance considerations",
    "text": "Performance considerations\nSeveral factors can affect actual performance in practice:\n\narchitecture (network structure)\nactivation function(s)\nloss function\noptimization method\nparameter initialization and training epochs\ndata quality (don’t forget this one!)"
  },
  {
    "objectID": "pstat197a/slides/week6-nn.html#activations",
    "href": "pstat197a/slides/week6-nn.html#activations",
    "title": "Artificial Neural Networks",
    "section": "Activations",
    "text": "Activations\nActivation functions \\(\\sigma(\\cdot)\\) determine whether a given unit ‘fires’.\n\n\n\n\n\n\n\nG\n\n \n\ncluster_0\n\n   \n\nunit1\n\n     \n\nsum\n\n Σ   \n\nunit1-&gt;sum\n\n   \n\nunit2\n\n     \n\nunit2-&gt;sum\n\n   \n\nunit3\n\n     \n\nunit3-&gt;sum\n\n   \n\nactivation\n\n σ(⋅)   \n\nsum-&gt;activation\n\n   \n\nout\n\n     \n\nactivation-&gt;out\n\n   \n\n\n\n\n\n\nFor example:\n\nif \\(Z_{k - 1}\\beta_{kj} = -28.2\\) and \\(\\sigma_k(x) = \\frac{1}{1 + e^{-x}}\\),\nthen \\(z_{kj} = \\sigma_k(Z_{k - 1}\\beta_j) \\approx 0\\)."
  },
  {
    "objectID": "pstat197a/slides/week6-nn.html#common-activation-functions",
    "href": "pstat197a/slides/week6-nn.html#common-activation-functions",
    "title": "Artificial Neural Networks",
    "section": "Common activation functions",
    "text": "Common activation functions\nThe most common activation functions are:\n\n(identity) \\(\\sigma(x) = x\\)\n(sigmoid) \\(\\sigma(x) = \\frac{1}{1 + \\exp\\{-x\\}}\\)\n(hyperbolic tangent) \\(\\sigma(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\\)\n(rectified linear unit) \\(\\sigma(x) = \\max (0, x)\\)"
  },
  {
    "objectID": "pstat197a/slides/week6-nn.html#loss-functions",
    "href": "pstat197a/slides/week6-nn.html#loss-functions",
    "title": "Artificial Neural Networks",
    "section": "Loss functions",
    "text": "Loss functions\nThe most common loss function for classification is\n\\[\nL(Y, f(X)) = -\\frac{1}{n}\\sum_i \\left[y_i\\log p_i + (1 - y_i)\\log(1 - p_i)\\right]\n\\qquad\\text{(cross-entropy)}\n\\]\n\nThe most common loss function for regression is:\n\\[\nL(Y, f(X)) = \\frac{1}{n}\\sum_i (y_i - f(x_i))^2\n\\qquad\\text{(mean squared error)}\n\\]"
  },
  {
    "objectID": "pstat197a/slides/week6-nn.html#references",
    "href": "pstat197a/slides/week6-nn.html#references",
    "title": "Artificial Neural Networks",
    "section": "References",
    "text": "References\n\n\n\n\n\n\nBottou, Léon et al. 1998. “Online Learning and Stochastic Approximations.” On-Line Learning in Neural Networks 17 (9): 142.\n\n\nDuchi, John, Elad Hazan, and Yoram Singer. 2011. “Adaptive Subgradient Methods for Online Learning and Stochastic Optimization.” Journal of Machine Learning Research 12 (7).\n\n\nGoodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. Deep Learning. MIT Press.\n\n\nHornik, Kurt, Maxwell Stinchcombe, and Halbert White. 1989. “Multilayer Feedforward Networks Are Universal Approximators.” Neural Networks 2 (5): 359–66.\n\n\nKingma, Diederik P, and Jimmy Ba. 2014. “Adam: A Method for Stochastic Optimization.” arXiv Preprint arXiv:1412.6980.\n\n\nLu, Zhou, Hongming Pu, Feicheng Wang, Zhiqiang Hu, and Liwei Wang. 2017. “The Expressive Power of Neural Networks: A View from the Width.” Advances in Neural Information Processing Systems 30.\n\n\nRumelhart, David E, Geoffrey E Hinton, and Ronald J Williams. 1986. “Learning Representations by Back-Propagating Errors.” Nature 323 (6088): 533–36."
  },
  {
    "objectID": "pstat197a/slides/week8-spatial.html#announcementsreminders",
    "href": "pstat197a/slides/week8-spatial.html#announcementsreminders",
    "title": "Spatial prediction",
    "section": "Announcements/reminders",
    "text": "Announcements/reminders\n\nI am away Thursday; no class meeting due to strike\nStudents in the 4pm section should attend Josh or Erika’s section this week\nNext week (Thanksgiving):\n\nwe are meeting Tuesday\nbut there are no Wednesday section meetings on 11/23\n\nYou should start working on your last group assignment before Thanksgiving"
  },
  {
    "objectID": "pstat197a/slides/week8-spatial.html#final-group-assignment",
    "href": "pstat197a/slides/week8-spatial.html#final-group-assignment",
    "title": "Spatial prediction",
    "section": "Final group assignment",
    "text": "Final group assignment\n\ngroups posted [here]\ntask: create a method vignette on a data science topic or theme\n\ngoal: create a reference that you or someone else might use as a starting point next term\ndeliverable: public repository in the pstat197 workspace"
  },
  {
    "objectID": "pstat197a/slides/week8-spatial.html#possible-vignette-topics",
    "href": "pstat197a/slides/week8-spatial.html#possible-vignette-topics",
    "title": "Spatial prediction",
    "section": "Possible vignette topics",
    "text": "Possible vignette topics\n\nclustering methods\nneural net architecture(s) for … [images, text, time series, spatial data]\nconfiguring a database and writing queries in R\nanalysis of network data\nnumerical optimization\nbootstrapping\ngeospatial data structures\nanomaly detection\nfunctional regression"
  },
  {
    "objectID": "pstat197a/slides/week8-spatial.html#outputs",
    "href": "pstat197a/slides/week8-spatial.html#outputs",
    "title": "Spatial prediction",
    "section": "Outputs",
    "text": "Outputs\nYour repository should contain:\n\nA brief .README summarizing repo content and listing the best references on your topic for a user to consult after reviewing your vignette if they wish to learn more\nA primary vignette document that explains methods and walks through implementation line-by-line (similar to an in-class or lab activity)\nAt least one example dataset\nA script containing commented codes appearing in the vignette"
  },
  {
    "objectID": "pstat197a/slides/week8-spatial.html#timeline",
    "href": "pstat197a/slides/week8-spatial.html#timeline",
    "title": "Spatial prediction",
    "section": "Timeline",
    "text": "Timeline\n\nlet me know your topic by end of day Thursday 11/17\nI will confirm by end of day Friday 11/18\nmake a start before Thanksgiving\npresent a draft in class Thursday 12/1\nfinalize repository by Thursday 12/8"
  },
  {
    "objectID": "pstat197a/slides/week8-spatial.html#expectations",
    "href": "pstat197a/slides/week8-spatial.html#expectations",
    "title": "Spatial prediction",
    "section": "Expectations",
    "text": "Expectations\nYou’ll need to yourself learn about the topic and implementation by finding reference materials and code examples.\n\nIt is okay to borrow closely from other vignettes in creating your own, but you should:\n\ncite them\nuse different data\ndo something new\n\n\n\nIt is not okay to make a collage of reference materials by copying verbatim, or simply rewrite an existing vignette.\n\nthe best safeguard against this is to find your own data so you’re forced to translate codes/steps to apply in your particular case\nwe’ll do a brief search and skim your references to ensure sufficient originality"
  },
  {
    "objectID": "pstat197a/slides/week8-spatial.html#from-last-time",
    "href": "pstat197a/slides/week8-spatial.html#from-last-time",
    "title": "Spatial prediction",
    "section": "From last time",
    "text": "From last time\nWe had fit the site-specific model:\n\\[\n\\begin{aligned}\nY_{i, t} &= f_i (t) + \\epsilon_{i, t} \\quad\\text{(nonlinear regression)} \\\\\n\\epsilon_{i, t} &= \\sum_{d = 1}^D \\alpha_{i,d}\\epsilon_{i, t - d} + \\xi_{i, t} \\quad\\text{(AR(D) errors)}\n\\end{aligned}\n\\]\n\nAnd computed forecasts \\(\\hat{Y}_{i, t+ 1} = \\mathbb{E}(Y_{i, t + 1}|Y_{i, t})\\)"
  },
  {
    "objectID": "pstat197a/slides/week8-spatial.html#fitting-and-forecasts-for-one-site",
    "href": "pstat197a/slides/week8-spatial.html#fitting-and-forecasts-for-one-site",
    "title": "Spatial prediction",
    "section": "Fitting and forecasts for one site",
    "text": "Fitting and forecasts for one site\n\nPartitionsFittingForecastingVisualization\n\n\n\n# data partitioning\nsite15 &lt;- soil %&gt;% \n  dplyr::select(-year, -elev) %&gt;%\n  filter(site == soil$site[15]) %&gt;%\n  arrange(date)\n\ntrain &lt;- site15 %&gt;%\n  filter(date &lt; ymd('2018-06-01'))\n\ntest &lt;- site15 %&gt;%\n  filter(date &gt;= ymd('2018-06-01'))\n\ntrain %&gt;% head()\n\n# A tibble: 6 × 7\n  site     day date        temp longitude latitude elevation\n  &lt;chr&gt;  &lt;dbl&gt; &lt;date&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 B21K-1   226 2017-08-14  3.82     -155.     69.6        96\n2 B21K-1   227 2017-08-15  3.72     -155.     69.6        96\n3 B21K-1   228 2017-08-16  3.31     -155.     69.6        96\n4 B21K-1   229 2017-08-17  4.95     -155.     69.6        96\n5 B21K-1   230 2017-08-18  5.46     -155.     69.6        96\n6 B21K-1   231 2017-08-19  5.80     -155.     69.6        96\n\n\n\n\n\nx_train &lt;- pull(train, day) %&gt;% \n  fourier(nbasis = 4, period = 365)\ny_train &lt;- pull(train, temp)\n\nfit &lt;- Arima(y_train, \n      order = c(2, 0, 0), \n      xreg = x_train, \n      include.mean = F,\n      method = 'ML')\n\nfit\n\nSeries: y_train \nRegression with ARIMA(2,0,0) errors \n\nCoefficients:\n         ar1      ar2     const      sin1       cos1     sin2     cos2\n      1.4200  -0.5192  -97.0613  -88.1952  -113.2035  -8.7344   5.5794\ns.e.  0.0497   0.0497   11.5069    9.0469    13.2328   9.5149  12.0896\n\nsigma^2 = 0.7837:  log likelihood = -375.26\nAIC=766.52   AICc=767.03   BIC=795.9\n\n\n\n\n\nx_test &lt;- pull(test, day) %&gt;% \n  fourier(nbasis = 4, period = 365)\n\npreds &lt;- forecast(fit, h = nrow(x_test), xreg = x_test)\n\nhead(preds$mean)\n\nTime Series:\nStart = 292 \nEnd = 297 \nFrequency = 1 \n[1] -0.2837326 -0.1614655 -0.0189233  0.1355583  0.2962263  0.4592358"
  },
  {
    "objectID": "pstat197a/slides/week8-spatial.html#now-for-many-sites",
    "href": "pstat197a/slides/week8-spatial.html#now-for-many-sites",
    "title": "Spatial prediction",
    "section": "Now for many sites",
    "text": "Now for many sites\nRemember the functional programming iteration strategy?\n\nFittingFitPredictions\n\n\n\n\n# A tibble: 26 × 5\n   site   train              test               fit        pred      \n   &lt;chr&gt;  &lt;list&gt;             &lt;list&gt;             &lt;list&gt;     &lt;list&gt;    \n 1 C27K-1 &lt;tibble [485 × 3]&gt; &lt;tibble [78 × 3]&gt;  &lt;fr_ARIMA&gt; &lt;forecast&gt;\n 2 F24K-1 &lt;tibble [485 × 3]&gt; &lt;tibble [54 × 3]&gt;  &lt;fr_ARIMA&gt; &lt;forecast&gt;\n 3 G25K-2 &lt;tibble [485 × 3]&gt; &lt;tibble [24 × 3]&gt;  &lt;fr_ARIMA&gt; &lt;forecast&gt;\n 4 G26K-2 &lt;tibble [485 × 3]&gt; &lt;tibble [69 × 3]&gt;  &lt;fr_ARIMA&gt; &lt;forecast&gt;\n 5 G27K-3 &lt;tibble [485 × 3]&gt; &lt;tibble [76 × 3]&gt;  &lt;fr_ARIMA&gt; &lt;forecast&gt;\n 6 M17K-2 &lt;tibble [358 × 3]&gt; &lt;tibble [338 × 3]&gt; &lt;fr_ARIMA&gt; &lt;forecast&gt;\n 7 M18K-5 &lt;tibble [357 × 3]&gt; &lt;tibble [382 × 3]&gt; &lt;fr_ARIMA&gt; &lt;forecast&gt;\n 8 M16K-2 &lt;tibble [354 × 3]&gt; &lt;tibble [323 × 3]&gt; &lt;fr_ARIMA&gt; &lt;forecast&gt;\n 9 N17K-3 &lt;tibble [353 × 3]&gt; &lt;tibble [357 × 3]&gt; &lt;fr_ARIMA&gt; &lt;forecast&gt;\n10 L16K-1 &lt;tibble [352 × 3]&gt; &lt;tibble [386 × 3]&gt; &lt;fr_ARIMA&gt; &lt;forecast&gt;\n# … with 16 more rows"
  },
  {
    "objectID": "pstat197a/slides/week8-spatial.html#spatial-prediction",
    "href": "pstat197a/slides/week8-spatial.html#spatial-prediction",
    "title": "Spatial prediction",
    "section": "Spatial prediction",
    "text": "Spatial prediction\nWe could consider our data to be more explicitly spatial:\n\\[\nY_{i, t} = Y_t(s_i)\n\\qquad\\text{where}\\qquad\ns_i = \\text{location of site }i\n\\]\n\nIn other words, our data at a given time are a realization of a spatial process \\(Y(s)\\) observed at locations \\(s_1, \\dots, s_n\\).\n\n\nCan we predict \\(Y(s_{n + 1})\\) based on \\(Y(s_1), \\dots, Y(s_n)\\)?"
  },
  {
    "objectID": "pstat197a/slides/week8-spatial.html#intuition",
    "href": "pstat197a/slides/week8-spatial.html#intuition",
    "title": "Spatial prediction",
    "section": "Intuition",
    "text": "Intuition\nTobler’s first law of geography:\n\n“everything is related to everything else, but near things are more related than distant things”\n\n\nSo a weighted average of some kind makes sense for spatial prediction\n\\[\n\\hat{Y}(s) = \\sum_i w_i Y(s_i)\n\\]\nwhere the weights \\(w_i\\) are larger for \\(s_i\\) closer to \\(s\\)."
  },
  {
    "objectID": "pstat197a/slides/week8-spatial.html#inverse-distance-weighting",
    "href": "pstat197a/slides/week8-spatial.html#inverse-distance-weighting",
    "title": "Spatial prediction",
    "section": "Inverse distance weighting",
    "text": "Inverse distance weighting\nA simple and fully nonparametric method of spatial prediction is to set \\(w_i \\propto 1/d(s, s_i)\\) where \\(d\\) is a distance measure.\n\nInverse distance weighting does just that, for powers of distance:\n\\[\n\\hat{Y}(s) = \\sum_i c \\times d(s, s_i)^{-p} \\times Y(s_i)\n\\]\nWhere \\(c\\) is the normalizing constant \\(1/\\sum_i d(s, s_i)^{-p}\\)."
  },
  {
    "objectID": "pstat197a/slides/week8-spatial.html#power-parameter",
    "href": "pstat197a/slides/week8-spatial.html#power-parameter",
    "title": "Spatial prediction",
    "section": "Power parameter",
    "text": "Power parameter\n\n\nThe power parameter \\(p\\) controls the rate of weight decay with distance:\n\\[\nw_i \\propto \\frac{1}{d(s, s_i)^p}\n\\]"
  },
  {
    "objectID": "pstat197a/slides/week8-spatial.html#interpolation",
    "href": "pstat197a/slides/week8-spatial.html#interpolation",
    "title": "Spatial prediction",
    "section": "Interpolation",
    "text": "Interpolation\nSpatial interpolation refers to ‘filling in’ values between observed locations.\n\nGenerate a spatial mesh of with centers \\(g_1, g_2, \\dots, g_m\\)\nPredict \\(\\hat{Y}(g_j)\\) for every center \\(g_j\\)\nMake a raster plot\n\n\n\n\n\n\n\n\nMesh\n\n\nFor spatial problems, a mesh is a mutually exclusive partitioning of an area into subregions. Subregions could be regular (e.g., squares, polygons) or irregular (try googling ‘Voronoi tesselation’)."
  },
  {
    "objectID": "pstat197a/slides/week8-spatial.html#map-of-locations",
    "href": "pstat197a/slides/week8-spatial.html#map-of-locations",
    "title": "Spatial prediction",
    "section": "Map of locations",
    "text": "Map of locations\nEarlier, I fit models and generated forecasts for 26 sites chosen largely based on having overlapping observation windows."
  },
  {
    "objectID": "pstat197a/slides/week8-spatial.html#forecasts",
    "href": "pstat197a/slides/week8-spatial.html#forecasts",
    "title": "Spatial prediction",
    "section": "Forecasts",
    "text": "Forecasts\nI also truncated the training data to stop on the same date (April 30, 2018). So we can plot point forecasts for May 1."
  },
  {
    "objectID": "pstat197a/slides/week8-spatial.html#interpolations-using-idw",
    "href": "pstat197a/slides/week8-spatial.html#interpolations-using-idw",
    "title": "Spatial prediction",
    "section": "Interpolations using IDW",
    "text": "Interpolations using IDW\nSo interpolating between forecasts yields spatial forecasts."
  },
  {
    "objectID": "pstat197a/slides/week8-spatial.html#effect-of-idw-parameter-p",
    "href": "pstat197a/slides/week8-spatial.html#effect-of-idw-parameter-p",
    "title": "Spatial prediction",
    "section": "Effect of IDW parameter \\(p\\)",
    "text": "Effect of IDW parameter \\(p\\)\nThe power parameter \\(p\\) controls the rate of decay of interpolation weight \\(w_i\\) with distance."
  },
  {
    "objectID": "pstat197a/slides/week8-spatial.html#considerations",
    "href": "pstat197a/slides/week8-spatial.html#considerations",
    "title": "Spatial prediction",
    "section": "Considerations",
    "text": "Considerations\n\nChoosing \\(p\\) can be done based on optimizing predictions or by hand.\nUncertainty quantification?\n\nusually, could use variance of weighted average\nbut also tricky in this case because we are interpolating forecasts, which themselves have some associated uncertainty"
  },
  {
    "objectID": "pstat197a/activities/making-trees.html",
    "href": "pstat197a/activities/making-trees.html",
    "title": "Activity: making trees",
    "section": "",
    "text": "For the purposes of this activity we’ll use data from the 1994 census (there are fewer variables than the proteomics data and sometimes a little variety is nice).\nlibrary(tidyverse)\n\n# data location\nurl &lt;- \"http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n\n# import\ncensus &lt;- read_csv(url,\n                   col_names = c(\"age\", \n                                 \"workclass\", \n                                 \"fnlwgt\", \n                                 \"education\",\n                                 \"education_1\",\n                                 \"marital_status\",\n                                 \"occupation\",\n                                 \"relationship\",\n                                 \"race\",\n                                 \"sex\",\n                                 \"capital_gain\",\n                                 \"capital_loss\",\n                                 \"hours_per_week\",\n                                 \"native_country\",\n                                 \"income\")) %&gt;%\n  mutate(income = factor(income)) %&gt;%\n  select(-fnlwgt, -education_1)\n\ncensus %&gt;% head(4)\n\n# A tibble: 4 × 13\n    age workclass   education marital_status occupation relationship race  sex  \n  &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;     &lt;chr&gt;          &lt;chr&gt;      &lt;chr&gt;        &lt;chr&gt; &lt;chr&gt;\n1    39 State-gov   Bachelors Never-married  Adm-cleri… Not-in-fami… White Male \n2    50 Self-emp-n… Bachelors Married-civ-s… Exec-mana… Husband      White Male \n3    38 Private     HS-grad   Divorced       Handlers-… Not-in-fami… White Male \n4    53 Private     11th      Married-civ-s… Handlers-… Husband      Black Male \n# … with 5 more variables: capital_gain &lt;dbl&gt;, capital_loss &lt;dbl&gt;,\n#   hours_per_week &lt;dbl&gt;, native_country &lt;chr&gt;, income &lt;fct&gt;\nYour group’s job is to build a tree to classify high earners and low earners based on a bootstrap sample and a random subset of predictors.\nThe income variable is the response or variable of interest; the rest are potential predictors.\n# inspect repsonse\ncensus %&gt;% pull(income) %&gt;% str()\n\n Factor w/ 2 levels \"&lt;=50K\",\"&gt;50K\": 1 1 1 1 1 1 1 2 2 2 ..."
  },
  {
    "objectID": "pstat197a/activities/making-trees.html#building-a-tree",
    "href": "pstat197a/activities/making-trees.html#building-a-tree",
    "title": "Activity: making trees",
    "section": "Building a tree",
    "text": "Building a tree\n\nStep 1: resample the data\nFirst, draw a sample with replacement from the observations. This is known as a bootstrap sample. We’ll keep this relatively small to simplify matters.\n\n\n\n\n\n\nAction\n\n\n\nDraw a bootstrap sample\nCopy the code chunk below and execute once.\n\n# resample data\ncensus_boot &lt;- census %&gt;%\n  sample_n(size = 200, replace = T)\n\n\n\nYou will build your tree using this data.\n\n\nStep 2: select predictors at random\nNext draw a set of predictors at random. We’ll also keep this set small so that you can develop the tree ‘by hand’.\n\n\n\n\n\n\nAction\n\n\n\nSelect two random predictors\nCopy and paste the code chunk below into your script and execute once without modification.\n\n# retrieve column names\npossible_predictors &lt;- census %&gt;% \n  select(-income) %&gt;%\n  colnames()\n\n# grab 2 columns at random\npredictors &lt;- sample(possible_predictors,\n                     size = 2, \n                     replace = F)\n\n# select these columns from the bootstrap sample\ntrain &lt;- census_boot %&gt;% \n  select(c(income, any_of(predictors)))\n\n\n\nYou will build your tree using only these predictors from the bootstrap sample as training data.\n\n\nStep 3a: find your first split\nNow your job is to choose exactly one of the predictors to make a binary split of the data.\n\nfor categorical variables, determine which categories you will classify as high-income vs. low-income\nfor continuous variables, choose a cutoff value so that any observation greater/less than the cutoff is classified as a high/low (or vice-versa) earner\n\nYou do not need to make quantitatively rigorous choices. Try to make a choice that you think is reasonable, but don’t agonize over it. The code below might help you decide which variable to use and how to make the split: use it to inspect each of the two variables and decide which one better distinguishes the income groups.\n\n# comment out -- don't overwrite your bootstrap sample!\ncensus_boot &lt;- census %&gt;% \n  sample_n(size = 200, replace = T)\n\n# for continuous variables\ncensus_boot %&gt;%\n  ggplot(aes(x = age, # replace with predictor name \n             y = income)) +\n  geom_jitter(height = 0.1) +\n  geom_vline(xintercept = 35) # adjust cutoff\n\n\n\ncensus_boot %&gt;%\n  ggplot(aes(x = age, # replace with predictor name\n           y = ..density..)) +\n  geom_density(aes(color = income, fill = income),\n               alpha = 0.5) +\n  geom_vline(xintercept = 35) # adjust cutoff\n\n\n\n# for categorical variables\ncensus_boot %&gt;%\n  group_by(workclass, income) %&gt;%\n  count() %&gt;%\n  spread(income, n) %&gt;%\n  mutate_all(~ replace_na(.x, 0)) %&gt;%\n  mutate(high.inc = `&lt;=50K` &gt; `&gt;50K`)\n\n# A tibble: 7 × 4\n# Groups:   workclass [7]\n  workclass        `&lt;=50K` `&gt;50K` high.inc\n  &lt;chr&gt;              &lt;int&gt;  &lt;int&gt; &lt;lgl&gt;   \n1 ?                     11      3 TRUE    \n2 Federal-gov            3      1 TRUE    \n3 Local-gov              9      5 TRUE    \n4 Private              108     29 TRUE    \n5 Self-emp-inc           0      4 FALSE   \n6 Self-emp-not-inc      14      5 TRUE    \n7 State-gov              6      2 TRUE    \n\n# pick out categories that are majority high income\nhighinc_categories &lt;- census_boot %&gt;%\n  group_by(workclass, # replace with predictor name\n           income) %&gt;%\n  count() %&gt;%\n  spread(income, n) %&gt;%\n  mutate_all(~ replace_na(.x, 0)) %&gt;%\n  mutate(high.inc = `&lt;=50K` &lt; `&gt;50K`) %&gt;%\n  filter(high.inc == T) %&gt;%\n  pull(workclass) # replace with predictor name\n\n\n\n\n\n\n\nAction\n\n\n\nMake your first split\n\nChoose whichever of the two predictors you think best distinguishes the high income and low income groups.\nFind a cutoff value if the variable is continuous, or the categories that you will classify as high income if the variable is categorical.\nIf categorical, store a vector of the category names that are classified as high income (see code above). If continuous, store the cutoff value.\nWrite down the rule.\n\n\n\n\n\nStep 3b: find your second split\nNow filter the data to just those rows classified as (but not necessarily actually) high income based on your first split.\n\n# continuous case -- example\ncutoff &lt;- 35\n\ncensus_boot_sub &lt;- census_boot %&gt;%\n  filter(age &gt; cutoff)\n\n# categorical case -- example\ncensus_boot_sub &lt;- census_boot %&gt;%\n  filter(workclass %in% highinc_categories)\n\n\n\n\n\n\n\nAction\n\n\n\nFind a second split\n\nRepeat step 3a but with the filtered data census_boot_sub instead of the full bootstrap sample.\nWrite down the rule.\n\n\n\n\n\nStep 4: draw the tree\nWe could in theory keep creating binary splits until all observations are correctly classified. However, since we’re doing this by hand and just for illustration purposes, we’ll stop after two splits – it’ll be more of a sapling than a fully grown tree.\n\n\n\n\n\n\nAction\n\n\n\nMake a diagram\nDraw your tree on your group’s whiteboard. It should have just one ‘root’ node and just three ‘leaf’ nodes."
  },
  {
    "objectID": "pstat197a/activities/making-trees.html#classifying-a-new-observation",
    "href": "pstat197a/activities/making-trees.html#classifying-a-new-observation",
    "title": "Activity: making trees",
    "section": "Classifying a new observation",
    "text": "Classifying a new observation\nUse your tree to determine how to classify the following observation:\n\ncensus %&gt;%\n  sample_n(size = 1) %&gt;%\n  t() %&gt;%\n  knitr::kable()\n\n\n\n\nage\n56\n\n\nworkclass\nPrivate\n\n\neducation\nHS-grad\n\n\nmarital_status\nDivorced\n\n\noccupation\nAdm-clerical\n\n\nrelationship\nOther-relative\n\n\nrace\nAmer-Indian-Eskimo\n\n\nsex\nFemale\n\n\ncapital_gain\n0\n\n\ncapital_loss\n0\n\n\nhours_per_week\n35\n\n\nnative_country\nUnited-States\n\n\nincome\n&lt;=50K"
  },
  {
    "objectID": "pstat197a/activities/making-trees.html#algorithmic-considerations",
    "href": "pstat197a/activities/making-trees.html#algorithmic-considerations",
    "title": "Activity: making trees",
    "section": "Algorithmic considerations",
    "text": "Algorithmic considerations\nYou just did a loose version of what’s known as recursive partitioning – repeatedly splitting the data. That’s a specific method of constructing a tree.\nHow did you decide which of your two variables to use? Could you write code to make the same choice automatically? This, it turns out, is the main challenge in fully automating the process. The recursive partitioning algorithm requires two things:\n\na criterion by which one split is considered ‘better’ than another\na stopping rule\n\nIt is fairly simple to compute the best cutoff (or categorical mapping) for a given predictor – one can do a brute-force search for the split that minimizes misclassifications. However, when there are many possible variables to split on, a criterion is needed to determine the best choice. You can read about how this is done in MDSR 11.1.1."
  },
  {
    "objectID": "pstat197a/vignette-guidelines.html",
    "href": "pstat197a/vignette-guidelines.html",
    "title": "Vignette Guidelines",
    "section": "",
    "text": "A vignette is a simple example intended to help learn a method or tool.\nThe overarching goal of creating vignettes is to provide starting points for learning about specialized topics in data science that students in the class can later consult to familiarize themselves with an unfamiliar topic during their project work.\nThis document sets expectations for the organization and content of vignette repositories."
  },
  {
    "objectID": "pstat197a/vignette-guidelines.html#configuring-your-repository",
    "href": "pstat197a/vignette-guidelines.html#configuring-your-repository",
    "title": "Vignette Guidelines",
    "section": "Configuring your repository",
    "text": "Configuring your repository\nCreate a public repository in the PSTAT197 workspace and add your teammates as collaborators.\nSelect options at the creation step to initialize the repository with:\n\na .README file\na .gitignore file\na license\n\nGive the repository a descriptive name. Use the naming convention\n\nvignette-[keyword]\n\nfor example, “vignette-lstm”, “vignette-kriging”, “vignette-cnn”, and the like. A single keyword is best if possible, but consider using two or three if needed to make your repo name sufficiently specific, e.g., “vignette-database-configuration” or “vignette-distribution-based-clustering”. Do not use more than three keywords in your repository name.\nSupply an optional description that contains a long title for your vignette topic, for instance:\n\nDistribution-based clustering in R and application to unsupervised cell type classification\n\nLastly, once the repository is created, add a few topics to the “About” section on the far right of your repository homepage."
  },
  {
    "objectID": "pstat197a/vignette-guidelines.html#directory-organization",
    "href": "pstat197a/vignette-guidelines.html#directory-organization",
    "title": "Vignette Guidelines",
    "section": "Directory organization",
    "text": "Directory organization\nAs a general guideline, all files except the README should be placed in appropriately-named subdirectories so that your repository homepage is free from file clutter.\nIf your project has a single main file – in this case the vignette document – it is reasonable to place that in the root directory with the README. Everything else should go in a subfolder.\nThe high-level directories should clearly differentiate the main project contents, and overall there should not be too many levels of subdirectory, especially for a simple project like a code vignette. Your directory structure might look something like this in the end:\nroot directory\n|-- data\n    |-- raw\n    |-- processed\n|-- scripts\n    |-- drafts\n    |-- vignette-script.R\n|-- img\n    |-- fig1.png\n    |-- fig2.png\n|-- vignette.qmd\n|-- vignette.html\n|-- README.md\nAs a guiding principle, each subdirectory should contain either\n\n(a)a few primary files and one or more subdirectories\nscripts\n|-- functions\n|-- drafts\n|-- exploratory-analysis.R\n|-- model-fitting.R\n|-- visualizations.R\n(b) a single file type with an obvious naming convention\nimg\n|-- fig-autocorrelation.png\n|-- fig-forecasts.png\n|-- fig-rawseries.png\n|-- logo-ucsb.png\n\nTry to organize your repository so that it is easy to navigate for the general coding public (and for your future self)."
  },
  {
    "objectID": "pstat197a/vignette-guidelines.html#readme-contents",
    "href": "pstat197a/vignette-guidelines.html#readme-contents",
    "title": "Vignette Guidelines",
    "section": "README contents",
    "text": "README contents\nYour README file should contain five main pieces of information in the following order:\n\nA one-sentence description at the very top before any (sub)headers:\n\nVignette on implementing distribution-based clustering using cell type data; created as a class project for PSTAT197A in Fall 2022.\n\nContributors\nVignette abstract: a brief description in a few sentences of your vignette topic, example data, and outcomes.\nRepository contents: an explanation of the directory structure of the repository\nReference list: 2 or more references to learn more about your topic.\n\nA typical README file would also contain instructions on use and instructions on contributing to the repository."
  },
  {
    "objectID": "pstat197a/vignette-guidelines.html#repository-contents",
    "href": "pstat197a/vignette-guidelines.html#repository-contents",
    "title": "Vignette Guidelines",
    "section": "Repository contents",
    "text": "Repository contents\nYour repository should contain at minimum the following:\n\nan example dataset with which you illustrate the use of the method(s) or tool(s) of your topic\na primary vignette document – either a notebook or rendered markdown file – that teaches your method(s) and/or tool(s). this document should integrate codes with step-by-step explanation and read much like a lab activity\na script with line annoations that replicates all results shown in the primary vignette document end-to-end"
  },
  {
    "objectID": "pstat197a/vignette-guidelines.html#evaluation",
    "href": "pstat197a/vignette-guidelines.html#evaluation",
    "title": "Vignette Guidelines",
    "section": "Evaluation",
    "text": "Evaluation\nYour work will be evaluated on:\n\nhow well the repository and contents conform to the expectations outlined above\nthe clarity of the vignette, from the perspective of another student in the class\nthe correctness of the data analysis and any other technical aspects of the vignette"
  },
  {
    "objectID": "pstat197a/vignette-guidelines.html#deadlines",
    "href": "pstat197a/vignette-guidelines.html#deadlines",
    "title": "Vignette Guidelines",
    "section": "Deadlines",
    "text": "Deadlines\nThere are two deadlines associated with this project, a draft deadline and a final deadline.\n\ndraft deadline Thursday, December 1, 2pm PST (in class) – be prepared to share a draft of your primary vignette document and explain it to a small group of other students\nfinal deadline Thursday, December 8, 11:59pm PST – the repository and all contained files should be in final form ready for evaluation by course staff"
  },
  {
    "objectID": "projects/projects-20-21.html",
    "href": "projects/projects-20-21.html",
    "title": "2020-2021 Projects",
    "section": "",
    "text": "Listed alphabetically by project sponsor."
  },
  {
    "objectID": "projects/projects-20-21.html#appfolio",
    "href": "projects/projects-20-21.html#appfolio",
    "title": "2020-2021 Projects",
    "section": "Appfolio",
    "text": "Appfolio\nAI: teaching a machine to learn math\nStudent team: Priyanka Banerjee, Shaka Kanenobu, Vikram Pasupathy, and Ari Polakof\nAdvisors: Soeren Thust, Peng-Yuan Lai, Sikun Lin, and Ambuj Singh\nThis project aims to teach a machine to learn math. Students will attempt to train a robot to recognize important data contained within invoices and to perform the necessary calculations on those invoices. This task is important. Figuring out credits and debits is the bottleneck of the payment process. Learning the spatial relationship of data with an arithmetic relationship is a crucial piece of intelligence. With an all-in-one system, AppFolio would like to recover any financial snapshot of any customer in their history."
  },
  {
    "objectID": "projects/projects-20-21.html#california-cooperative-oceanic-fisheries-investigations-calcofi",
    "href": "projects/projects-20-21.html#california-cooperative-oceanic-fisheries-investigations-calcofi",
    "title": "2020-2021 Projects",
    "section": "California Cooperative Oceanic Fisheries Investigations (CalCOFI)",
    "text": "California Cooperative Oceanic Fisheries Investigations (CalCOFI)\nLarval fish assemblage as an indicator to predict the fisheries catch\nStudent team: Leron Reznikov, Andie Rupprecht, Gavin Tieng, and Meng Vong\nAdvisors: Erin Satterthwaite, Julia Coates, Chau Tran, and Sang-Yun Oh\nCALCOFI studies the marine environment of coastal California and investigates the relationships between climate, fish population, and other human activities. This project seeks to establish whether any correlation can be drawn from two separate datasets, as well as possibly investigate any environmental factors that add variance to the relationship. The two data sets under investigation are:\n\nFish larvae catch data from CALCOFI cruises intended to measure populations of various species\nCommercial fish data from fisheries in California\n\nHow does fish catch change in relation to changing environmental conditions & larval community composition? Do we find that El Nino year conditions have lower recreational and/or commercial catch (lbs) or catch value ($)?\n\n\n\nFigure: California Yellowtail 2016-2019\n\n\nConsider this initial project visualization of Yellowtail larvae identified in 2016 (purple) compared to all locations Yellowtail were caught in the years 2017-2019 (yellow). The time lag was picked to give enough time for recruitment to happen, so the larvae identified in 2016 could possibly be the same fish caught in the later years. All the larvae and almost all of the catches occurred within the Southern California bight which might suggest some spatial relations.\nThe team will explore the data in more depth, also looking at other species and timeframes.\nClimate change and young fish: the relationship between pH and aspects of larval fish assemblage in the California current\nStudent team: Cristian Razo, Robin Hollingsworth, and Jay Patwardhan\nAdvisors: Erin Satterthwaite, Chau Tran, and Sang-Yun Oh\nThis project looks for correlations between the water characteristics of multiple coastal regions and the makeup of the marine life there, with hopes of providing insight to future observations on what the makeup of marine life should be. This can be vital to preserving the environment, helping endangered marine species, and ensuring the well health of our oceans in general as well.\n\n\n\n\n\n\nYoung fish\n\n\n\n\n\n\n\nFish larvae\n\n\n\n\n\nThe team has access to two large time-series datasets. The first is a large collection of water characteristics across many regions and depths of California’s coast from 2008-1015. The second is an even larger data set comprising the topology and makeup of many species of California marine life from 1904 to the present. Given the disparity in time-steps, the water characteristic data set requires backward forecasts that accurately predict water temperature and salinity.\nIn addition to correlations between the makeup of water and how these characteristics affect the current and future states of how marine life, the team is also exploring the various causal factors for water characteristics that vary from region to region, which in turn affects the makeup of marine life."
  },
  {
    "objectID": "projects/projects-20-21.html#carpe-data",
    "href": "projects/projects-20-21.html#carpe-data",
    "title": "2020-2021 Projects",
    "section": "Carpe Data",
    "text": "Carpe Data\nMining criminal records based on HTML data\nStudent team: Andrew Nguyen, Samual Sheridan, Matthew Xu, and Jason Yu\nAdvisors: Crystal Zhang, Joshua Bang, and Mike Ludkovski\nThe goal of this project is to build an automated tool to predict whether a given personal record includes prior arrest information. There are two objectives. The first is to build a binary classifier to predict whether a webpage contains a criminal record or not. The second objective is to provide information about the arrest (e.g arrest date). The team aims to extract the date of the arrest and the arrest code.\nSome similar projects have already been put into production, with text mining algorithms generally supplemented with manual validation and checking to ensure data mining quality. Fairness and ethics become a point of concern. The team will need to discern whether any bias (e.g. inappropriate use of demographic information to predict criminal record) occurs."
  },
  {
    "objectID": "projects/projects-20-21.html#center-for-science-and-engineering-partnerships-csep",
    "href": "projects/projects-20-21.html#center-for-science-and-engineering-partnerships-csep",
    "title": "2020-2021 Projects",
    "section": "Center for Science and Engineering Partnerships (CSEP)",
    "text": "Center for Science and Engineering Partnerships (CSEP)\nUCSB undergraduate alumni tracking\nStudent team: Andrea Anez, Karanveer Benipal, Manny Medrano, and Romtin Toranji\nAdvisors: Lubi Lenaburg, Joshua Bang, and Alex Franks\nThe CSEP Alumni Tracking Project tracks students graduating from 2000 - 2018 from the University of California, Santa Barbara. The overall goal of tracking students is to monitor opportunities UCSB provides and the impact on student outcomes.\nThe data comes from a mix of sources: online - LinkedIn, personal websites, and professional organizations - and UCSB’s own information about students. While not all of the data was accessed through an automated system, some of it required manual revisions to determine if a student’s data is updated. Inevitably, there will exist some type of error – whether it is a simple misspelling of a word, or perhaps a text string existing in an integer column.\nThis project provides a unique opportunity to explore data. Here are some possible avenues:\n\nOutcomes by Major\nRanking of Graduate Schools UCSB students attend\nJob Opportunities\nGeographical Distribution of Students\nFreshmen vs. Transfer Admit Outcomes\nGreek life vs Non-Greek life Students\nCareer outcomes for athletes\nFemale athletes in leadership roles\nStudents from low-income high schools\nDistribution by undergradute major\nCareer outcomes and future earnings"
  },
  {
    "objectID": "projects/projects-20-21.html#cheadle-center-for-biodiversity-and-ecological-restoration-ccber",
    "href": "projects/projects-20-21.html#cheadle-center-for-biodiversity-and-ecological-restoration-ccber",
    "title": "2020-2021 Projects",
    "section": "Cheadle Center for Biodiversity and Ecological Restoration (CCBER)",
    "text": "Cheadle Center for Biodiversity and Ecological Restoration (CCBER)\nBees + flowers / GloBI interactions\nStudent team: Nick Bachelder, Angel Chen, Zoe Fang, Mitchell Rapaport, and Samantha Solomon\nAdvisors: Michelle Lee, Katja Seltmann, Joshua Bang, and Kate Kharitonova\nWe are experiencing a global loss of insects. Already, the loss of any one species can have dramatic cascading impacts on an ecosystem. With 35% of global crop production relying on plant-pollinators, the loss of multiple species of pollinators can have devastating effects on the world’s food production.\nThis project utilizes the vast Global Biotic Interactions (GloBI) dataset to explore biodiversity. In particular, the project looks at the Apidae family of insects (honey bees, bumble bees, carpenter bees, etc.) and the plants they pollinate. Team members will build a network to discern more information about the importance of bees in our global ecosystem.\n\n\n\n\nBachelder, N. R, Chen, A., Zoe, F., Rapaport, M. K, Bang, J., Solomon, S. J, et al. (2021). Leveraging Large Biological Interaction Data to Quantify Plant Specialization by Bees. Poster presented at Ecological Society of America Meeting. August, 2021. https://escholarship.org/uc/item/33b2t2bq"
  },
  {
    "objectID": "projects/projects-20-21.html#evidation-health",
    "href": "projects/projects-20-21.html#evidation-health",
    "title": "2020-2021 Projects",
    "section": "Evidation Health",
    "text": "Evidation Health\nTracking activity patterns in those recently infected by COVID-19\nStudent team: Peter Bayerle, Matthew Coleman, Joanna Kim, and Karsyn Lee\nAdvisors: Eric Daza, Jiajing Zheng, and Alex Franks\nThe aim of this project is to discover methods of pre-symptomatic COVID-19 detection using data collected from wearables. Using a recent study, “Pre-symptomatic detection of COVID-19 from smartwatch data” (Mishra et al), as the baseline for research and the source of data for analysis, the Evidation team will use the described outlier detection algorithms to determine the likelihood of COVID-19 infection in an individual given their heart rate, step count, and sleep data.\nShort term goals for the project include:\n\nRecreating visualizations from Mishra et al\nResearching Mishra et al’s HROS metric and exploring alternative metrics\nUsing anomaly detection algorithms described in Mishra et al and perhaps tweaking and improving upon these established algorithms\n\nUltimately, the project is not only looking to replicate the Mishra et al study but also to improve upon it and try alternative methods (like changing time frames used in computing metrics, exploring different anomaly detection algorithms, etc.). A long term goal of this project is to explore regularized cross-validated linear mixed models and causal modeling and inference."
  },
  {
    "objectID": "projects/projects-20-21.html#flir",
    "href": "projects/projects-20-21.html#flir",
    "title": "2020-2021 Projects",
    "section": "FLIR",
    "text": "FLIR\nHow pixel differences can affect sensors in self-driving cars\nStudent team: Sairisheek Muttukuru, Nicholas Nunes, Neil Sadhukhan, and Bill Zhang\nAdvisors: Louis Tremblay, Joshua Bang, and Sang-Yun Oh\nThe FLIR team is working on real time object detection models with a primary use for autonomous driving cars. Much of the current technology is centered around RGB cameras as seen in Figure 1.\n\n\n\nFigure 1: RGB Sensing\n\n\nWhile RGB sensing is a de facto standard in real time object detection models it is not without its weaknesses and must be used in conjunction with other sensing technology to provide reliably safe object detection for autonomous driving.\nFigure 2 depicts a scene behind the lens of an RGB sensor on the left and the object detection of a thermal sensor on the right. RGB sensing tends to fail at night due to the reduced color space, which poses a large risk for the safety of autonomous driving cars. FLIR as a company specializes in the design and production of thermal imaging cameras, components, and imaging sensors, and hopes to leverage that technology in the field of computer vision.\n\n\n\nFigure 2: comparison of RGB sensing (left) and thermal sensing (right) for identical night scenes.\n\n\nThis project is primarily concerned with analyzing the robustness of a thermal object detector for ADAS (Advanced Driver Assistance Systems) subject to the effect of added noise. Understanding the effect of noise is incredibly important to the field of computer vision. It has been observed that small amounts of noise can result in severe misclassifications. The team is also exploring adversarial attacks (adding inputs to a model to intentionally cause misclassifications) in an attempt to make the object detection models as robust as possible."
  },
  {
    "objectID": "projects/projects-20-21.html#invoca",
    "href": "projects/projects-20-21.html#invoca",
    "title": "2020-2021 Projects",
    "section": "Invoca",
    "text": "Invoca\nSpeech and text analysis\nStudent team: Amy Cho, Anton de la Fuente, Sai Kathika, Austin Quinn, and Shuyun Tang\nAdvisors: Mike Lawrence, Sikun Lin, and Ambuj Singh\nThis project explores the development of applications for collecting and categorizing key elements in a conversation. One idea is to extend the designed application to a chatbot. To build their models, the team is using the open-source Google Taskmaster-1 dataset, which consists of 13,215 task-based dialogs created between a customer and an agent having a conversation recorded in text about one of the following six topics: ordering pizza, creating auto repair appointments, setting up ride service, ordering movie tickets, ordering coffee drinks, and making restaurant reservations."
  },
  {
    "objectID": "projects/projects-20-21.html#national-center-for-ecological-analysis-and-synthesis-nceas",
    "href": "projects/projects-20-21.html#national-center-for-ecological-analysis-and-synthesis-nceas",
    "title": "2020-2021 Projects",
    "section": "National Center for Ecological Analysis and Synthesis (NCEAS)",
    "text": "National Center for Ecological Analysis and Synthesis (NCEAS)\nDeveloping a data set for each of the 28 sites of the Long Term Ecological Research (LTER) network\nStudent team: Sam Guo, Adhitya Logan, Lia Ran, Sophia Sternberg, and Karen Zhao\nAdvisors: Julien Brun, Allison Horst, Chau Tran, and Sang-Yun Oh\nDr. Julien Brun (NCEAS) and Dr. Allison Horst (UCSB Bren School) have been developing an R package to help teach data science. The main goal of this package is to offer a set of data sets to replace the usual datasets shipped with R (mtcars, iris, …) with something more exciting and geared towards Environmental Sciences. We aim at developing a data set for each of the 28 sites of the Long Term Ecological Research (LTER) Network. Students on this team will help Julien and Allison with their R package (and accompanying website).\nThe LTER network is an NSF-funded research program to observe long-term and large scale ecological effects at various sites across the United States. LTER datasets are archived in a centralized data repository managed by the Environmental Data Initiative (EDI).\nThis project is based on a previous project led by Allison on the Palmer Penguins data set."
  },
  {
    "objectID": "projects/projects-20-21.html#neuroscience-research-institute-nri",
    "href": "projects/projects-20-21.html#neuroscience-research-institute-nri",
    "title": "2020-2021 Projects",
    "section": "Neuroscience Research Institute (NRI)",
    "text": "Neuroscience Research Institute (NRI)\nExploring and understanding a rare genetic mutation that causes early-onset Alzheimer’s\nStudent team: Helen Huang, Shivam Misra, Priya Padmanabhan, Pugal Surya, and Rey Yue\nAdvisors: Juliana Acosta Uribe, Ken Kosik, Joshua Bang, and Alex Franks\nHow has a present day population been shaped by the risk of Alzheimer’s disease? This population genetics project seeks to understand a rare, variant genetic mutation that causes early-onset Alzheimer’s disease.\nWith access to a data set made up of the full genome sequences from hundreds of individuals in Columbia, the capstone team’s first goal will be to trace how far back the mutation/mutations responsible for said disorder go. Identifying common ancestors and the timeframe when the first mutation occurred can provide key insights into whether or not the mutation was the result of potential selective advantage. Another goal of the project is to estimate the age of each variant of the disease.\nThis rewarding project introduces exciting biology concepts and terminology, as well as software and tools for analyzing genotype data."
  },
  {
    "objectID": "projects/projects-20-21.html#sansum-diabetes-research-institute",
    "href": "projects/projects-20-21.html#sansum-diabetes-research-institute",
    "title": "2020-2021 Projects",
    "section": "Sansum Diabetes Research Institute",
    "text": "Sansum Diabetes Research Institute\nMil familias data analysis and communication: impact of diabetes in the Latino community\n\n\nStudent team: Zoe Holzer, Lawrence Lin, Vanessa Salgado Zavaleta, Emily San Juan, and Jake Simon\nAdvisors: Namino Glantz, David Kerr, Jiajing Zheng, and Alex Franks\n\n\n\n\nAlmost 1 in 2 Latino adults are at risk of type 2 diabetes. Recent data from the CDC shows that diabetes rates are twice as high for Latinos of Mexican heritage compared with Whites.\nSDRI has been operating the Mil Familias project, which focuses on the impact of diabetes in the Latino community. The vision of the Mil Familias Program is to reduce dramatically the burden of diabetes, heart disease, and stroke among Latino families in the United States. Bilingual, specially trained community health workers who are knowledgeable about type 2 diabetes (Especialistas) have interacted with families to collect data, answer questions, and connect families to the appropriate resources. Every year for 10 years, more than 100 variables are measured for each family member using surveys, physical exams, lab tests, and wearable activity monitors. The information that families contribute are then placed in a database about Latino health, behavior, lifestyle, and environment. Based on the data, Mil Familias plans to intervene in the areas of education, prevention, technology, policy, treatment, and more.\nThis project asks, which factors out of biology, behavior, psychology, society/environment are causing the highest amount of disproportionate burden in the Latino/Latina community of type 2 diabetes in mild (&gt;7%) and severe (&gt;9%) cases respectively?"
  },
  {
    "objectID": "projects/projects-21-22.html",
    "href": "projects/projects-21-22.html",
    "title": "2021-2022 Projects",
    "section": "",
    "text": "Listed alphabetically by project sponsor."
  },
  {
    "objectID": "projects/projects-21-22.html#allthenticate",
    "href": "projects/projects-21-22.html#allthenticate",
    "title": "2021-2022 Projects",
    "section": "Allthenticate",
    "text": "Allthenticate\nQuantifying Security - Visualizing Usability of Allthenticate\nStudent team: Akshat Ataliwala, Calvin Jenkins, Bosco De Enrique Romeu, and Yuki Yamazaki\nProject advisors: Chad Spensky and Leron Reznikov\nAllthenticate provides a proximity-based app-enabled locking/unlocking system in place of traditional badge or lock-and-key security systems. The primary objectives of this project are to quantify the benefits that our Allthenticate’s security product provides and identifying opportunities to improve the Allthenticate user experience.\nFirst, we want to quantify how much of an improvement Allthenticate’s security solution provides over existing solutions. This includes tasks such as measuring time saved by using Allthenticate, tracking changes in Bluetooth connection, and monitoring app status. We will analyze the user data provided by the logs from readers, as well as the phone and computer apps. Second, we intend to increase how well the product is working. Allthenticate is constantly improving, and with user-happiness being at the core of the company’s values, we want to cater the best experience for the user. An example of this could be predicting a user’s app behaviors to personalize their UI to give them the most seamless experience for their particular needs.\nUsing preliminary product data, we developed a visualization dashboard for real-time monitoring of a client’s system activity."
  },
  {
    "objectID": "projects/projects-21-22.html#appfolio",
    "href": "projects/projects-21-22.html#appfolio",
    "title": "2021-2022 Projects",
    "section": "AppFolio",
    "text": "AppFolio\nDeep Learning Recommendation Systems for Property Management Software\nStudent team: Alyssa Keehan, Jordan Tran, Joshua Harasaki, Philip Carey, and Romina Fareghbal\nProject advisors: Shyr-Shea Chang, Soeren Thust, and Erika McPhillips\nFor this project, we created a recommender system that predicts users interests and recommends properties to users based on previous user interactions and property features. Recommender systems are among the most powerful machine learning systems that are used to help users identify items they may be interested in and also helps companies increase interaction and sales. Our sponsor is AppFolio, a tech company based in Santa Barbara that provides innovative software, services, and data analytics to the real estate industry.\nBuilding this model will allow AppFolio to analyze the results of our recommender system to assess how interaction data can be used to better their products. We explored multiple models including the Bilateral Variational Autoencoder (BiVae), TriVae, Visual Bayesian Personalized Ranking (VBPR), and Deep Cross Network (DCN) in an effort to achieve the highest prediction accuracy using 16 property features from our given dataset and images. Ultimately, we want to integrate our model into the Santa Barbara property website, which was built by the AppFolio Sponsored CS Capstone team."
  },
  {
    "objectID": "projects/projects-21-22.html#california-cooperative-oceanic-fisheries-investigations-calcofi",
    "href": "projects/projects-21-22.html#california-cooperative-oceanic-fisheries-investigations-calcofi",
    "title": "2021-2022 Projects",
    "section": "California Cooperative Oceanic Fisheries Investigations (CalCOFI)",
    "text": "California Cooperative Oceanic Fisheries Investigations (CalCOFI)\nInteractive Spatio-Temporal Visualization of Long-Term Physical Characteristics of California Coastal Waters\nStudent team: Prerana Kottapalli (PK), Mallika Gupta, Teresa Gonzalez, and Dante Coletta\nProject advisors: Erin Satterthwaite and Trevor Ruiz\nThe goal of this project is to visualize the variation of ocean chemistry over time and space in the California current to help understand changing ocean conditions and identify potential impacts on marine life. We will create an exploratory tool using long-term oceaongraphic data collected by CalCOFI. Data includes quarterly measurements of oceanic samples, at a depth of up to 1,000m, collected from 1970-2020. Parameters explored are dissolved oxygen, temperature, salinity, and chlorophyll."
  },
  {
    "objectID": "projects/projects-21-22.html#carpe-data",
    "href": "projects/projects-21-22.html#carpe-data",
    "title": "2021-2022 Projects",
    "section": "Carpe Data",
    "text": "Carpe Data\nMulti-class Claims Activity Classification Based on HTML data\nStudent team: Annie Huang, Anum Damani, Rithvik Vobbilisetty, Alex Rudolph, and Tyler Chia\nProject advisors: Crystal Zhang, Kevin Neal, and Joshua Bang\nInsurance companies typically receive a significant number of claims each year. By automatically flagging and classifying web pages that have information potentially relevant to the claims, our project would help to significantly reduce the amount of manual inspection required for potential cases of insurance fraud. Our project focuses on building machine learning classification models that predict and flag whether or not a web page contains evidence about a fraud claim, and identify the specific type of potentially fraudlent activity that is present.\nOur dataset contained 42,485 web pages potentially containing claimant activity information identified through proprietary search methods and manually labeled. We classified each page into one of several classes of potentially fraudulent activity. Ultimately, our best performing model was our SVM classifier with a weighted average precision of 85%."
  },
  {
    "objectID": "projects/projects-21-22.html#cheadle-center-for-biodiversity-and-ecological-restoration-ccber",
    "href": "projects/projects-21-22.html#cheadle-center-for-biodiversity-and-ecological-restoration-ccber",
    "title": "2021-2022 Projects",
    "section": "Cheadle Center for Biodiversity and Ecological Restoration (CCBER)",
    "text": "Cheadle Center for Biodiversity and Ecological Restoration (CCBER)\nBig Bee: Hair Recognition and Quantification\nStudent team: Harper Klauke, Jonathan Wong-Lau, Harleen Kaur, and Nicholas Alexander\nProject advisors: Katja Seltmann and Joshua Bang\nBee species are decreasing in both number and diversity. As important pollinators, bees are responsible for creating and maintaining the ecosystems that many animals and humans rely on. Examining their physical traits and the differences between species will allow us to study the resiliency of different bees using computer vision and machine learning. Our objective in this project is to use computer vision techniques to detect and count bee hairs from high-resolution lateral images.\nWe worked with 550 high resolution lateral bee images that were gathered from various institutions for the Big Bee Library, an online repository of bee image, trait, and specimen data. Contributors included the Museum of Comparative Zoology, Harvard University, University of Kansas Natural History Museum Entomology Division, and the UC Santa Barbara Cheadle Center for Biodiversity and Ecological Restoration. This research was supported by the National Science Foundation: Extending Anthophila research through image and trait digitization (Big-Bee) project (DBI2102006)."
  },
  {
    "objectID": "projects/projects-21-22.html#center-for-information-technology-and-society-cits",
    "href": "projects/projects-21-22.html#center-for-information-technology-and-society-cits",
    "title": "2021-2022 Projects",
    "section": "Center for Information Technology and Society (CITS)",
    "text": "Center for Information Technology and Society (CITS)\nAnalyzing Changes in Social Media Usage Behavior\nStudent team: Eoin Hayes, Qiru Hu, Jasmine Kwok, Lucas Nguyen, and Xavier Speropoulos\nProject advisors: CITS Center, Qing Huang, and Erika McPhillips\nIn 2020, The Social Dilemma became the most popular movie on Netflix. The documentary reveals the downsides of the major social media platforms. The goal of this project is to analyze the impact of The Social Dilemma on its audience’s attitudes towards social media. Does watching this movie increase/decrease/not change people’s social media usage?\nTo answer this question, we performed a series of statistical analyses on user-generated posts from Twitter and Reddit. Although our study reveals a difference in the user behavior patterns between the two platforms, the statistical results suggest that, based on the social media data, the movie did not change people’s social media usage."
  },
  {
    "objectID": "projects/projects-21-22.html#energy-and-environment-transitions-envent-lab",
    "href": "projects/projects-21-22.html#energy-and-environment-transitions-envent-lab",
    "title": "2021-2022 Projects",
    "section": "Energy and Environment Transitions (ENVENT) Lab",
    "text": "Energy and Environment Transitions (ENVENT) Lab\nDemographic Predictors of Climate Opinion\nStudent team: Emma Franzblau, Annie Adams, Andrew Bissell, and Johnny Yu\nProject Advisors: Matto Mildenberger and Adam Waterbury\nThe aim of our project is to explore how demographic attributes are differentially predictive of climate opinion in different countries, continents, and time periods. We first cleaned and organized covariate data from 140+ surveys and merged the demographic information into existing data processing routines. Then we developed preliminary and exploratory predictive models to gain insights into our question of interest.\nWe were ultimately able to set the foundation for further exploration of the data in the next iteration of the project as a result of completing the necessary proof of concepts; which we did by honing in on the demographic of gender as it relates to various climate opinion metrics through different methods of modeling.\nSee our project poster here."
  },
  {
    "objectID": "projects/projects-21-22.html#earth-research-institute-eri",
    "href": "projects/projects-21-22.html#earth-research-institute-eri",
    "title": "2021-2022 Projects",
    "section": "Earth Research Institute (ERI)",
    "text": "Earth Research Institute (ERI)\nCombining Computer Vision and Satellite Imagery to Detect Center Pivot Irrigation\nStudent team: Ashley Grinstead, Evan Bui, Tiffany Hsu, and Tristan Chen\nProject advisors: Kelly Caylor, Leron Reznikov, and Anna Boser\nIrrigated agriculture is expanding rapidly across southern Africa. Because this expansion is occurring without much regulation or monitoring, it’s not exactly clear where and when expansion is happening, or the fate of expanded agriculture. In most expansionary cycles, there is a boom-and-bust dynamic, where many investments fail quickly and only a few persist. The Earth Research Institute (ERI) at UCSB spans areas covering natural hazards, human impacts, earth system science, and earth evolution by collaborating across interdisciplinary departments on campus. Under the guidance of Kelly Caylor (Director of ERI and WAVES lab), we are undertaking a project to explore how center pivot-irrigated agriculture is expanding globally. We’re interested in mapping these dynamics over the recent years and monitoring them going forward.\nSee our project poster here."
  },
  {
    "objectID": "projects/projects-21-22.html#evidation",
    "href": "projects/projects-21-22.html#evidation",
    "title": "2021-2022 Projects",
    "section": "Evidation",
    "text": "Evidation\nStress Indicators for Frontline COVID-19 Healthcare Workers\nStudent team: Priscilla Siow, Ali Vakilimafakheri, Sarah Schlund, and Eric Wang\nProject advisors: Eric J. Daza and Alex Franks\nUsing wearable, survey, and biomedical data from healthcare workers measured during the pandemic, we aim to identify variables indicative of stress such as sleep patterns, heart rate, PTSD, etc. We hope that our findings will give insights into how wearable technology can help detect stress in this population.\nThe features we used primarily came from wearable and survey data such as the Oura Ring, Daily Stress Measure, PTSD, Adverse Childhood Events (ACE), and Demographics datasets that were provided from Synapse database. We built two models: a mixed effects model that accounts for subject-specific baseline stress levels in predicting excess stress; and a logistic regression that identifies population-level indicators of stress.\n\n\n\nWearables Predict Stress in Frontline Healthcare Workers\nStudent team: Joshua Alvarado, Ian Gascon, Joanne Kim, Mac Kul, and Jessica Nguyen\nProject advisors: Julio Vega and Alex Franks\nWe are using data collected in “The Stress and Recovery in Frontline COVID-19 Health Care Workers Study.” Daily notifications prompted 365 participants to complete tasks through a REDCap study app including a Demographic Survey and a Daily Stress Measure and Work Shift Details survey.\nOur project objectives are twofold: identifying the main factors of stress (duration of sleep in different stages, heart rate, respiratory rate, and levels of oxygen saturation); and predicting whether a participant is stressed given those factors.\nSee our project poster here"
  },
  {
    "objectID": "projects/projects-21-22.html#hg-insights",
    "href": "projects/projects-21-22.html#hg-insights",
    "title": "2021-2022 Projects",
    "section": "HG Insights",
    "text": "HG Insights\nProduct Recommendation on Scraped Usage Data\nStudent team: Atherv Gole, Cristian Razo, Eric Cha, Natasha Leodjaja, and Qimin Tao\nProject advisors: HG Insights, Rob Fox, and Lauren Wong\nThe primary goal of this project is to analyze patterns in product adoption to effectively engage with existing and/or potential customers. Our primary dataset consists of companies’ product quality ratings across the United States over a span of 20 years, ranging from 2001 to the present. Two of the main features we used are ‘signal score,’ which is a 1-3 score of how recently the product was observed, and ‘intensity’ which is a measure of how much a product is observed over time."
  },
  {
    "objectID": "projects/projects-21-22.html#neuroscience-research-institute-nri",
    "href": "projects/projects-21-22.html#neuroscience-research-institute-nri",
    "title": "2021-2022 Projects",
    "section": "Neuroscience Research Institute (NRI)",
    "text": "Neuroscience Research Institute (NRI)\nModeling Neurodegeneration with Single Cell RNA Sequencing\nStudent team: Jake Stenger, Nancy Zha, and Sara Mandic\nProject advisors: Stella Glasauer, Megan Elcheikhali\nThis project aims to model neurodegeneration in brain organoids by single-cell RNA sequencing. Through the use of statistical methods we are aiming to compare differences in gene expression between control and mutant brain organoids. In addition we aim to identify cell populations that are more or less abundant in the mutant brain organoids. Thus, we are aiming to gain insight in disease mechanisms that lead to neurodegeneration."
  },
  {
    "objectID": "projects/projects-21-22.html#pricewaterhouse-coopers-pwc",
    "href": "projects/projects-21-22.html#pricewaterhouse-coopers-pwc",
    "title": "2021-2022 Projects",
    "section": "Pricewaterhouse Coopers (PwC)",
    "text": "Pricewaterhouse Coopers (PwC)\nCompression of NLP-Domain Deep Learning Models\nStudent team: Yan Lashchev, Jorge Murillo, Nathan Roll, Lawrence Su, and Yangyi Zhang\nProject advisors: Robert Bernard and Erika McPhillips\nThe field of model compression has enjoyed many advancements in recent years, yet few reliable methods have been developed specifically for the natural language processing (NLP) domain. In this project, we provide a review of model compression techniques and develop custom NLP-specific compression methods. We demonstrate the implementation of our methods on a sentiment analysis task."
  },
  {
    "objectID": "projects/projects-21-22.html#sprague-lab",
    "href": "projects/projects-21-22.html#sprague-lab",
    "title": "2021-2022 Projects",
    "section": "Sprague Lab",
    "text": "Sprague Lab\nDeep Image Reconstruction from Natural Scenes Dataset\nStudent team: James Du, Michael La, Connor Levenson, and Elise Nguyen\nProject advisors: Thomas Sprague and Sikun Lin\nWe leverage the recent progress made in text-to-image generation for deep image reconstruction. After building a joint-embedding model which maps the patient’s fMRI to a shared image-text space, we use a pre-trained decoder (LAFITE) which is able to recreate the stimulus image from the encoded vector."
  },
  {
    "objectID": "pstat197bc/archived/slides/week1-intro.html#capstone-goals",
    "href": "pstat197bc/archived/slides/week1-intro.html#capstone-goals",
    "title": "Welcome to Data Science Capstone",
    "section": "Capstone goals",
    "text": "Capstone goals\n\nmake original contributions on your research topic;\ndevelop domain expertise in your project area(s);\ndevelop and practice communicating research outcomes to diverse audiences;\nimprove collaboration and teamwork skills;\nadvance your career development;\nproduce a strong work sample for your professional portfolio"
  },
  {
    "objectID": "pstat197bc/archived/slides/week1-intro.html#course-structure",
    "href": "pstat197bc/archived/slides/week1-intro.html#course-structure",
    "title": "Welcome to Data Science Capstone",
    "section": "Course structure",
    "text": "Course structure\n\n1 weekly class meeting, usually Mondays, except Wednesday meetings weeks 2 and 7 (MLK and Prez days)\n1 weekly team meeting, scheduled at a time of your choice\noptional but highly recommended: second team meeting or working session without advisors\nfew class assignments each term:\n\none short presentation\nmid-term peer review\nsummative deliverable at end of term"
  },
  {
    "objectID": "pstat197bc/archived/slides/week1-intro.html#expectations",
    "href": "pstat197bc/archived/slides/week1-intro.html#expectations",
    "title": "Welcome to Data Science Capstone",
    "section": "Expectations",
    "text": "Expectations\n\nweekly time commitment: 12h = 2h in meetings + 10h on projects\nengage fully in project work, with some accountability:\n\ndraft, sign, and adhere to a team contract\ntake on one pre-defined organizational role in your team\nparticipate in peer reviews once per quarter"
  },
  {
    "objectID": "pstat197bc/archived/slides/week1-intro.html#roles",
    "href": "pstat197bc/archived/slides/week1-intro.html#roles",
    "title": "Welcome to Data Science Capstone",
    "section": "Roles",
    "text": "Roles\nYou’ll take on one of the following roles:\n\nspokesperson\nmeeting organizer\nnote-taker\nrepository and data manager\nfor five-person teams, equity manager"
  },
  {
    "objectID": "pstat197bc/archived/slides/week1-intro.html#peer-review",
    "href": "pstat197bc/archived/slides/week1-intro.html#peer-review",
    "title": "Welcome to Data Science Capstone",
    "section": "Peer review",
    "text": "Peer review\nEach quarter you’ll evaluate yourself/teammates on the extent to which you/they achieved the following:\n\nAttended, participated in, and contributed to project meetings\nHonored agreements specified in the team contract\nFulfilled assigned team role\nContributed fair share of work\nCooperated with teammates\nCommunicated clearly and respectfully with teammates\nHelped teammates when asked"
  },
  {
    "objectID": "pstat197bc/archived/slides/week1-intro.html#class-meetings",
    "href": "pstat197bc/archived/slides/week1-intro.html#class-meetings",
    "title": "Welcome to Data Science Capstone",
    "section": "Class meetings",
    "text": "Class meetings\nClass meetings will be run like a seminar: we’ll rotate through the project teams, with two teams presenting each meeting.\n\n~30min available per team, but budget time for discussion\ncurrently in alphabetical order, but can swap by mutual agreement\nto prepare: 10-ish slides and 1-2 page handout\nfurther guidelines TBA"
  },
  {
    "objectID": "pstat197bc/archived/slides/week1-intro.html#assessments",
    "href": "pstat197bc/archived/slides/week1-intro.html#assessments",
    "title": "Welcome to Data Science Capstone",
    "section": "Assessments",
    "text": "Assessments\n\nattendance record and class assignments;\nend-of-term advisor assessments of individual participation and contributions;\ninstructor assessments of in-class presentations and poster;\npeer reviews and end-of-term individual reflections."
  },
  {
    "objectID": "pstat197bc/archived/slides/week1-intro.html#initial-meetings",
    "href": "pstat197bc/archived/slides/week1-intro.html#initial-meetings",
    "title": "Welcome to Data Science Capstone",
    "section": "Initial meetings",
    "text": "Initial meetings\nSchedule an initial meeting for week 2. Can recommend: Slack channel + [when2meet].\nSuggested agenda:\n\nintroductions (15m):\n\nname, position/background, fun fact\nsomething exciting about the project\n\nproject intro (20m)\n\nhave sponsor tell you about the project in detail in their own words\nfocus on the big picture, ask questions\n\ndata access and logistics (10m)\n\nmake arrangements to access/obtain data\n\naction items (10m)\n\ndetermine what actions will be taken before the next meeting\nassign responsibility for each action"
  },
  {
    "objectID": "pstat197bc/archived/slides/week1-intro.html#assignments",
    "href": "pstat197bc/archived/slides/week1-intro.html#assignments",
    "title": "Welcome to Data Science Capstone",
    "section": "Assignments",
    "text": "Assignments\n\nMotivational statement due Friday 1/20 11:59pm PST. Take some time to write a paragraph or two in response to the following prompts:\n\nWhy are you participating in the capstone? What attracted you to the opportunity in the first place and how does it relate to your goals for your undergraduate education?\nWhat do you hope to get out of the experience? Why is that important to you?\nSubmit via Google form\n\nTeam contracts due Friday 1/27 11:59pm PST. See template."
  },
  {
    "objectID": "pstat197bc/archived/about-spring-meetings.html",
    "href": "pstat197bc/archived/about-spring-meetings.html",
    "title": "Spring class meetings",
    "section": "",
    "text": "Schedule is tentative and subject to change. Check back for updates.\n\n\n\n\n\n\n\nWeek\nClass meeting\nAssignments/comments\n\n\n\n\n1 (4/3/23)\nIntroduction and spring quarter overview\n\n\n\n2 (4/10/23)\nPair 1 presents\nRevisions to interim reports due Friday 11:59pm PST\n\n\n3 (4/17/23)\nPair 2 presents\n\n\n\n4 (4/24/23)\nPair 3 presents\n\n\n\n5 (5/1/23)\nPair 4 presents\n\n\n\n6 (5/8/23)\nPair 5 presents\nPeer review due Friday 11:59pm PST\n\n\n7 (5/15/23)\nWorkshopping: figure drafts\n\n\n\n8 (5/22/23)\nWorkshopping: presenting figures\n\n\n\n9 (5/29/23)\nWorkshopping: poster drafts\nNo class Monday;WEDNESDAY meeting instead\n\n\n10 (6/5/23)\nPublic showcase\nLoma Pelona center 12pm – 4pm\n\n\n11 (finals)\n\nIndividual project summaries due Monday 6/12"
  },
  {
    "objectID": "pstat197bc/archived/about-spring-meetings.html#useful-links",
    "href": "pstat197bc/archived/about-spring-meetings.html#useful-links",
    "title": "Spring class meetings",
    "section": "Useful links",
    "text": "Useful links\n\nAttendance form (please fill out once per meeting)\nDrive folder for presentation filesharing (be aware, contents visible with a UCSB account and the link)\nPresentation guidelines – requirements and suggestions for work-in-progress presentations and agenda"
  },
  {
    "objectID": "pstat197bc/archived/about-spring-meetings.html#week-1",
    "href": "pstat197bc/archived/about-spring-meetings.html#week-1",
    "title": "Spring class meetings",
    "section": "Week 1",
    "text": "Week 1\n\nMonday meeting: welcome back and spring term overview [slides]\nClass assignments:\n\nrevisions to interim reports due Friday 4/14 11:59pm PST (upload to same folder as before with suffix -revised)\nfigure for website due Friday 4/14 11:59pm PST [upload here]"
  },
  {
    "objectID": "pstat197bc/archived/about-spring-meetings.html#weeks-2-6",
    "href": "pstat197bc/archived/about-spring-meetings.html#weeks-2-6",
    "title": "Spring class meetings",
    "section": "Weeks 2-6",
    "text": "Weeks 2-6\n\nMonday meetings: work-in-progress presentations, as scheduled by table [slides]"
  },
  {
    "objectID": "pstat197bc/archived/about-spring-meetings.html#week-7",
    "href": "pstat197bc/archived/about-spring-meetings.html#week-7",
    "title": "Spring class meetings",
    "section": "Week 7",
    "text": "Week 7\n\nAssignments:\n\nComplete [peer review] by end of week\nPrepare figure drafts for poster by next class meeting on 5/22\nreview [poster template]\n\nMonday meeting: Drafting presentation figures [slides]"
  },
  {
    "objectID": "pstat197bc/archived/about-spring-meetings.html#week-8",
    "href": "pstat197bc/archived/about-spring-meetings.html#week-8",
    "title": "Spring class meetings",
    "section": "Week 8",
    "text": "Week 8\n\nAssignments:\n\nPrepare poster draft by next class meeting\n\nMonday meeting: Presenting figures, poster layout [slides]"
  },
  {
    "objectID": "pstat197bc/archived/about-spring-meetings.html#week-9",
    "href": "pstat197bc/archived/about-spring-meetings.html#week-9",
    "title": "Spring class meetings",
    "section": "Week 9",
    "text": "Week 9\n\nAssignments:\n\nFinalize and print poster (see slides for printing instructions)\n\nWednesday meeting: Practice poster presentations, logistics [slides]"
  },
  {
    "objectID": "pstat197bc/archived/about-spring-meetings.html#week-10",
    "href": "pstat197bc/archived/about-spring-meetings.html#week-10",
    "title": "Spring class meetings",
    "section": "Week 10",
    "text": "Week 10\n\nAssignments\n\nSubmit individual project summary by Friday, 6/16 11:59pm PST [guidelines] [upload]\nSubmit PDF copy of your group’s poster [drive folder]\n\nMonday meeting: poster showcase at Loma Pelona [photos]"
  },
  {
    "objectID": "pstat197bc/archived/interim-report-guidelines.html",
    "href": "pstat197bc/archived/interim-report-guidelines.html",
    "title": "Interim report guidelines",
    "section": "",
    "text": "This assignment takes the place of a final exam or project in a typical class, and is an opportunity for your team to reflect on their progress and plan for the term ahead. It is also an opportunity to practice writing a technical report. You will get more out of the assignment if you take the time to do that reflection together.\nBroadly, the interim report should summarize your work to date in a presentable manner."
  },
  {
    "objectID": "pstat197bc/archived/interim-report-guidelines.html#purpose",
    "href": "pstat197bc/archived/interim-report-guidelines.html#purpose",
    "title": "Interim report guidelines",
    "section": "",
    "text": "This assignment takes the place of a final exam or project in a typical class, and is an opportunity for your team to reflect on their progress and plan for the term ahead. It is also an opportunity to practice writing a technical report. You will get more out of the assignment if you take the time to do that reflection together.\nBroadly, the interim report should summarize your work to date in a presentable manner."
  },
  {
    "objectID": "pstat197bc/archived/interim-report-guidelines.html#expectations",
    "href": "pstat197bc/archived/interim-report-guidelines.html#expectations",
    "title": "Interim report guidelines",
    "section": "Expectations",
    "text": "Expectations\n\nAudience\nYou are writing for a broad audience. You can assume the reader is familiar with statistics and data science at about the same level as your peers, but should not assume they have any expertise in your area or with the data or methods you are using.\nAvoid jargon, define key terms or acronyms when they first appear, and dedicate space to explaining data and methodology carefully, but avoid over-explaining or providing excessive detail – you are describing research, not teaching a class.\n\n\nLength\nThe report should be no less than 5 pages and no more than 10 pages. If necessary, you can include supplementary material or appendices following the main body of the report or as separate files/documents.\n\n\nContent\nOrganize your report into the sections outlined below. Use subheadings as appropriate to your project to further differentiate report contents within sections. Whenever possible, please make subheadings specific (e.g., “Biological species surveys” is better than “Data”). In general, the headings and subheadings should provide an outline of the contents of a report that is easy for the reader to skim and informative.\n\nIntroduction\nIntroduce the topic of your project and provide any background needed to understand your work. Above all, your introduction should convey the primary motivation and potential value of your work. You are not expected to do a literature review, but if there is closely related prior work or your sponsor has provided you with a collection of relevant papers, you should summarize these and provide citations in the introduction. Close the introduction with a statement of your project objective(s) and a summary of the organization of the report.\nProblem(s) of interest\nConcisely state the problems you’re working on. These should be the high-level problems corresponding to the project objectives that you hope to solve by the end of the capstone – not practical/technical problems encountered along the way like data representation or wrangling, issues with model training or algorithm implementations, computational constraints, etc.\nIf your project has multiple arms, dedicate one short paragraph to each arm. Aim to sum up the problems of interest for each arm in just 1-3 sentences (not required, but a good aspiration).\nTip: If you find yourself struggling to summarize the problems of interest concisely or providing a lot of context or explanation, that’s probably a sign that you need to go back and revise your introduction.\nMaterials and methods\nDescribe your datasets and the statistical/computational techniques you are using to address the problems of interest. Provide high-level summaries of any key methods: if you are fitting a model, write the model clearly and explain how you estimate parameters; if you are designing an algorithm, write it up in pseudo-code; if you are performing hypothesis tests, state the hypotheses and indicate the test. This should be detailed but not long; if it begins to sprawl over many pages, that’s a sign that you need to edit it down.\nPreliminary findings\nPresent key findings from this quarter. Resist the temptation to describe everything you’ve done – focus just on the results that capture the most significant advances. Prepare a table or figure that helps to communicate each key finding with an appropriate caption. If you include a table or figure, you should also describe it in words and explain its significance. No tables or figures should appear that you do not also discuss in the text of the report.\nTip: start by preparing the tables and figures, determine the order to present them in, and then write this section around those elements.\nDiscussion\nThis is your opportunity to describe any particular challenges, interpretations of findings, and the like. Try to stick to the facts in the findings section, and use this section to explain how they relate to your problems of interest and project objectives.\nFuture work\nDescribe how you anticipate extending your project into spring quarter. Be specific. You do not need to present a detailed plan or timeline, but should convey what you expect to work on during Spring quarter. Use this as an opportunity to set goals and expectations for your group.\n\n\n\nFormat\nYour report should be in PDF (.pdf) or Word (.docx) format.\nAll figures and tables should be labeled and captioned, e.g., “Fig. 1: map of sampling locations in 2018”.\nCopies of any figures should be provided as separate images in JPEG, TIFF, or PNG format; images should have a resolution of at least 300 dots per inch (dpi).\nSupplementary material can be submitted in any format."
  },
  {
    "objectID": "pstat197bc/archived/interim-report-guidelines.html#how-to-submit",
    "href": "pstat197bc/archived/interim-report-guidelines.html#how-to-submit",
    "title": "Interim report guidelines",
    "section": "How to submit",
    "text": "How to submit\nUpload your PDF report and any supplementary files to this Drive folder. Use the naming conventions:\n\nreport document: TEAM-interim-report-w23.pdf\n\n\n\ncopies of figures from the report: img/TEAM-fig1.png (filename should match figure label in document)\nsupplementary material: TEAM-[description].[extention]"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "The capstone aims to provide students with hands-on research training and career development opportunities related to data science. It is equally suitable for students interested in entering the job market as it is for those considering graduate school – we partner with both industry and academic project sponsors to offer research experiences spanning a breadth of application domains."
  },
  {
    "objectID": "about.html#student-experience",
    "href": "about.html#student-experience",
    "title": "About",
    "section": "Student experience",
    "text": "Student experience\nThe capstone is for senior students of any discipline having an intermediate level of experience with applied quantitative methods of any kind and some exposure to programming. Participation is by application only; to ensure each student has close interaction with peer collaborators, faculty, and sponsors, roughly 60 students are admitted each year.\nFor participating students, the capstone is a year-long course sequence comprising two distinct portions: a preparatory course in data science topics and collaborative research skills (PSTAT197A/CMPSC190DD, offered fall term) and two experiential learning courses in which students work in teams on interdisciplinary projects sponsored by an external partner and advised by a technical specialist (PSTAT197B-C/CMPSC190DE-DF, offered winter and spring terms). The course sequence culminates with a public showcase of student work.\nStudents can expect to gain training and skills in collaborative data science and produce a strong work sample by the end of the program. All students will prepare a poster presentation for the public showcase, and posters will be published online. Depending on the project, students may also contribute during or after the project work period to other outputs such as public repositories, software products, conference presentations, or publications."
  }
]